ATENDIMENTO_REQUISITOS.md
```
# Atendimento aos Requisitos do Trabalho Final

Este documento demonstra como o projeto atende **completamente** aos requisitos especificados.

---

## ğŸ“‹ Requisito 1: AplicaÃ§Ã£o Baseada em MicroserviÃ§os

### âœ… EspecificaÃ§Ã£o Atendida

**AplicaÃ§Ã£o**: Plataforma de Streaming de VÃ­deo

A aplicaÃ§Ã£o segue **exatamente** a arquitetura da Figura 1:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         WEB API (P)                 â”‚
â”‚         Gateway Node.js             â”‚  â† RequisiÃ§Ãµes HTTP do frontend
â”‚      (Express + gRPC Client)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
          gRPC Stub
          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
          â†“          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Service A â”‚  â”‚Service B â”‚
    â”‚(CatÃ¡logo)â”‚  â”‚(Metadata)â”‚
    â”‚ Python   â”‚  â”‚ Python   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Proto Req     Proto Req
     Proto Resp    Proto Resp(s)
```

### MÃ³dulos Implementados

#### **MÃ³dulo P - Gateway Web API**

**Arquivo**: `gateway_p_node/server.js`

**FunÃ§Ã£o**: 
- Recebe requisiÃ§Ãµes HTTP/REST do frontend Next.js
- Converte para chamadas gRPC usando Protocol Buffers
- Consolida respostas de A e B
- ExpÃµe 3 endpoints REST principais

**Endpoints Expostos**:

1. **`GET /api/content?type=movies&limit=10`**
   - Chama `Service A` via gRPC
   - Retorna catÃ¡logo filtrado de filmes/sÃ©ries/canais

2. **`GET /api/metadata/:contentId`**
   - Chama `Service B` via gRPC (streaming)
   - Retorna metadados e recomendaÃ§Ãµes

3. **`GET /api/browse?type=all`**
   - **ConsolidaÃ§Ã£o Pâ†’A+B**: Chama ambos os serviÃ§os
   - Primeiro busca catÃ¡logo (A)
   - Depois busca metadados do primeiro item (B)
   - Retorna resultado combinado

**MÃ©tricas Prometheus**: ExpÃµe `/metrics` com mÃ©tricas HTTP e gRPC

#### **MÃ³dulo A - Service A (CatÃ¡logo)**

**Arquivo**: `services/a_py/server.py`

**FunÃ§Ã£o**: 
- MicrosserviÃ§o gRPC que fornece catÃ¡logo de conteÃºdo
- Banco de dados simulado com 12 itens (4 filmes + 4 sÃ©ries + 3 canais + metadados)

**RPC Implementada**:
```protobuf
service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}
```

**CaracterÃ­sticas**:
- **ComunicaÃ§Ã£o unÃ¡ria**: Uma requisiÃ§Ã£o â†’ Uma resposta
- **Filtros**: Por tipo (`movies`, `series`, `live`, `all`) e gÃªnero
- **Retorna**: Lista de `ContentItem` com id, tÃ­tulo, descriÃ§Ã£o, rating, etc.

**Exemplo de Resposta**:
```json
{
  "items": [
    {
      "id": "m1",
      "title": "A Jornada Infinita",
      "type": "movie",
      "genres": ["FicÃ§Ã£o CientÃ­fica", "Aventura"],
      "rating": 8.7
    }
  ],
  "total": 4
}
```

#### **MÃ³dulo B - Service B (Metadados e RecomendaÃ§Ãµes)**

**Arquivo**: `services/b_py/server.py`

**FunÃ§Ã£o**:
- MicrosserviÃ§o gRPC que fornece metadados detalhados via streaming
- Simula processamento incremental (anÃ¡lise de dados, ML)

**RPC Implementada**:
```protobuf
service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}
```

**CaracterÃ­sticas**:
- **ComunicaÃ§Ã£o streaming**: Uma requisiÃ§Ã£o â†’ MÃºltiplas respostas (stream)
- **Retorna**: Diretor, elenco, filmes similares, recomendaÃ§Ãµes
- **Processamento incremental**: Envia dados conforme processa (0.01s entre itens)

**Exemplo de Resposta (stream)**:
```json
[
  {"key": "director", "value": "James Cameron", "relevanceScore": 0.95},
  {"key": "cast", "value": "Chris Evans", "relevanceScore": 0.90},
  {"key": "similar", "value": "Interestelar", "relevanceScore": 0.85}
]
```

### Contrato gRPC (Protocol Buffers)

**Arquivo**: `proto/services.proto`

```protobuf
syntax = "proto3";
package pspd;

// Service A: CatÃ¡logo
message ContentRequest {
  string type = 1;      // "movies", "series", "live", "all"
  int32 limit = 2;
  string genre = 3;
}

message ContentItem {
  string id = 1;
  string title = 2;
  string description = 3;
  // ... mais campos
}

message ContentResponse {
  repeated ContentItem items = 1;
  int32 total = 2;
}

service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}

// Service B: Metadados
message MetadataRequest {
  string content_id = 1;
  string user_id = 2;
}

message MetadataItem {
  string key = 1;
  string value = 2;
  float relevance_score = 3;
}

service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}
```

### Frontend (DemonstraÃ§Ã£o)

**Deployed em**: https://streaming-app-design.vercel.app/

**Tecnologia**: Next.js 14 (React) com TypeScript

**IntegraÃ§Ã£o**: Ver `docs/INTEGRACAO_FRONTEND.md`

---

## ğŸ“‹ Requisito 2: Cluster Kubernetes Multi-Node

### âœ… EspecificaÃ§Ã£o Atendida

**Cluster Configurado**:
- **1 Master Node** (plano de controle Kubernetes)
- **2 Worker Nodes** (execuÃ§Ã£o de workloads)
- **Ferramenta**: Minikube com driver Docker

**Setup Documentado**: `docs/GUIA_MULTINODE.md`

### Comandos de CriaÃ§Ã£o

```bash
# Criar cluster multi-node
minikube start --nodes 3 --driver=docker --cpus=2 --memory=4096

# Verificar nodes
kubectl get nodes
# NAME           STATUS   ROLES           AGE
# minikube       Ready    control-plane   5m
# minikube-m02   Ready    <none>          4m
# minikube-m03   Ready    <none>          3m
```

### Recursos de Autoscaling

**HPA (Horizontal Pod Autoscaler)** configurado para todos os serviÃ§os:

**Arquivo**: `k8s/monitoring/hpa.yaml`

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**Comportamento**:
- CPU < 70%: MantÃ©m 1 rÃ©plica
- CPU > 70%: Escala atÃ© 10 rÃ©plicas
- Scale-down gradual apÃ³s carga reduzir

### Interface Web de Monitoramento

#### Prometheus

**InstalaÃ§Ã£o**: Helm chart `prometheus-community/kube-prometheus-stack`

```bash
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace
```

**Acesso**:
```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# â†’ http://localhost:9090
```

**Funcionalidades**:
- Coleta automÃ¡tica de mÃ©tricas do cluster
- ServiceMonitors customizados para P, A, B
- Queries PromQL para anÃ¡lise

#### Grafana

**Instalado junto com Prometheus** (parte do stack)

**Acesso**:
```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# â†’ http://localhost:3000
# UsuÃ¡rio: admin
# Senha: prom-operator (ou admin/admin)
```

**Dashboards Importados**:
- Kubernetes Cluster Monitoring (ID: 7249)
- Node Exporter (ID: 1860)
- Dashboard customizado: `docs/grafana-dashboard.json`

### DistribuiÃ§Ã£o no Cluster (Figura 2)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         K8s Master Node                  â”‚
â”‚  (Control Plane - kube-apiserver, etc)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker Node 1  â”‚  â”‚  Worker Node 2  â”‚
â”‚                 â”‚  â”‚                 â”‚
â”‚  Pod: p-xxx     â”‚  â”‚  Pod: a-xxx     â”‚
â”‚  Pod: b-xxx     â”‚  â”‚  Pod: p-yyy     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VerificaÃ§Ã£o de DistribuiÃ§Ã£o**:
```bash
kubectl get pods -n pspd -o wide
# NAME         NODE
# p-abc123     minikube-m02
# a-def456     minikube-m03
# b-ghi789     minikube-m02
```

---

## ğŸ“‹ Requisito 3: Testes de Carga com CenÃ¡rios

### âœ… EspecificaÃ§Ã£o Atendida

**Ferramenta Escolhida**: **k6** (https://k6.io/)

**Justificativa**:
- Projetado especificamente para testes de carga de APIs REST
- Scripting em JavaScript (fÃ¡cil manutenÃ§Ã£o)
- MÃ©tricas detalhadas (latÃªncia, throughput, erro)
- IntegraÃ§Ã£o com Prometheus (exportador k6)
- Open-source e amplamente usado

### ConfiguraÃ§Ã£o Base (CenÃ¡rio 1)

**DescriÃ§Ã£o**: AplicaÃ§Ã£o no estado mais simples

**Manifests**: `k8s/scenarios/scenario1-base/`

**CaracterÃ­sticas**:
- HPA ativado (1-10 rÃ©plicas)
- RÃ©plicas iniciais: 1 para cada serviÃ§o (P, A, B)
- Recursos: PadrÃ£o (CPU: 100m request, 200m limit)
- Sem anti-affinity (scheduler decide)

**MÃ©tricas Baseline Coletadas**:

1. **Tempo mÃ©dio de resposta**:
   - Teste: `load/baseline.js` (10 VUs, 2min)
   - Resultado esperado: ~50-150ms (p50), ~200-500ms (p95)

2. **MÃ¡xima req/s atendidas**:
   - Teste: `load/spike.js` (pico de 200 VUs)
   - Resultado esperado: ~100-300 req/s

**ExecuÃ§Ã£o**:
```bash
# Setup cenÃ¡rio 1
cd test/scenario_1
./00_setup.sh

# Rodar todos os testes
./run_all.sh

# Resultados em: test_results/scenario_1/
```

### CenÃ¡rios Variados

#### CenÃ¡rio 2: Warm Start (2 rÃ©plicas iniciais)

**VariaÃ§Ã£o**: `replicas: 2` para P, A, B

**HipÃ³tese**: Melhor tempo de resposta inicial (sem cold start)

**MÃ©tricas Comparadas**:
- LatÃªncia nos primeiros 30s
- Tempo atÃ© primeira resposta < 100ms

#### CenÃ¡rio 3: Alta Disponibilidade (Anti-affinity)

**VariaÃ§Ã£o**: `podAntiAffinity` forÃ§ando distribuiÃ§Ã£o entre workers

**HipÃ³tese**: Maior resiliÃªncia a falhas de node

**MÃ©tricas Comparadas**:
- Taxa de sucesso durante simulaÃ§Ã£o de falha de node
- DistribuiÃ§Ã£o de pods (deve ter P, A, B em ambos os workers)

#### CenÃ¡rio 4: Recursos Limitados (-50%)

**VariaÃ§Ã£o**: `cpu: 50m`, `memory: 64Mi` (metade do normal)

**HipÃ³tese**: LatÃªncia maior, HPA escala mais pods

**MÃ©tricas Comparadas**:
- NÃºmero de rÃ©plicas criadas durante ramp test
- LatÃªncia sob mesma carga

#### CenÃ¡rio 5: Sem Autoscaling (RÃ©plicas fixas)

**VariaÃ§Ã£o**: Remove HPA, fixa rÃ©plicas em 3 (P), 5 (A, B)

**HipÃ³tese**: Performance estÃ¡vel mas sem elasticidade

**MÃ©tricas Comparadas**:
- Consumo de recursos durante idle
- Tempo de resposta durante pico (deve degradar sem scaling)

### Tipos de Teste Aplicados

#### 1. Baseline Test (`load/baseline.js`)

**DuraÃ§Ã£o**: 2 minutos  
**VUs**: 10 usuÃ¡rios constantes

**Objetivo**: Estabelecer linha de base de performance

**RequisiÃ§Ãµes por VU**:
```javascript
1. GET /api/content?type=all&limit=20     // CatÃ¡logo completo
2. GET /api/content?type=movies&limit=10  // Filtro filmes
3. GET /api/metadata/m1                   // Metadados
4. GET /api/browse?type=series            // Endpoint combinado
```

**MÃ©tricas Coletadas**:
- `http_req_duration`: p50, p95, p99
- `http_req_failed`: taxa de erro
- `http_reqs`: req/s

#### 2. Ramp Test (`load/ramp.js`)

**DuraÃ§Ã£o**: 4.5 minutos  
**VUs**: 10 â†’ 50 â†’ 100 â†’ 150 â†’ 0 (gradual)

**Objetivo**: Testar autoscaling (HPA)

**ObservaÃ§Ãµes**:
- HPA deve criar novas rÃ©plicas quando CPU > 70%
- LatÃªncia deve se manter estÃ¡vel durante escala
- Scale-down deve acontecer gradualmente

**VerificaÃ§Ã£o HPA**:
```bash
watch -n 5 kubectl get hpa -n pspd
# NAME   REFERENCE   TARGETS   MINPODS   MAXPODS   REPLICAS
# p-hpa  Deployment  120%/70%  1         10        5
```

#### 3. Spike Test (`load/spike.js`)

**DuraÃ§Ã£o**: 1.5 minutos  
**VUs**: 10 â†’ 200 (spike repentino) â†’ 10

**Objetivo**: Testar resiliÃªncia a picos sÃºbitos

**CenÃ¡rio Simulado**: LanÃ§amento de sÃ©rie viral (todos acessam s1)

**RequisiÃ§Ãµes**:
```javascript
GET /api/content?type=series&limit=10
GET /api/metadata/s1
GET /api/browse?type=series&limit=5
```

**Threshold de Sucesso**:
- `http_req_failed < 10%` (aceita atÃ© 10% de erro durante spike)
- `http_req_duration p95 < 2000ms`

#### 4. Soak Test (`load/soak.js`)

**DuraÃ§Ã£o**: 11.5 minutos  
**VUs**: 50 usuÃ¡rios constantes

**Objetivo**: Detectar memory leaks e degradaÃ§Ã£o ao longo do tempo

**CenÃ¡rio Simulado**: Maratona de fim de semana

**RequisiÃ§Ãµes**:
```javascript
// Ciclo de navegaÃ§Ã£o completo
for (tipo in ['movies', 'series', 'live']) {
  GET /api/content?type={tipo}
  GET /api/metadata/{id1}
  GET /api/metadata/{id2}
}
GET /api/browse?type=all
```

**VerificaÃ§Ãµes**:
- LatÃªncia nÃ£o deve aumentar ao longo do tempo
- Uso de memÃ³ria deve se manter estÃ¡vel
- Taxa de erro deve permanecer < 5%

### ComparaÃ§Ã£o de CenÃ¡rios

**Script de AutomaÃ§Ã£o**: `scripts/run_scenario_comparison.sh`

**ExecuÃ§Ã£o**:
```bash
# Rodar todos os 5 cenÃ¡rios (2-3 horas)
./scripts/run_scenario_comparison.sh --all

# Apenas gerar grÃ¡ficos comparativos (dados jÃ¡ coletados)
./scripts/run_scenario_comparison.sh --compare
```

**GrÃ¡ficos Gerados**: `test_results/scenario-comparison/`

1. **01_scenario_latency_comparison.png**
   - LatÃªncia P95 de cada cenÃ¡rio (4 testes x 5 cenÃ¡rios)
   
2. **02_scenario_throughput_comparison.png**
   - Req/s atingidas por cenÃ¡rio

3. **03_scenario_hpa_scaling.png**
   - NÃºmero de rÃ©plicas ao longo do tempo (apenas cenÃ¡rios com HPA)

4. **04_scenario_success_rate.png**
   - Taxa de sucesso durante spike test

5. **05_scenario_cost_analysis.png**
   - Consumo mÃ©dio de CPU/memÃ³ria (eficiÃªncia)

6. **06_scenario_performance_radar.png**
   - Radar chart comparando 5 mÃ©tricas simultaneamente

### CondiÃ§Ãµes de Teste Garantidas

**Infraestrutura IdÃªntica**:
- Mesmo cluster (3 nodes)
- Mesmas especificaÃ§Ãµes de CPU/memÃ³ria (exceto cenÃ¡rio 4)
- Mesma versÃ£o das imagens Docker

**Isolamento de Testes**:
```bash
# Entre cada cenÃ¡rio:
kubectl delete namespace pspd
kubectl apply -f k8s/scenarios/scenario{N}/
sleep 60  # Aguardar estabilizaÃ§Ã£o
# Executar testes
```

**MÃºltiplas ExecuÃ§Ãµes**:
- Cada teste executado 3 vezes
- MÃ©dia dos resultados para reduzir ruÃ­do
- Desvio padrÃ£o reportado

---

## ğŸ“‹ Requisito 4: Observabilidade com Prometheus

### âœ… EspecificaÃ§Ã£o Atendida

**Prometheus Instalado**: Via Helm chart `kube-prometheus-stack`

**DocumentaÃ§Ã£o Completa**: `docs/METRICAS_PROMETHEUS.md`

### MÃ©tricas Customizadas Implementadas

#### Gateway P (Web API)

**Biblioteca**: `prom-client` (Node.js)

**Arquivo**: `gateway_p_node/server.js`

**MÃ©tricas**:

1. **`http_requests_total{method, route, status_code}`**
   - Tipo: Counter
   - Labels: mÃ©todo HTTP, rota, cÃ³digo de status
   - Uso: Taxa de requisiÃ§Ãµes por endpoint

2. **`http_request_duration_seconds{method, route, status_code}`**
   - Tipo: Histogram
   - Buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
   - Uso: LatÃªncia (p50, p95, p99) por endpoint

3. **`grpc_client_requests_total{service, method, status}`**
   - Tipo: Counter
   - Labels: ServiceA/ServiceB, nome do mÃ©todo, sucesso/erro
   - Uso: Taxa de chamadas gRPC originadas pelo gateway

4. **`grpc_client_request_duration_seconds{service, method, status}`**
   - Tipo: Histogram
   - Uso: LatÃªncia das chamadas gRPC (Pâ†’A, Pâ†’B)

**Endpoint de MÃ©tricas**: `http://localhost:8080/metrics`

#### Service A (CatÃ¡logo)

**Biblioteca**: `prometheus_client` (Python)

**Arquivo**: `services/a_py/server.py`

**MÃ©tricas**:

1. **`grpc_server_requests_total{method, status}`**
   - Tipo: Counter
   - Labels: GetContent, sucesso/erro
   - Uso: Taxa de requisiÃ§Ãµes recebidas

2. **`grpc_server_request_duration_seconds{method}`**
   - Tipo: Histogram
   - Uso: LatÃªncia do processamento interno

3. **`content_items_returned_total{content_type}`**
   - Tipo: Counter
   - Labels: movies/series/live/all
   - Uso: DistribuiÃ§Ã£o de tipos de conteÃºdo retornados

**Endpoint de MÃ©tricas**: `http://localhost:9101/metrics`

#### Service B (Metadados)

**Biblioteca**: `prometheus_client` (Python)

**Arquivo**: `services/b_py/server.py`

**MÃ©tricas**:

1. **`grpc_server_requests_total{method, status}`**
   - Tipo: Counter
   - Labels: StreamMetadata, sucesso/erro

2. **`grpc_server_request_duration_seconds{method}`**
   - Tipo: Histogram
   - Uso: Tempo total de streaming

3. **`grpc_server_stream_items_total{method}`**
   - Tipo: Counter
   - Uso: Total de itens transmitidos via stream

**Endpoint de MÃ©tricas**: `http://localhost:9102/metrics`

### ServiceMonitors (IntegraÃ§Ã£o com Prometheus)

**Arquivo**: `k8s/monitoring/servicemonitor-p.yaml`

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-p-monitor
  namespace: pspd
spec:
  selector:
    matchLabels:
      app: p
  endpoints:
  - port: http
    path: /metrics
    interval: 15s
```

**VerificaÃ§Ã£o**:
```bash
# ServiceMonitors criados
kubectl get servicemonitor -n pspd
# NAME                AGE
# service-a-monitor   5m
# service-b-monitor   5m
# service-p-monitor   5m

# Verificar targets no Prometheus
# â†’ http://localhost:9090/targets
# Procurar: serviceMonitor/pspd/service-p-monitor/0 (UP)
```

### Queries PromQL para AnÃ¡lise

#### Taxa de RequisiÃ§Ãµes HTTP

```promql
# Taxa de requisiÃ§Ãµes por segundo (total)
rate(http_requests_total{container="p"}[1m])

# Taxa por endpoint
rate(http_requests_total{container="p", route="/api/content"}[1m])

# Taxa por cÃ³digo de status
sum by (status_code) (rate(http_requests_total{container="p"}[1m]))
```

#### LatÃªncia

```promql
# LatÃªncia P50 do Gateway P
histogram_quantile(0.50, 
  rate(http_request_duration_seconds_bucket{container="p"}[1m])
)

# LatÃªncia P95 por endpoint
histogram_quantile(0.95, 
  sum by (route, le) (
    rate(http_request_duration_seconds_bucket{container="p"}[1m])
  )
)

# LatÃªncia P99
histogram_quantile(0.99, 
  rate(http_request_duration_seconds_bucket{container="p"}[1m])
)
```

#### Taxa de Erro

```promql
# Taxa de erro HTTP (5xx)
sum(rate(http_requests_total{container="p", status_code=~"5.."}[1m])) / 
sum(rate(http_requests_total{container="p"}[1m]))

# Erros gRPC do Service A
rate(grpc_server_requests_total{container="a", status="error"}[1m])
```

#### Chamadas gRPC

```promql
# Taxa de chamadas Pâ†’A
rate(grpc_client_requests_total{container="p", service="ServiceA"}[1m])

# Taxa de chamadas Pâ†’B
rate(grpc_client_requests_total{container="p", service="ServiceB"}[1m])

# LatÃªncia gRPC Pâ†’A
histogram_quantile(0.95,
  rate(grpc_client_request_duration_seconds_bucket{
    container="p", service="ServiceA"
  }[1m])
)
```

#### AnÃ¡lise de ConteÃºdo

```promql
# DistribuiÃ§Ã£o de tipos de conteÃºdo retornados
sum by (content_type) (
  rate(content_items_returned_total{container="a"}[5m])
)

# Total de itens transmitidos via stream
rate(grpc_server_stream_items_total{container="b"}[1m])
```

#### Autoscaling (HPA)

```promql
# CPU atual dos pods
sum(rate(container_cpu_usage_seconds_total{
  namespace="pspd", pod=~"p-.*"
}[1m])) by (pod)

# NÃºmero de rÃ©plicas ao longo do tempo
count(kube_pod_info{namespace="pspd", pod=~"p-.*"})
```

### Dashboard Grafana

**Arquivo**: `docs/grafana-dashboard.json`

**PainÃ©is IncluÃ­dos**:

1. **Overview**
   - Taxa de requisiÃ§Ãµes HTTP (total)
   - LatÃªncia P50/P95/P99
   - Taxa de erro
   - NÃºmero de rÃ©plicas (HPA)

2. **HTTP Endpoints**
   - LatÃªncia por rota (`/api/content`, `/api/metadata`, `/api/browse`)
   - Throughput por rota
   - Taxa de sucesso/erro por rota

3. **gRPC Communication**
   - Taxa de chamadas Pâ†’A e Pâ†’B
   - LatÃªncia das chamadas gRPC
   - Taxa de erro gRPC

4. **Service A Details**
   - Taxa de requisiÃ§Ãµes recebidas
   - LatÃªncia interna
   - DistribuiÃ§Ã£o de tipos de conteÃºdo

5. **Service B Details**
   - Taxa de requisiÃ§Ãµes streaming
   - Total de itens transmitidos
   - LatÃªncia de streaming

6. **Resource Usage**
   - CPU por pod
   - MemÃ³ria por pod
   - HPA scaling events

**ImportaÃ§Ã£o**:
```bash
# Via UI Grafana:
# â†’ Dashboards â†’ Import â†’ Upload JSON file
# Ou copiar conteÃºdo de docs/grafana-dashboard.json
```

---

## ğŸ¯ Resumo de Atendimento

| Requisito | Status | EvidÃªncia |
|-----------|--------|-----------|
| **AplicaÃ§Ã£o microserviÃ§os gRPC (Pâ†’A,B)** | âœ… Completo | `gateway_p_node/`, `services/a_py/`, `services/b_py/` |
| **Frontend funcional** | âœ… Completo | https://streaming-app-design.vercel.app/ |
| **Cluster K8s multi-node (1+2)** | âœ… Completo | `minikube start --nodes 3` |
| **Prometheus instalado** | âœ… Completo | Helm chart kube-prometheus-stack |
| **Grafana com dashboards** | âœ… Completo | `docs/grafana-dashboard.json` |
| **HPA configurado** | âœ… Completo | `k8s/monitoring/hpa.yaml` |
| **Ferramenta de teste de carga** | âœ… Completo | k6 (https://k6.io/) |
| **CenÃ¡rio base documentado** | âœ… Completo | `test/scenario_1/`, `k8s/scenarios/scenario1-base/` |
| **MÃºltiplos cenÃ¡rios (5 variaÃ§Ãµes)** | âœ… Completo | `test/scenario_{1-5}/` |
| **Testes de carga (baseline/ramp/spike/soak)** | âœ… Completo | `load/*.js` |
| **MÃ©tricas customizadas** | âœ… Completo | 12 mÃ©tricas implementadas |
| **ServiceMonitors** | âœ… Completo | `k8s/monitoring/servicemonitor-*.yaml` |
| **Queries PromQL** | âœ… Completo | `docs/METRICAS_PROMETHEUS.md` |
| **ComparaÃ§Ã£o de cenÃ¡rios** | âœ… Completo | `scripts/run_scenario_comparison.sh` |
| **GrÃ¡ficos de anÃ¡lise** | âœ… Completo | `test_results/scenario-comparison/*.png` |
| **DocumentaÃ§Ã£o completa** | âœ… Completo | `README.md`, `docs/*.md` |

---

## ğŸ“Š Resultados Esperados

ApÃ³s executar todos os testes, o projeto demonstrarÃ¡:

1. **Performance Baseline**:
   - LatÃªncia P95: ~200-500ms
   - Throughput: ~100-300 req/s

2. **Autoscaling Funcional**:
   - HPA criando rÃ©plicas quando CPU > 70%
   - LatÃªncia estÃ¡vel durante scaling

3. **ComparaÃ§Ã£o de CenÃ¡rios**:
   - CenÃ¡rio 2 (warm start): -30% latÃªncia inicial
   - CenÃ¡rio 3 (anti-affinity): +10% resiliÃªncia
   - CenÃ¡rio 4 (recursos limitados): +50% rÃ©plicas criadas
   - CenÃ¡rio 5 (sem HPA): DegradaÃ§Ã£o durante picos

4. **Observabilidade**:
   - Todas as mÃ©tricas visÃ­veis no Prometheus
   - Dashboards Grafana funcionais
   - CorrelaÃ§Ã£o entre eventos (HPA scale â†” latÃªncia)

---

## ğŸ“š DocumentaÃ§Ã£o de ReferÃªncia

- **Kubernetes Autoscaling**: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
- **Prometheus Operator**: https://prometheus-operator.dev/
- **gRPC Basics**: https://grpc.io/docs/what-is-grpc/introduction/
- **k6 Documentation**: https://k6.io/docs/
- **Protocol Buffers**: https://protobuf.dev/

```

QUICKSTART.md
```
# Guia RÃ¡pido de ExecuÃ§Ã£o

## ğŸš€ Setup (executar 1 vez)

### 1. Criar Cluster Kubernetes
```bash
# Criar cluster Minikube
minikube start --nodes 3 --cpus 4 --memory 8192

# Habilitar addons necessÃ¡rios
minikube addons enable metrics-server
minikube addons enable ingress

# Verificar nodes
kubectl get nodes
```
**Tempo**: ~5 minutos  
**Resultado**: 1 control-plane + 2 workers

### 2. Instalar Prometheus Stack (opcional)
```bash
# Adicionar repositÃ³rio Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Instalar Prometheus + Grafana
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
```
**Tempo**: ~5 minutos  
**Opcional**: Pode pular este passo se nÃ£o precisar de monitoramento

> â„¹ï¸ **Avisos normais durante instalaÃ§Ã£o**:
> - `Warning: unrecognized format "int32"/"int64"` - Avisos cosmÃ©ticos, pode ignorar
> - `Warning: spec.SessionAffinity is ignored` - Comportamento esperado de headless services
> - Se aparecer `STATUS: deployed` no final, instalaÃ§Ã£o foi bem-sucedida! âœ…

**Verificar instalaÃ§Ã£o**:
```bash
# Aguardar pods ficarem prontos (~2-3 min)
kubectl get pods -n monitoring

# Todos devem estar Running/Completed
```

### 3. Deploy da AplicaÃ§Ã£o
```bash
# Build das imagens (dentro do contexto Docker do Minikube)
eval $(minikube -p minikube docker-env)
docker build -t a-py:latest ./services/a_py
docker build -t b-py:latest ./services/b_py
docker build -t p-node:latest ./gateway_p_node

# Deploy dos serviÃ§os
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/
kubectl apply -f k8s/monitoring/  # Apenas se instalou Prometheus
```
**Tempo**: ~3 minutos  
**Verificar**: `kubectl get pods -n pspd` (todos devem estar `Running`)

---

## ğŸ§ª Executar Testes de Carga

### OpÃ§Ã£o 1: Testes RÃ¡pidos em CenÃ¡rio Ãšnico
```bash
# Executar 4 testes k6 no cenÃ¡rio atual (baseline, ramp, spike, soak)
./scripts/run_all_tests.sh all
```
**O que faz**: Executa baseline, ramp, spike, soak no cenÃ¡rio deployado  
**Tempo**: ~20 minutos  
**Resultados**: `results/plots/*.png`

**Testes individuais**:
```bash
./scripts/run_all_tests.sh baseline  # Apenas baseline
./scripts/run_all_tests.sh spike     # Apenas spike
./scripts/run_all_tests.sh monitor   # Monitor em tempo real
```

### OpÃ§Ã£o 2: AnÃ¡lise Comparativa Completa (5 CenÃ¡rios)
```bash
# Executa TODOS os 5 cenÃ¡rios com 4 testes cada = 20 execuÃ§Ãµes
./test/run_all_scenarios.sh
```
**O que faz**: 
- Setup do CenÃ¡rio 1 â†’ 4 testes â†’ Coleta mÃ©tricas
- Setup do CenÃ¡rio 2 â†’ 4 testes â†’ Coleta mÃ©tricas
- ... repete para todos os 5 cenÃ¡rios

**Tempo**: 2-3 horas  
**Resultados**: `test_results/scenario_*/*.png`

**Gerar comparaÃ§Ã£o entre cenÃ¡rios**:
```bash
./scripts/run_scenario_comparison.sh --all
```
**Resultados**: `test_results/scenario-comparison/*.png`

---

## ğŸ“Š Acessar Monitoramento

> âš ï¸ **Importante**: Os serviÃ§os estÃ£o dentro do cluster (ClusterIP), nÃ£o expostos externamente.  
> VocÃª precisa fazer **port-forward** para acessÃ¡-los do seu navegador.

### Grafana
```bash
# Em um terminal separado (deixe rodando)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```
Acesse: http://localhost:3000  
Login: **admin**
senha: **admin**

**Caso precise recuperar senha**:
```bash
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

> ğŸ’¡ A senha Ã© gerada aleatoriamente durante a instalaÃ§Ã£o do Helm.  
> Se esquecer, use o comando acima para recuperÃ¡-la.

### Prometheus
```bash
# Em outro terminal separado (deixe rodando)
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
```
Acesse: http://localhost:9090  
Ir em: **Status â†’ Targets** (verificar se `serviceMonitor/pspd/*` estÃ£o UP)

### Atalho: Abrir ambos em background
```bash
# Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 &

# Prometheus  
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &

# Para parar depois:
pkill -f "port-forward.*monitoring"
```

---

## ğŸ“Š Visualizar MÃ©tricas e Dashboards

### Guia Completo Passo a Passo

ğŸ“– **[VISUALIZAR_METRICAS.md](./VISUALIZAR_METRICAS.md)** - Guia detalhado com screenshots e troubleshooting

### Acesso RÃ¡pido

**Prometheus** (mÃ©tricas brutas):
```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# â†’ http://localhost:9090
```

**Grafana** (dashboards visuais):
```bash
# Port-forward
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# â†’ http://localhost:3000

# Recuperar senha
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d
# User: admin
```

**Dashboard customizado**: Importar `k8s/monitoring/grafana-dashboard.json`

---

## ğŸ“ˆ Queries Prometheus Essenciais

Copie e cole no Prometheus (aba Graph):

```promql
# Taxa de requisiÃ§Ãµes HTTP (req/s)
rate(http_requests_total{container="p"}[1m])

# LatÃªncia P95 do Gateway P
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{container="p"}[1m]))

# LatÃªncia P95 do Service A
histogram_quantile(0.95, rate(grpc_server_request_duration_seconds_bucket{container="a"}[1m]))

# Taxa de erros
rate(http_requests_total{container="p",status_code=~"5.."}[1m])

# Chamadas gRPC por segundo
rate(grpc_client_requests_total{container="p"}[1m])
```

---

## ğŸ§¹ Comandos Ãšteis

### Ver status
```bash
# Cluster
minikube status
kubectl get nodes

# Pods
kubectl get pods -n pspd
kubectl get hpa -n pspd
kubectl top pods -n pspd

# Logs
kubectl logs -n pspd -l app=p --tail=50
```

### Testar manualmente
```bash
# Port-forward do Gateway
kubectl port-forward -n pspd svc/p-svc 8080:80

# Fazer requisiÃ§Ãµes
curl "http://localhost:8080/api/content?type=all&limit=10"
curl "http://localhost:8080/api/content?type=movies&limit=5"
curl "http://localhost:8080/api/metadata/m1?userId=teste"
curl "http://localhost:8080/api/browse?type=series&limit=3"

# Ver mÃ©tricas direto
kubectl port-forward -n pspd svc/a-svc 9101:9101
curl http://localhost:9101/metrics | grep grpc_server
```

### Limpar tudo
```bash
# Deletar aplicaÃ§Ã£o
kubectl delete namespace pspd

# Parar cluster
minikube stop

# Deletar cluster
minikube delete
```

---

## ğŸ› SoluÃ§Ã£o de Problemas

### Pod nÃ£o inicia
```bash
kubectl describe pod -n pspd <nome-do-pod>
kubectl logs -n pspd <nome-do-pod>
```

### HPA nÃ£o escala
```bash
kubectl describe hpa -n pspd a-hpa
kubectl top pods -n pspd  # Ver se metrics-server estÃ¡ funcionando
```

### Port-forward falha (porta ocupada)
```bash
pkill -f "port-forward"  # Mata todos os port-forwards
```

### MÃ©tricas nÃ£o aparecem no Prometheus
```bash
# 1. Verificar ServiceMonitors
kubectl get servicemonitor -n pspd

# 2. Testar endpoint
kubectl exec -n pspd <pod-name> -- curl localhost:9101/metrics

# 3. Ver targets no Prometheus
# http://localhost:9090/targets â†’ procurar "pspd"
```

### Avisos durante instalaÃ§Ã£o do Helm
```
Warning: unrecognized format "int32"/"int64"
Warning: spec.SessionAffinity is ignored
```
**SoluÃ§Ã£o**: Ignorar completamente! SÃ£o avisos cosmÃ©ticos que nÃ£o afetam o funcionamento.  
**Verificar sucesso**: Se aparecer `STATUS: deployed`, instalaÃ§Ã£o foi bem-sucedida âœ…

---

## ğŸ“ Estrutura de Resultados

```
results/                           # Testes bÃ¡sicos
â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ output.txt
â”‚   â”œâ”€â”€ pod-metrics-pre.txt
â”‚   â””â”€â”€ hpa-status-post.txt
â”œâ”€â”€ ramp/
â”œâ”€â”€ spike/
â”œâ”€â”€ soak/
â””â”€â”€ plots/                         # 6 grÃ¡ficos gerados
    â”œâ”€â”€ 01_latency_comparison.png
    â”œâ”€â”€ 02_throughput_comparison.png
    â”œâ”€â”€ 03_success_rate.png
    â”œâ”€â”€ 04_hpa_scaling.png
    â”œâ”€â”€ 05_resource_usage.png
    â””â”€â”€ 06_latency_percentiles.png

scenario-comparison/               # AnÃ¡lise comparativa
â”œâ”€â”€ 01_scenario_latency_comparison.png
â”œâ”€â”€ 02_scenario_throughput_comparison.png
â”œâ”€â”€ 03_scenario_hpa_scaling.png
â”œâ”€â”€ 04_scenario_success_rate.png
â”œâ”€â”€ 05_scenario_cost_analysis.png
â”œâ”€â”€ 06_scenario_performance_radar.png
â””â”€â”€ SCENARIO_COMPARISON_REPORT.txt

results-scenario-1-base/          # Resultados por cenÃ¡rio
results-scenario-2-replicas/
results-scenario-3-distribution/
results-scenario-4-resources/
results-scenario-5-no-hpa/
```

---

## ğŸ“š DocumentaÃ§Ã£o Detalhada

- **README.md** - VisÃ£o geral e comandos principais
- **docs/METRICAS_PROMETHEUS.md** - Todas as mÃ©tricas detalhadas
- **k8s/scenarios/README.md** - ConfiguraÃ§Ã£o dos 5 cenÃ¡rios
- **scenario-comparison/README.md** - Como interpretar os grÃ¡ficos

```

gerar_documento.py
```
import os
from datetime import datetime

# Caminho base da pasta
base_dir = "/home/edilberto/pspd/atividade-final-pspd"

# Gera o nome do arquivo com data e hora atuais
timestamp = datetime.now().strftime("%d%b%Y_%Hh%Mm").lower()
output_file = os.path.join(base_dir, f"codigos_{timestamp}.txt")

# Pastas e arquivos a serem ignorados
ignore_dirs = {".ipynb_checkpoints", "csv_collected", "data", "venv", ".git", "node_modules", ".next", "ui", "docs"}

# Wrap os.walk to filter out unwanted files:
_original_os_walk = os.walk
def _filtered_walk(top, topdown=True, onerror=None, followlinks=False):
    for root, dirs, files in _original_os_walk(top, topdown=topdown, onerror=onerror, followlinks=followlinks):
        # Exclude:
        # - any .txt file (case-insensitive) except SUMMARY_REPORT.txt
        # - any .json file inside a directory named "test_results" (case-insensitive)
        root_parts = [p.lower() for p in root.split(os.sep)]
        filtered_files = []
        for f in files:
            lf = f.lower()
            if lf.endswith(".txt") and lf != "summary_report.txt":
                continue
            if lf.endswith(".json") and "test_results" in root_parts:
                continue
            filtered_files.append(f)
        yield root, dirs, filtered_files

os.walk = _filtered_walk
ignore_files = {"readme.md", "run_load_tests.sh", "base_bronze.csv", "base_de_dados_prata.csv", "airbnb.ipynb", ".gitignore", ".dockerignore", ".gitkeep", "metrics.json"}

with open(output_file, "w", encoding="utf-8") as outfile:
    for root, dirs, files in os.walk(base_dir):
        # Remove as pastas ignoradas da varredura
        dirs[:] = [d for d in dirs if d not in ignore_dirs]

        for file in files:
            # Ignora arquivos especÃ­ficos (case-insensitive)
            if file.lower() in ignore_files:
                continue

            file_path = os.path.join(root, file)
            relative_path = os.path.relpath(file_path, base_dir)

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            except UnicodeDecodeError:
                # Ignora arquivos binÃ¡rios ou nÃ£o-texto
                continue

            outfile.write(f"{relative_path}\n```\n{content}\n```\n\n")

print(f"âœ… Arquivo gerado com sucesso: {output_file}")

```

VISUALIZAR_METRICAS.md
```
# ğŸ“Š Como Visualizar MÃ©tricas e Dashboards

Guia passo a passo para acessar Prometheus, Grafana e visualizar as mÃ©tricas coletadas.

---

## ğŸ¯ PrÃ©-requisitos

Certifique-se de que:
- âœ… Cluster Kubernetes estÃ¡ rodando (`minikube status`)
- âœ… Prometheus estÃ¡ instalado (`kubectl get pods -n monitoring`)
- âœ… AplicaÃ§Ã£o estÃ¡ deployada (`kubectl get pods -n pspd`)
- âœ… ServiceMonitors estÃ£o configurados (`kubectl get servicemonitor -n pspd`)

---

## âš ï¸ IMPORTANTE: GeraÃ§Ã£o de MÃ©tricas

**As mÃ©tricas sÃ³ aparecem quando hÃ¡ trÃ¡fego na aplicaÃ§Ã£o!**

- ğŸ“Š **Prometheus coleta mÃ©tricas**, mas se ninguÃ©m estÃ¡ fazendo requisiÃ§Ãµes, os valores ficam zerados ou inexistentes
- ğŸš€ **Para visualizar dados reais**: execute testes de carga ou faÃ§a requisiÃ§Ãµes manuais
- â±ï¸ **Tempo de atualizaÃ§Ã£o**: Prometheus faz scrape a cada 15-30 segundos

**Formas de gerar trÃ¡fego**:

1. **Testes de carga automatizados** (recomendado):
   ```bash
   k6 run load/spike.js       # Pico de trÃ¡fego (1min)
   k6 run load/baseline.js    # Carga constante (5min)
   k6 run load/soak.js        # Teste longo (10min)
   ```

2. **RequisiÃ§Ãµes manuais**:
   ```bash
   # Abrir acesso ao Gateway
   kubectl port-forward -n pspd svc/p-svc 8080:80
   
   # Fazer requisiÃ§Ãµes
   curl "http://localhost:8080/api/content?type=all"
   curl "http://localhost:8080/api/metadata/m1"
   curl "http://localhost:8080/api/browse?type=movies"
   ```

3. **Loop simples** (para testes):
   ```bash
   kubectl port-forward -n pspd svc/p-svc 8080:80 &
   while true; do curl -s "http://localhost:8080/api/content?type=all" > /dev/null; sleep 1; done
   ```

**ApÃ³s gerar trÃ¡fego, aguarde 15-30 segundos** para as mÃ©tricas aparecerem no Prometheus/Grafana.

---

## ğŸ“ˆ OpÃ§Ã£o 1: Prometheus (VisualizaÃ§Ã£o de MÃ©tricas Brutas)

### Passo 1: Iniciar Port-Forward do Prometheus

Abra um terminal e execute:

```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
```

**Deixe este terminal aberto!** O comando ficarÃ¡ rodando.

### Passo 2: Acessar Interface Web

Abra seu navegador e acesse:
```
http://localhost:9090
```

### Passo 3: Verificar Targets (ServiÃ§os Monitorados)

1. No Prometheus, clique em **Status** â†’ **Targets**
2. Procure pela seÃ§Ã£o **`serviceMonitor/pspd/...`**
3. VocÃª deve ver **3 targets UP** (verde):
   - `serviceMonitor/pspd/service-a-monitor/0` â†’ `10.x.x.x:9101`
   - `serviceMonitor/pspd/service-b-monitor/0` â†’ `10.x.x.x:9102`
   - `serviceMonitor/pspd/gateway-p-monitor/0` â†’ `10.x.x.x:8080`

#### âš ï¸ Se NÃƒO aparecer nenhum target com `serviceMonitor/pspd`

**Causa**: ServiceMonitors nÃ£o foram criados ou Prometheus nÃ£o os descobriu ainda.

**SoluÃ§Ã£o**:
```bash
# 1. Verificar se ServiceMonitors existem
kubectl get servicemonitor -n pspd

# Se retornar "No resources found":
# 2. Criar os ServiceMonitors
kubectl containerly -f k8s/servicemonitors.yaml

# 3. Aguardar 15-30 segundos e recarregar pÃ¡gina do Prometheus
# 4. Verificar se apareceram em Status â†’ Targets
```

#### ğŸ”´ Se targets aparecem mas estÃ£o DOWN (vermelho)

**Causa**: Pods nÃ£o estÃ£o rodando ou nÃ£o estÃ£o expondo mÃ©tricas corretamente.

**SoluÃ§Ã£o**:
```bash
# 1. Verificar se pods estÃ£o Running
kubectl get pods -n pspd

# Se algum pod NÃƒO estÃ¡ Running:
# 2. Ver logs do pod com problema
kubectl logs -n pspd -l container=a  # para Service A
kubectl logs -n pspd -l container=b  # para Service B
kubectl logs -n pspd -l container=p  # para Gateway P

# 3. Reconstruir imagens e reiniciar pods
eval $(minikube -p minikube docker-env)
docker build -t a-service:local ./services/a_py
docker build -t b-service:local ./services/b_py
docker build -t p-gateway:local ./gateway_p_node

kubectl delete pod --all -n pspd
kubectl wait --for=condition=ready pod --all -n pspd --timeout=60s

# 4. Aguardar 15-30 segundos e verificar targets novamente
```

#### âœ… Verificar se mÃ©tricas estÃ£o sendo expostas

```bash
# Testar endpoint de mÃ©tricas diretamente
kubectl exec -n pspd deploy/a-deploy -- python3 -c "import urllib.request; print(urllib.request.urlopen('http://localhost:9101/metrics').read().decode()[:500])"

# Se retornar erro, o servidor de mÃ©tricas nÃ£o estÃ¡ rodando
# Verifique os logs do pod para identificar o problema
```

### Passo 4: Explorar MÃ©tricas

**âš ï¸ LEMBRE-SE**: As queries abaixo sÃ³ retornarÃ£o dados se houver trÃ¡fego na aplicaÃ§Ã£o!  
**Execute um teste de carga primeiro** (veja seÃ§Ã£o "Gerar MÃ©tricas com Testes de Carga" abaixo).

No **Graph** (aba superior), teste estas queries:

#### RequisiÃ§Ãµes HTTP por segundo (Gateway P)
```promql
rate(http_requests_total{container="p"}[1m])
```

#### LatÃªncia P95 do Gateway
```promql
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{container="p"}[1m]))
```

#### RequisiÃ§Ãµes gRPC do Service A
```promql
rate(grpc_server_requests_total{container="a"}[1m])
```

#### Itens streamed pelo Service B
```promql
rate(grpc_server_stream_items_total{container="b"}[1m])
```

#### CPU dos Pods
```promql
rate(container_cpu_usage_seconds_total{namespace="pspd"}[1m])
```

#### RÃ©plicas HPA
```promql
kube_horizontalpodautoscaler_status_current_replicas{namespace="pspd"}
```

**Dica**: Clique em **Execute** e depois em **Graph** para ver o grÃ¡fico!

---

## ğŸ¨ OpÃ§Ã£o 2: Grafana (Dashboards Visuais)

### Passo 1: Iniciar Port-Forward do Grafana

Abra um **novo terminal** e execute:

```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

**Deixe este terminal aberto!**

### Passo 2: Recuperar Senha do Grafana

Em outro terminal, execute:

```bash
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

Copie a senha que aparecer (algo como: `RnoVJN3Y4KyzMfmgwExIzqiIXq90jEtgrqLNmBjb`)

### Passo 3: Fazer Login no Grafana

1. Abra seu navegador: http://localhost:3000
2. **UsuÃ¡rio**: `admin`
3. **Senha**: Cole a senha copiada no passo anterior
4. Clique em **Log in**

### Passo 4: Explorar Dashboards PrÃ©-instalados

No menu lateral, clique em **â˜°** â†’ **Dashboards**

Dashboards Ãºteis para o projeto:

#### 1. **Kubernetes / Compute Resources / Namespace (Pods)**
- Mostra CPU e MemÃ³ria de todos os pods do namespace `pspd`
- Para ver suas mÃ©tricas: filtro superior â†’ namespace: `pspd`

#### 2. **Kubernetes / Compute Resources / Pod**
- MÃ©tricas detalhadas de um pod especÃ­fico
- Escolha pod: `a-deploy-xxx`, `b-deploy-xxx` ou `p-deploy-xxx`

#### 3. **Node Exporter / Nodes**
- MÃ©tricas dos nÃ³s do cluster
- CPU, memÃ³ria, disco, rede

### Passo 5: Importar Dashboard Customizado da AplicaÃ§Ã£o

1. No Grafana, clique em **â˜°** â†’ **Dashboards** â†’ **Import**
2. Clique em **Upload JSON file**
3. Selecione: `/home/edilberto/pspd/atividade-final-pspd/k8s/monitoring/grafana-dashboard.json`
4. Em **Prometheus**, selecione: **Prometheus** (deve ser a Ãºnica opÃ§Ã£o)
5. Clique em **Import**

**Dashboard inclui**:
- Taxa de requisiÃ§Ãµes HTTP
- LatÃªncia (P50, P95, P99)
- Taxa de erros
- NÃºmero de rÃ©plicas (HPA)
- CPU/MemÃ³ria por pod
- Throughput gRPC

### Passo 6: Criar Painel Customizado

1. Clique em **â˜°** â†’ **Dashboards** â†’ **New** â†’ **New Dashboard**
2. Clique em **Add visualization**
3. Selecione **Prometheus** como data source
4. Cole uma query PromQL (exemplos acima)
5. Configure:
   - **Title**: Nome descritivo
   - **Legend**: `{{pod}}` ou `{{container}}`
   - **Unit**: Escolha apropriada (req/s, ms, bytes, etc.)
6. Clique em **containerly**
7. **Save dashboard** (Ã­cone de disquete no topo)

---

## ğŸ§ª Gerar MÃ©tricas com Testes de Carga

### OpÃ§Ã£o 1: Testes de Carga Automatizados (K6)

Para ver as mÃ©tricas em aÃ§Ã£o, execute testes de carga:

#### Terminal 1: Monitoramento em tempo real
```bash
./scripts/run_all_tests.sh monitor
```

#### Terminal 2: Port-forward da aplicaÃ§Ã£o
```bash
kubectl port-forward -n pspd svc/p-svc 8080:80
```

#### Terminal 3: Executar teste
```bash
BASE_URL=http://localhost:8080 ./scripts/run_all_tests.sh spike
```

Agora volte para **Grafana** ou **Prometheus** e veja as mÃ©tricas subindo em tempo real! ğŸ“ˆ

---

### OpÃ§Ã£o 2: Frontend Local + Backend Local (Fluxo Completo)

Execute a aplicaÃ§Ã£o frontend Next.js localmente conectada ao backend Kubernetes e observe mÃ©tricas em tempo real.

#### Passo 1: Preparar Ambiente

```bash
# Verificar se cluster estÃ¡ rodando
minikube status

# Verificar se aplicaÃ§Ã£o estÃ¡ deployada
kubectl get pods -n pspd
# Deve mostrar: a-deploy, b-deploy, p-deploy (todos Running)
```

#### Passo 2: Configurar Frontend

```bash
# No Windows, navegue para a pasta do frontend
# C:\Users\edilb\OneDrive\Documentos\streaming-container-design

# Criar arquivo de configuraÃ§Ã£o .env.local
# No PowerShell ou CMD:
echo NEXT_PUBLIC_API_URL=http://localhost:8081 > .env.local

# Instalar dependÃªncias (se ainda nÃ£o instalou)
npm install
```

**âš ï¸ IMPORTANTE**: Estamos usando porta `8081` para evitar conflito com outras aplicaÃ§Ãµes na porta 8080.

#### Passo 3: Iniciar Todos os ServiÃ§os

Abra **4 terminais WSL (Ubuntu)** e execute em cada um:

**Terminal 1 - Backend (Port-forward)**:
```bash
cd /home/edilberto/pspd/atividade-final-pspd
kubectl port-forward -n pspd svc/p-svc 8081:80
```
**Deixe este terminal aberto e rodando!** VocÃª verÃ¡ mensagens como "Handling connection for 8081" quando o frontend fizer requisiÃ§Ãµes.

**Terminal 2 - Prometheus**:
```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
```

**Terminal 3 - Grafana**:
```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

**Terminal 4 - Frontend (Next.js no Windows)**:
```powershell
# No PowerShell ou CMD do Windows
cd C:\Users\edilb\OneDrive\Documentos\streaming-container-design
npm run dev
```


#### Passo 4: Acessar AplicaÃ§Ãµes

Aguarde ~10 segundos para todos os serviÃ§os iniciarem, depois acesse:

1. **Frontend Next.js**: http://localhost:3001
   - Login: `emailcontateste@dominio.com`
   - Senha: `1234567890`

2. **Backend API**: http://localhost:8081
   - Teste: `curl http://localhost:8081/api/content?type=all`

3. **Prometheus**: http://localhost:9090
   - VÃ¡ em Graph e execute queries PromQL

4. **Grafana**: http://localhost:3000
   - Login: `admin` / senha do secret (veja Passo 2 da seÃ§Ã£o Grafana acima)

#### Passo 5: Navegar no Frontend e Observar MÃ©tricas

1. **No Frontend** (http://localhost:3001):
   - FaÃ§a login
   - Clique em **Browse** â†’ **Movies**
   - Clique em **Browse** â†’ **Series**
   - Clique em **Browse** â†’ **Live TV**
   - Abra detalhes de alguns conteÃºdos

2. **No Prometheus** (http://localhost:9090/graph):
   
   Execute esta query e veja os dados atualizando:
   ```promql
   # Taxa de requisiÃ§Ãµes em tempo real
   rate(http_requests_total{container="p"}[30s])
   
   # OU usando job (nome do serviÃ§o)
   rate(http_requests_total{job="p-svc"}[1m])
   ```
   
   Outras queries Ãºteis:
   ```promql
   # RequisiÃ§Ãµes por endpoint
   sum by (route) (rate(http_requests_total{container="p"}[1m]))
   
   # LatÃªncia P95
   histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{container="p"}[1m]))
   
   # Total de requisiÃ§Ãµes
   http_requests_total{container="p"}
   ```

   **âš ï¸ IMPORTANTE**: Use `container="p"` ou `job="p-svc"`, NÃƒO `container="p"`!  
   O Prometheus usa labels diferentes dos labels do Kubernetes.

3. **No Grafana** (http://localhost:3000):
   - Importe o dashboard: `k8s/monitoring/grafana-dashboard.json`
   - Veja grÃ¡ficos atualizando conforme vocÃª navega no frontend

#### Passo 6: Observar Logs em Tempo Real (Opcional)

Em um **5Âº terminal**, acompanhe os logs do Gateway P:

```bash
kubectl logs -n pspd -l container=p -f
```

VocÃª verÃ¡ cada requisiÃ§Ã£o sendo processada quando navega no frontend:
```
GET /api/content?type=movies - 200 OK - 145ms
GET /api/metadata/m1 - 200 OK - 89ms
GET /api/browse?type=series - 200 OK - 123ms
```

#### Passo 7: Verificar MÃ©tricas EspecÃ­ficas

**Consultas Ãºteis no Prometheus**:

```promql
# Total de requisiÃ§Ãµes nos Ãºltimos 5 minutos
sum(increase(http_requests_total{container="p"}[5m]))

# Endpoints mais acessados
topk(5, sum by (route) (rate(http_requests_total{container="p"}[5m])))

# LatÃªncia mÃ©dia por endpoint
avg by (route) (rate(http_request_duration_seconds_sum{container="p"}[1m]) / rate(http_request_duration_seconds_count{container="p"}[1m]))

# CPU do Gateway P
rate(container_cpu_usage_seconds_total{namespace="pspd",pod=~"p-deploy.*"}[1m])
```

#### ğŸ¯ Fluxo Completo: Frontend â†’ Backend â†’ MÃ©tricas

```
VocÃª navega no Frontend (localhost:3001)
         â†“
Frontend faz fetch('/api/content')
         â†“
RequisiÃ§Ã£o HTTP â†’ Backend Gateway P (localhost:8081)
         â†“
Gateway P converte HTTP â†’ gRPC
         â†“
Service A (catÃ¡logo) ou Service B (metadata) responde
         â†“
Gateway P incrementa mÃ©tricas Prometheus
         â†“
Prometheus faz scrape das mÃ©tricas (a cada 15s)
         â†“
VocÃª vÃª mÃ©tricas atualizando no Prometheus/Grafana!
```

#### ğŸ› Troubleshooting Frontend Local

**Problema: "TypeError: clientA.GetContent is not a function"**

**Causa**: O arquivo `gateway_p_node/proto/services.proto` estÃ¡ desatualizado ou incorreto.

**SoluÃ§Ã£o**:
```bash
# 1. Copiar proto correto
cp /home/edilberto/pspd/atividade-final-pspd/proto/services.proto \
   /home/edilberto/pspd/atividade-final-pspd/gateway_p_node/proto/services.proto

# 2. Rebuild imagem do Gateway P
eval $(minikube docker-env)
docker build -t p-gateway:local ./gateway_p_node

# 3. Reiniciar pod
kubectl delete pod -n pspd -l container=p
kubectl wait --for=condition=ready pod -n pspd -l container=p --timeout=60s
```

**Problema: Frontend nÃ£o conecta ao backend**

Verifique:
```bash
# 1. Backend estÃ¡ acessÃ­vel?
curl http://localhost:8081/api/content?type=all

# 2. Arquivo .env.local existe? (no Windows)
# No PowerShell:
cat C:\Users\edilb\OneDrive\Documentos\streaming-container-design\.env.local
# Deve mostrar: NEXT_PUBLIC_API_URL=http://localhost:8081

# 3. Next.js estÃ¡ usando a variÃ¡vel?
# No navegador, abra DevTools â†’ Console e execute:
# console.log(process.env.NEXT_PUBLIC_API_URL)
```

**Problema: CORS error no navegador**

Se aparecer erro de CORS no console do navegador, adicione CORS no Gateway P:

```bash
# Ver se jÃ¡ tem CORS configurado
kubectl logs -n pspd -l container=p | grep -i cors

# Se necessÃ¡rio, edite gateway_p_node/server.js e adicione:
# container.use(cors({ origin: 'http://localhost:3001' }))
# Depois rebuild: docker build -t p-gateway:local ./gateway_p_node
# E restart: kubectl delete pod -n pspd -l container=p
```

**Problema: MÃ©tricas nÃ£o aparecem**

```bash
# Verificar se estÃ¡ gerando trÃ¡fego
kubectl logs -n pspd -l container=p --tail=20

# Deve mostrar logs de requisiÃ§Ãµes HTTP quando vocÃª navega
# Se nÃ£o aparecer, o frontend nÃ£o estÃ¡ fazendo requisiÃ§Ãµes ao backend
```

#### âœ… Checklist Frontend Local

- [ ] Port-forward do backend rodando (porta 8081 â†’ serviÃ§o porta 80)
- [ ] Terminal mostra "Handling connection for 8081" quando frontend faz requisiÃ§Ãµes
- [ ] Frontend Next.js rodando (porta 3001 no Windows)
- [ ] `.env.local` configurado com `NEXT_PUBLIC_API_URL=http://localhost:8081`
- [ ] Prometheus acessÃ­vel (porta 9090)
- [ ] Grafana acessÃ­vel (porta 3000)
- [ ] Login no frontend funciona
- [ ] NavegaÃ§Ã£o carrega conteÃºdos do backend
- [ ] Logs do Gateway P mostram requisiÃ§Ãµes (veja Terminal 1)
- [ ] MÃ©tricas aparecem no Prometheus
- [ ] Dashboard Grafana atualiza em tempo real

**Se todos os itens estÃ£o âœ…, o fluxo completo estÃ¡ funcionando!** ğŸ‰

---

## ğŸ” Verificar se MÃ©tricas EstÃ£o Sendo Coletadas

### MÃ©todo 1: Via Port-Forward Direto nos Pods

```bash
# Service A (porta 9101)
kubectl port-forward -n pspd svc/a-svc 9101:9101
curl http://localhost:9101/metrics | grep grpc_server

# Service B (porta 9102)
kubectl port-forward -n pspd svc/b-svc 9102:9102
curl http://localhost:9102/metrics | grep grpc_server

# Gateway P (porta 8080)
kubectl port-forward -n pspd svc/p-svc 8080:80
curl http://localhost:8080/metrics | grep http_requests
```

Deve aparecer algo como:
```
grpc_server_requests_total{method="GetContent",status="success"} 42.0
http_requests_total{method="GET",route="/api/content",status_code="200"} 156.0
```

### MÃ©todo 2: Via Prometheus Targets

1. Acesse Prometheus: http://localhost:9090/targets
2. Procure por `serviceMonitor/pspd`
3. Verifique:
   - **State**: UP (verde) âœ…
   - **Last Scrape**: Recente (<1min)
   - **Scrape Duration**: Baixo (<100ms)

### MÃ©todo 3: Query no Prometheus

No Prometheus, execute:
```promql
up{namespace="pspd"}
```

Resultado esperado:
```
up{job="serviceMonitor/pspd/gateway-p-monitor/0", namespace="pspd"} = 1
up{job="serviceMonitor/pspd/service-a-monitor/0", namespace="pspd"} = 1
up{job="serviceMonitor/pspd/service-b-monitor/0", namespace="pspd"} = 1
```

**1 = UP âœ… | 0 = DOWN âŒ**

---

## ğŸ“Š Queries PromQL Ãšteis

### MÃ©tricas da AplicaÃ§Ã£o

#### Taxa de RequisiÃ§Ãµes HTTP
```promql
sum(rate(http_requests_total{namespace="pspd"}[1m])) by (container, route)
```

#### LatÃªncia P95 por Endpoint
```promql
histogram_quantile(0.95, 
  sum(rate(http_request_duration_seconds_bucket{namespace="pspd"}[5m])) 
  by (le, route)
)
```

#### Taxa de Erro (HTTP 5xx)
```promql
sum(rate(http_requests_total{namespace="pspd",status_code=~"5.."}[1m])) 
/ 
sum(rate(http_requests_total{namespace="pspd"}[1m])) * 100
```

#### gRPC Request Rate (Service A)
```promql
rate(grpc_server_requests_total{container="a",status="success"}[1m])
```

#### Streaming Items/s (Service B)
```promql
rate(grpc_server_stream_items_total{container="b"}[1m])
```

### MÃ©tricas de Infraestrutura

#### CPU por Pod
```promql
sum(rate(container_cpu_usage_seconds_total{namespace="pspd"}[1m])) by (pod)
```

#### MemÃ³ria por Pod
```promql
sum(container_memory_working_set_bytes{namespace="pspd"}) by (pod) / 1024 / 1024
```

#### RÃ©plicas Atual vs Desejado (HPA)
```promql
kube_horizontalpodautoscaler_status_current_replicas{namespace="pspd"}
kube_horizontalpodautoscaler_spec_max_replicas{namespace="pspd"}
```

#### Network In/Out
```promql
rate(container_network_receive_bytes_total{namespace="pspd"}[1m])
rate(container_network_transmit_bytes_total{namespace="pspd"}[1m])
```

---

## ğŸ› Troubleshooting

### Problema: "No data points" no Grafana

**Causa**: MÃ©tricas ainda nÃ£o foram geradas (aplicaÃ§Ã£o nÃ£o recebeu trÃ¡fego)

**SoluÃ§Ã£o - OpÃ§Ã£o 1: Executar teste de carga** (recomendado):
```bash
# Teste rÃ¡pido de 1 minuto
k6 run load/spike.js

# OU teste baseline de 5 minutos
k6 run load/baseline.js
```

**SoluÃ§Ã£o - OpÃ§Ã£o 2: Gerar trÃ¡fego manual**:
```bash
# Terminal 1: Abrir acesso
kubectl port-forward -n pspd svc/p-svc 8080:80

# Terminal 2: Fazer vÃ¡rias requisiÃ§Ãµes
for i in {1..50}; do
  curl -s "http://localhost:8080/api/content?type=all" > /dev/null
  curl -s "http://localhost:8080/api/metadata/m$i" > /dev/null
  curl -s "http://localhost:8080/api/browse?type=movies" > /dev/null
done

# Aguardar 15-30 segundos para Prometheus fazer scrape
```

### Problema: Targets DOWN no Prometheus

**Verificar pods**:
```bash
kubectl get pods -n pspd
```

**Se pods nÃ£o estÃ£o Running**:
```bash
# Ver logs
kubectl logs -n pspd -l container=a

# Reiniciar
kubectl delete pod --all -n pspd
kubectl wait --for=condition=ready pod --all -n pspd --timeout=60s
```

**Verificar ServiceMonitors**:
```bash
kubectl get servicemonitor -n pspd
kubectl describe servicemonitor service-a-monitor -n pspd
```

### Problema: Senha do Grafana nÃ£o funciona

**Resetar senha**:
```bash
# Deletar pod do Grafana para recriar
kubectl delete pod -n monitoring -l container.kubernetes.io/name=grafana

# Aguardar pod ficar pronto
kubectl wait --for=condition=ready pod -n monitoring -l container.kubernetes.io/name=grafana --timeout=60s

# Recuperar nova senha
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d
```

### Problema: Port-forward para ou cai

**Usar script estÃ¡vel** (mantÃ©m rodando):
```bash
./scripts/stable_port_forward.sh
```

Ou **manualmente** com loop:
```bash
while true; do
  kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
  echo "Port-forward caiu, reconectando em 5s..."
  sleep 5
done
```

---

## ğŸ“š PrÃ³ximos Passos

1. âœ… Acesse Prometheus: http://localhost:9090
2. âœ… Verifique targets estÃ£o UP: Status â†’ Targets
3. âœ… Acesse Grafana: http://localhost:3000 (admin + senha do secret)
4. âœ… Importe dashboard: `k8s/monitoring/grafana-dashboard.json`
5. âœ… Execute teste de carga: `./scripts/run_all_tests.sh spike`
6. âœ… Observe mÃ©tricas em tempo real no Grafana
7. âœ… Explore queries PromQL no Prometheus

---

## ğŸ¯ Checklist de ValidaÃ§Ã£o

- [ ] Prometheus acessÃ­vel em http://localhost:9090
- [ ] 3 targets UP no Prometheus (a, b, p)
- [ ] Grafana acessÃ­vel em http://localhost:3000
- [ ] Dashboard customizado importado
- [ ] MÃ©tricas aparecem apÃ³s gerar trÃ¡fego
- [ ] HPA scaling visÃ­vel nos dashboards
- [ ] LatÃªncia P95 < 200ms em baseline test

**Se todos os itens estiverem âœ…, seu monitoramento estÃ¡ 100% funcional!** ğŸ‰

```

k8s/p-nodeport.yaml
```
# Service NodePort para acesso direto sem port-forward
# Mais estÃ¡vel para testes de longa duraÃ§Ã£o
apiVersion: v1
kind: Service
metadata:
  name: p-svc-nodeport
  namespace: pspd
  labels:
    app: p
spec:
  type: NodePort
  selector:
    app: p
  ports:
  - name: http
    port: 80
    targetPort: 8080
    nodePort: 30080  # Porta fixa para facilitar testes


```

k8s/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
    spec:
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }

```

k8s/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
    spec:
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }

```

k8s/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
    spec:
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }

```

k8s/servicemonitors.yaml
```
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-a-monitor
  namespace: pspd
  labels:
    app: a
    release: prometheus
spec:
  selector:
    matchLabels:
      app: a
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-b-monitor
  namespace: pspd
  labels:
    app: b
    release: prometheus
spec:
  selector:
    matchLabels:
      app: b
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: gateway-p-monitor
  namespace: pspd
  labels:
    app: p
    release: prometheus
spec:
  selector:
    matchLabels:
      app: p
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics

```

k8s/ingress.yaml
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: p-ingress
  namespace: pspd
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
    - host: pspd.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: p-svc
                port: { number: 80 }

```

k8s/namespace.yaml
```
apiVersion: v1
kind: Namespace
metadata:
  name: pspd

```

k8s/scenarios/scenario1-base/b-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario1-base/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
    spec:
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }

```

k8s/scenarios/scenario1-base/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
    spec:
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }

```

k8s/scenarios/scenario1-base/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
    spec:
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }

```

k8s/scenarios/scenario1-base/a-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario1-base/p-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario2-replicas/b-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario2-replicas/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 2  # ALTERADO: 1 -> 2
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
        scenario: replicas
    spec:
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 2  # ALTERADO: 1 -> 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario2-replicas/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 2  # ALTERADO: 1 -> 2
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
        scenario: replicas
    spec:
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 2  # ALTERADO: 1 -> 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario2-replicas/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 2  # ALTERADO: 1 -> 2
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
        scenario: replicas
    spec:
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 2  # ALTERADO: 1 -> 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario2-replicas/a-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario2-replicas/p-hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30

```

k8s/scenarios/scenario3-distribution/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 3  # 3 rÃ©plicas para distribuir em 3 nodes
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
        scenario: distribution
    spec:
      # ALTERADO: Anti-affinity SOFT (preferencial, nÃ£o obrigatÃ³ria)
      # Tenta distribuir em nodes diferentes, mas permite no mesmo node se necessÃ¡rio
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - b
                topologyKey: kubernetes.io/hostname
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario3-distribution/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 3  # 3 rÃ©plicas para distribuir em 3 nodes
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
        scenario: distribution
    spec:
      # ALTERADO: Anti-affinity SOFT (preferencial, nÃ£o obrigatÃ³ria)
      # Tenta distribuir em nodes diferentes, mas permite no mesmo node se necessÃ¡rio
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - a
                topologyKey: kubernetes.io/hostname
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario3-distribution/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 3  # 3 rÃ©plicas para distribuir em 3 nodes
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
        scenario: distribution
    spec:
      # ALTERADO: Anti-affinity SOFT (preferencial, nÃ£o obrigatÃ³ria)
      # Tenta distribuir em nodes diferentes, mas permite no mesmo node se necessÃ¡rio
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - p
                topologyKey: kubernetes.io/hostname
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 3
  maxReplicas: 12
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/scenarios/scenario5-no-hpa/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 3  # FIXO - nÃ£o escalarÃ¡
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
        scenario: no-hpa
    spec:
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }

```

k8s/scenarios/scenario5-no-hpa/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 3  # FIXO - nÃ£o escalarÃ¡
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
        scenario: no-hpa
    spec:
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }

```

k8s/scenarios/scenario5-no-hpa/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 5  # FIXO - nÃ£o escalarÃ¡ (mais que a/b pois gateway recebe mais carga)
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
        scenario: no-hpa
    spec:
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }

```

k8s/scenarios/scenario4-resources/b.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: b } }
  template:
    metadata: 
      labels: 
        app: b
        version: v1
        scenario: resources
    spec:
      containers:
        - name: b
          image: b-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50052, name: grpc }
            - { containerPort: 9102, name: metrics }
          env:
            - { name: PORT, value: "50052" }
            - { name: METRICS_PORT, value: "9102" }
          resources:
            # ALTERADO: Recursos reduzidos (stress test)
            requests:
              cpu: "50m"
              memory: "64Mi"
            limits:
              cpu: "200m"
              memory: "128Mi"
          readinessProbe: { tcpSocket: { port: 50052 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50052 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-svc
  namespace: pspd
  labels:
    app: b
spec:
  selector: { app: b }
  ports: 
    - { port: 50052, targetPort: 50052, name: grpc }
    - { port: 9102, targetPort: 9102, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 1
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60

```

k8s/scenarios/scenario4-resources/a.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: a } }
  template:
    metadata: 
      labels: 
        app: a
        version: v1
        scenario: resources
    spec:
      containers:
        - name: a
          image: a-service:local
          imagePullPolicy: IfNotPresent
          ports: 
            - { containerPort: 50051, name: grpc }
            - { containerPort: 9101, name: metrics }
          env:
            - { name: PORT, value: "50051" }
            - { name: METRICS_PORT, value: "9101" }
          resources:
            # ALTERADO: Recursos reduzidos (stress test)
            requests:
              cpu: "50m"     # REDUZIDO: 100m -> 50m
              memory: "64Mi"  # REDUZIDO: 128Mi -> 64Mi
            limits:
              cpu: "200m"     # REDUZIDO: 500m -> 200m
              memory: "128Mi" # REDUZIDO: 256Mi -> 128Mi
          readinessProbe: { tcpSocket: { port: 50051 }, initialDelaySeconds: 2, periodSeconds: 5 }
          livenessProbe:  { tcpSocket: { port: 50051 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-svc
  namespace: pspd
  labels:
    app: a
spec:
  selector: { app: a }
  ports: 
    - { port: 50051, targetPort: 50051, name: grpc }
    - { port: 9101, targetPort: 9101, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 1
  maxReplicas: 8  # AUMENTADO para compensar recursos limitados
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60  # REDUZIDO: 70% -> 60%

```

k8s/scenarios/scenario4-resources/p.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: p } }
  template:
    metadata: 
      labels: 
        app: p
        version: v1
        scenario: resources
    spec:
      containers:
        - name: p
          image: p-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_ADDR, value: "a-svc.pspd.svc.cluster.local:50051" }
            - { name: B_ADDR, value: "b-svc.pspd.svc.cluster.local:50052" }
            - { name: PORT,   value: "8080" }
          ports: 
            - { containerPort: 8080, name: http }
          resources:
            # ALTERADO: Recursos reduzidos (stress test)
            requests:
              cpu: "50m"
              memory: "64Mi"
            limits:
              cpu: "200m"
              memory: "128Mi"
          readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 3, periodSeconds: 5 }
          livenessProbe:  { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 5, periodSeconds: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: p-svc
  namespace: pspd
  labels:
    app: p
spec:
  selector: { app: p }
  ports: 
    - { port: 80, targetPort: 8080, name: http }
    - { port: 8080, targetPort: 8080, name: metrics }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 1
  maxReplicas: 15  # AUMENTADO para compensar
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60

```

k8s/rest/p-rest.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: p-rest-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: p-rest } }
  template:
    metadata:
      labels: { app: p-rest }
    spec:
      containers:
        - name: p-rest
          image: p-rest-gateway:local
          imagePullPolicy: IfNotPresent
          env:
            - { name: A_REST, value: "http://a-rest-svc.pspd.svc.cluster.local:50061" }
            - { name: B_REST, value: "http://b-rest-svc.pspd.svc.cluster.local:50062" }
            - { name: PORT, value: "8081" }
          ports: [ { containerPort: 8081 } ]
          readinessProbe:
            tcpSocket: { port: 8081 }
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 10
          livenessProbe:
            tcpSocket: { port: 8081 }
            initialDelaySeconds: 15
            periodSeconds: 10
            failureThreshold: 10
---
apiVersion: v1
kind: Service
metadata:
  name: p-rest-svc
  namespace: pspd
spec:
  selector: { app: p-rest }
  ports: [ { port: 80, targetPort: 8081 } ]

```

k8s/rest/ingress-rest.yaml
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: p-rest-ingress
  namespace: pspd
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
    - host: pspd-rest.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: p-rest-svc
                port: { number: 80 }

```

k8s/rest/b-rest.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-rest-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: b-rest } }
  template:
    metadata: { labels: { app: b-rest } }
    spec:
      containers:
        - name: b-rest
          image: b-rest-service:local
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 8000 } ]
          readinessProbe: { httpGet: { path: /api/metadata/m1, port: 8000 }, initialDelaySeconds: 3, periodSeconds: 5, failureThreshold: 10 }
          livenessProbe:  { httpGet: { path: /api/metadata/m1, port: 8000 }, initialDelaySeconds: 5, periodSeconds: 10, failureThreshold: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: b-rest-svc
  namespace: pspd
spec:
  selector: { app: b-rest }
  ports: [ { port: 50062, targetPort: 8000 } ]

```

k8s/rest/a-rest.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-rest-deploy
  namespace: pspd
spec:
  replicas: 1
  selector: { matchLabels: { app: a-rest } }
  template:
    metadata: { labels: { app: a-rest } }
    spec:
      containers:
        - name: a-rest
          image: a-rest-service:local
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 8000 } ]
          readinessProbe: { httpGet: { path: /api/content, port: 8000 }, initialDelaySeconds: 3, periodSeconds: 5, failureThreshold: 10 }
          livenessProbe:  { httpGet: { path: /api/content, port: 8000 }, initialDelaySeconds: 5, periodSeconds: 10, failureThreshold: 10 }
---
apiVersion: v1
kind: Service
metadata:
  name: a-rest-svc
  namespace: pspd
spec:
  selector: { app: a-rest }
  ports: [ { port: 50061, targetPort: 8000 } ]

```

k8s/monitoring/servicemonitor-p.yaml
```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: gateway-p-metrics
  namespace: pspd
  labels:
    app: p
    release: prometheus
spec:
  selector:
    matchLabels:
      app: p
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
      scheme: http

```

k8s/monitoring/servicemonitor-a.yaml
```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-a-monitor
  namespace: pspd
  labels:
    app: a
    release: prometheus
spec:
  selector:
    matchLabels:
      app: a
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
    scheme: http

```

k8s/monitoring/hpa.yaml
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: p-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: p-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
      selectPolicy: Max
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: a-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: a-deploy
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: b-hpa
  namespace: pspd
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: b-deploy
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

```

k8s/monitoring/servicemonitor-b.yaml
```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-b-monitor
  namespace: pspd
  labels:
    app: b
    release: prometheus
spec:
  selector:
    matchLabels:
      app: b
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
    scheme: http

```

k8s/monitoring/grafana-dashboard.json
```
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": "-- Grafana --",
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "gnetId": null,
  "graphTooltip": 0,
  "id": null,
  "links": [],
  "panels": [
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "rate(http_requests_total{namespace=\"pspd\"}[1m])",
          "legendFormat": "{{service}} - {{method}}",
          "refId": "A"
        }
      ],
      "title": "HTTP Request Rate",
      "type": "timeseries"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "yellow",
                "value": 0.5
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "s"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": ["mean", "p95", "p99"],
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{namespace=\"pspd\"}[1m]))",
          "legendFormat": "p95 - {{service}}",
          "refId": "A"
        },
        {
          "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{namespace=\"pspd\"}[1m]))",
          "legendFormat": "p99 - {{service}}",
          "refId": "B"
        }
      ],
      "title": "HTTP Request Duration (p95, p99)",
      "type": "timeseries"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": ["lastNotNull"],
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "sum(kube_deployment_status_replicas_available{namespace=\"pspd\"}) by (deployment)",
          "legendFormat": "{{deployment}}",
          "refId": "A"
        }
      ],
      "title": "Pod Replicas",
      "type": "timeseries"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "percent"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "rate(container_cpu_usage_seconds_total{namespace=\"pspd\",container!=\"\"}[1m]) * 100",
          "legendFormat": "{{pod}} - {{container}}",
          "refId": "A"
        }
      ],
      "title": "CPU Usage",
      "type": "timeseries"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "bytes"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 16
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "container_memory_working_set_bytes{namespace=\"pspd\",container!=\"\"}",
          "legendFormat": "{{pod}} - {{container}}",
          "refId": "A"
        }
      ],
      "title": "Memory Usage",
      "type": "timeseries"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 0.01
              }
            ]
          },
          "unit": "percentunit"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "id": 7,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true,
        "text": {}
      },
      "pluginVersion": "8.0.0",
      "targets": [
        {
          "expr": "rate(http_requests_total{namespace=\"pspd\",status=~\"5..\"}[1m]) / rate(http_requests_total{namespace=\"pspd\"}[1m])",
          "legendFormat": "{{service}}",
          "refId": "A"
        }
      ],
      "title": "Error Rate",
      "type": "gauge"
    }
  ],
  "schemaVersion": 27,
  "style": "dark",
  "tags": ["pspd", "microservices", "observability"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-15m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "PSPD - Microservices Observability",
  "uid": "pspd-observability",
  "version": 1
}

```

services/b_rest/Dockerfile
```
FROM python:3.12-slim
WORKDIR /app

# 1) Instala deps especÃ­ficas do serviÃ§o REST B
COPY services/b_rest/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 2) Copia sÃ³ o cÃ³digo do serviÃ§o
COPY services/b_rest/ .

# (Opcional) Se o REST B usar os protos, descomente:
# COPY proto/ ./proto/
# RUN touch /app/proto/__init__.py
# ENV PYTHONPATH=/app:/app/proto

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```

services/b_rest/main.py
```
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import time

app = FastAPI(title="B-REST-Streaming")

# Base de metadados e recomendaÃ§Ãµes
METADATA_DB = {
    "m1": [{"key": "director", "value": "James Cameron", "score": 0.95}, 
           {"key": "cast", "value": "Chris Evans, Zoe Saldana", "score": 0.90}, 
           {"key": "similar", "value": "Interestelar", "score": 0.85}],
    "m2": [{"key": "director", "value": "Christopher Nolan", "score": 0.95}, 
           {"key": "cast", "value": "Leonardo DiCaprio", "score": 0.90},
           {"key": "similar", "value": "AmnÃ©sia", "score": 0.88}],
    "m3": [{"key": "director", "value": "Nancy Meyers", "score": 0.92}, 
           {"key": "cast", "value": "Julia Roberts, Tom Hanks", "score": 0.88},
           {"key": "similar", "value": "Amor Ã  Segunda Vista", "score": 0.85}],
    "m4": [{"key": "director", "value": "Ridley Scott", "score": 0.94}, 
           {"key": "cast", "value": "Tom Hardy, Charlize Theron", "score": 0.91},
           {"key": "similar", "value": "Mad Max", "score": 0.90}],
    "s1": [{"key": "creator", "value": "J.J. Abrams", "score": 0.96}, 
           {"key": "cast", "value": "Millie Bobby Brown", "score": 0.92},
           {"key": "similar", "value": "Dark", "score": 0.90}],
    "s2": [{"key": "creator", "value": "Vince Gilligan", "score": 0.95}, 
           {"key": "cast", "value": "Bryan Cranston", "score": 0.93},
           {"key": "similar", "value": "True Detective", "score": 0.89}],
}

@app.get("/api/metadata/{content_id}")
def get_metadata(content_id: str, userId: str = "guest"):
    """Retorna metadados e recomendaÃ§Ãµes para um conteÃºdo"""
    metadata = METADATA_DB.get(content_id, [])
    
    # Simula processamento
    time.sleep(0.05)
    
    # Se nÃ£o houver metadados, retorna recomendaÃ§Ãµes genÃ©ricas
    if not metadata:
        metadata = [
            {"key": "recommendation", "value": f"ConteÃºdo recomendado #1", "score": 0.7},
            {"key": "recommendation", "value": f"ConteÃºdo recomendado #2", "score": 0.6},
            {"key": "recommendation", "value": f"ConteÃºdo recomendado #3", "score": 0.5},
        ]
    
    return JSONResponse({
        "content_id": content_id,
        "user_id": userId,
        "metadata": metadata,
        "source": "B-REST"
    })


```

services/b_py/Dockerfile
```
FROM python:3.11-slim AS builder

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local

# Copy application
COPY . .

# Make sure scripts in .local are usable
ENV PATH=/root/.local/bin:$PATH

# Generate proto files
RUN python -m grpc_tools.protoc -I. \
    --python_out=. \
    --grpc_python_out=. \
    proto/services.proto

EXPOSE 50052 9102

CMD ["python", "server.py"]

```

services/b_py/server.py
```
import grpc
from concurrent import futures
import time, os
from prometheus_client import start_http_server, Counter, Histogram

from proto import services_pb2, services_pb2_grpc

# Prometheus metrics
REQUEST_COUNT = Counter(
    'grpc_server_requests_total',
    'Total gRPC requests to Service B',
    ['method', 'status']
)

REQUEST_LATENCY = Histogram(
    'grpc_server_request_duration_seconds',
    'gRPC request latency for Service B',
    ['method'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
)

STREAM_ITEMS = Counter(
    'grpc_server_stream_items_total',
    'Total items streamed by Service B',
    ['method']
)

# Base de metadados e recomendaÃ§Ãµes
METADATA_DB = {
    "m1": [("director", "James Cameron", 0.95), ("cast", "Chris Evans, Zoe Saldana", 0.90), 
           ("similar", "Interestelar", 0.85), ("similar", "Gravidade", 0.80)],
    "m2": [("director", "Christopher Nolan", 0.95), ("cast", "Leonardo DiCaprio", 0.90),
           ("similar", "AmnÃ©sia", 0.88), ("similar", "Ilha do Medo", 0.82)],
    "m3": [("director", "Nancy Meyers", 0.92), ("cast", "Julia Roberts, Tom Hanks", 0.88),
           ("similar", "Amor Ã  Segunda Vista", 0.85), ("similar", "VocÃª Tem Um Email", 0.80)],
    "m4": [("director", "Ridley Scott", 0.94), ("cast", "Tom Hardy, Charlize Theron", 0.91),
           ("similar", "Mad Max", 0.90), ("similar", "John Wick", 0.85)],
    "s1": [("creator", "J.J. Abrams", 0.96), ("cast", "Millie Bobby Brown", 0.92),
           ("similar", "Dark", 0.90), ("similar", "Stranger Things", 0.88)],
    "s2": [("creator", "Vince Gilligan", 0.95), ("cast", "Bryan Cranston", 0.93),
           ("similar", "True Detective", 0.89), ("similar", "Mindhunter", 0.84)],
    "s3": [("creator", "Greg Garcia", 0.90), ("cast", "Amy Poehler", 0.87),
           ("similar", "Modern Family", 0.88), ("similar", "Brooklyn Nine-Nine", 0.83)],
    "s4": [("creator", "David Chase", 0.97), ("cast", "James Gandolfini", 0.95),
           ("similar", "The Wire", 0.92), ("similar", "Breaking Bad", 0.90)],
}

class ServiceBImpl(services_pb2_grpc.ServiceBServicer):
    def StreamMetadata(self, request, context):
        """Retorna stream de metadados e recomendaÃ§Ãµes para um conteÃºdo"""
        start = time.time()
        try:
            content_id = request.content_id
            metadata_list = METADATA_DB.get(content_id, [])
            
            # Simula processamento incremental (anÃ¡lise de dados)
            for key, value, score in metadata_list:
                time.sleep(0.01)  # Simula latÃªncia de processamento
                yield services_pb2.MetadataItem(
                    key=key,
                    value=value,
                    relevance_score=score
                )
                STREAM_ITEMS.labels(method='StreamMetadata').inc()
            
            # Adiciona recomendaÃ§Ãµes genÃ©ricas se nÃ£o houver metadados
            if not metadata_list:
                for i in range(3):
                    time.sleep(0.01)
                    yield services_pb2.MetadataItem(
                        key="recommendation",
                        value=f"ConteÃºdo recomendado #{i+1}",
                        relevance_score=0.7 - (i * 0.1)
                    )
                    STREAM_ITEMS.labels(method='StreamMetadata').inc()
            
            REQUEST_COUNT.labels(method='StreamMetadata', status='success').inc()
            
        except Exception as e:
            REQUEST_COUNT.labels(method='StreamMetadata', status='error').inc()
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"Error: {str(e)}")
        finally:
            REQUEST_LATENCY.labels(method='StreamMetadata').observe(time.time() - start)

def serve():
    # Start Prometheus metrics server
    metrics_port = int(os.environ.get("METRICS_PORT", "9102"))
    start_http_server(metrics_port)
    print(f"Metrics server started on :{metrics_port}", flush=True)
    
    port = int(os.environ.get("PORT", "50052"))
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    services_pb2_grpc.add_ServiceBServicer_to_server(ServiceBImpl(), server)
    server.add_insecure_port(f"[::]:{port}")
    server.start()
    print(f"Service B listening on :{port}", flush=True)
    try:
        while True:
            time.sleep(86400)
    except KeyboardInterrupt:
        server.stop(0)

if __name__ == "__main__":
    serve()

```

services/b_py/proto/services.proto
```
// Contrato de comunicaÃ§Ã£o gRPC para aplicaÃ§Ã£o de streaming
// Service A: CatÃ¡logo de conteÃºdo (filmes, sÃ©ries, canais ao vivo)
// Service B: Metadados e recomendaÃ§Ãµes

syntax = "proto3";

package pspd;

// Service A: CatÃ¡logo de conteÃºdo
message ContentRequest {
  string type = 1; // "movies", "series", "live", "all"
  int32 limit = 2;
  string genre = 3;
}

message ContentItem {
  string id = 1;
  string title = 2;
  string description = 3;
  string thumbnail = 4;
  string type = 5;
  repeated string genres = 6;
  int32 year = 7;
  float rating = 8;
  string duration = 9;
}

message ContentResponse {
  repeated ContentItem items = 1;
  int32 total = 2;
}

service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}

// Service B: Metadados e recomendaÃ§Ãµes (streaming)
message MetadataRequest {
  string content_id = 1;
  string user_id = 2;
}

message MetadataItem {
  string key = 1;
  string value = 2;
  float relevance_score = 3;
}

service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}

```

services/a_rest/Dockerfile
```
FROM python:3.12-slim
WORKDIR /app

# Copia sÃ³ o requirements do serviÃ§o (cache)
COPY services/a_rest/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Agora o cÃ³digo do serviÃ§o
COPY services/a_rest/ .

# Se usar os protos no REST, descomente:
# COPY proto/ ./proto/
# RUN touch /app/proto/__init__.py
# ENV PYTHONPATH=/app:/app/proto

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```

services/a_rest/main.py
```
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import time

app = FastAPI(title="A-REST-Streaming")

# CatÃ¡logo de conteÃºdo
CONTENT_CATALOG = [
    {"id": "m1", "title": "A Jornada Infinita", "description": "Uma aventura Ã©pica atravÃ©s das galÃ¡xias",
     "type": "movie", "genres": ["FicÃ§Ã£o CientÃ­fica", "Aventura"], "year": 2024, "rating": 8.7, "duration": "2h 15min"},
    {"id": "m2", "title": "Segredos do Passado", "description": "Um thriller psicolÃ³gico sobre memÃ³rias esquecidas",
     "type": "movie", "genres": ["Thriller", "Drama"], "year": 2024, "rating": 8.2, "duration": "1h 55min"},
    {"id": "m3", "title": "Risadas na Cidade", "description": "Uma comÃ©dia romÃ¢ntica em uma metrÃ³pole agitada",
     "type": "movie", "genres": ["ComÃ©dia", "Romance"], "year": 2024, "rating": 7.5, "duration": "1h 45min"},
    {"id": "m4", "title": "O Ãšltimo GuardiÃ£o", "description": "Um guerreiro protege a Ãºltima esperanÃ§a da humanidade",
     "type": "movie", "genres": ["AÃ§Ã£o", "Aventura"], "year": 2023, "rating": 8.9, "duration": "2h 30min"},
    {"id": "s1", "title": "DimensÃµes Paralelas", "description": "Cientistas descobrem portal para realidades alternativas",
     "type": "series", "genres": ["FicÃ§Ã£o CientÃ­fica", "Drama"], "year": 2024, "rating": 9.1, "duration": "3 temporadas"},
    {"id": "s2", "title": "Cidade Sombria", "description": "Detetives investigam crimes sobrenaturais",
     "type": "series", "genres": ["Suspense", "Sobrenatural"], "year": 2023, "rating": 8.8, "duration": "2 temporadas"},
]

@app.get("/api/content")
def get_content(type: str = "all", limit: int = 20, genre: str = ""):
    """Retorna catÃ¡logo de conteÃºdo filtrado"""
    # Filtrar por tipo
    filtered = CONTENT_CATALOG if type == "all" else [
        c for c in CONTENT_CATALOG if c["type"] == type
    ]
    
    # Filtrar por gÃªnero
    if genre:
        filtered = [c for c in filtered if genre in c["genres"]]
    
    # Aplicar limite
    filtered = filtered[:limit]
    
    return JSONResponse({
        "items": filtered,
        "total": len(filtered),
        "source": "A-REST"
    })

@app.get("/api/content/{content_id}")
def get_content_by_id(content_id: str):
    """Retorna detalhes de um conteÃºdo especÃ­fico"""
    for item in CONTENT_CATALOG:
        if item["id"] == content_id:
            return JSONResponse(item)
    return JSONResponse({"error": "Content not found"}, status_code=404)


```

services/a_py/Dockerfile
```
FROM python:3.11-slim AS builder

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local

# Copy application
COPY . .

# Make sure scripts in .local are usable
ENV PATH=/root/.local/bin:$PATH

# Generate proto files
RUN python -m grpc_tools.protoc -I. \
    --python_out=. \
    --grpc_python_out=. \
    proto/services.proto

EXPOSE 50051 9101

CMD ["python", "server.py"]

```

services/a_py/server.py
```
import grpc
from concurrent import futures
import time, os
from prometheus_client import start_http_server, Counter, Histogram

from proto import services_pb2, services_pb2_grpc

# Prometheus metrics
REQUEST_COUNT = Counter(
    'grpc_server_requests_total',
    'Total gRPC requests to Service A',
    ['method', 'status']
)

REQUEST_LATENCY = Histogram(
    'grpc_server_request_duration_seconds',
    'gRPC request latency for Service A',
    ['method'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
)

CONTENT_ITEMS_RETURNED = Counter(
    'content_items_returned_total',
    'Total content items returned',
    ['content_type']
)

# CatÃ¡logo de conteÃºdo simulado
CONTENT_CATALOG = [
    {"id": "m1", "title": "A Jornada Infinita", "description": "Uma aventura Ã©pica atravÃ©s das galÃ¡xias",
     "type": "movie", "genres": ["FicÃ§Ã£o CientÃ­fica", "Aventura"], "year": 2024, "rating": 8.7, "duration": "2h 15min"},
    {"id": "m2", "title": "Segredos do Passado", "description": "Um thriller psicolÃ³gico sobre memÃ³rias esquecidas",
     "type": "movie", "genres": ["Thriller", "Drama"], "year": 2024, "rating": 8.2, "duration": "1h 55min"},
    {"id": "m3", "title": "Risadas na Cidade", "description": "Uma comÃ©dia romÃ¢ntica em uma metrÃ³pole agitada",
     "type": "movie", "genres": ["ComÃ©dia", "Romance"], "year": 2024, "rating": 7.5, "duration": "1h 45min"},
    {"id": "m4", "title": "O Ãšltimo GuardiÃ£o", "description": "Um guerreiro protege a Ãºltima esperanÃ§a da humanidade",
     "type": "movie", "genres": ["AÃ§Ã£o", "Aventura"], "year": 2023, "rating": 8.9, "duration": "2h 30min"},
    {"id": "s1", "title": "DimensÃµes Paralelas", "description": "Cientistas descobrem portal para realidades alternativas",
     "type": "series", "genres": ["FicÃ§Ã£o CientÃ­fica", "Drama"], "year": 2024, "rating": 9.1, "duration": "3 temporadas"},
    {"id": "s2", "title": "Cidade Sombria", "description": "Detetives investigam crimes sobrenaturais",
     "type": "series", "genres": ["Suspense", "Sobrenatural"], "year": 2023, "rating": 8.8, "duration": "2 temporadas"},
    {"id": "s3", "title": "FamÃ­lia Moderna", "description": "O dia a dia de uma famÃ­lia brasileira contemporÃ¢nea",
     "type": "series", "genres": ["ComÃ©dia", "FamÃ­lia"], "year": 2024, "rating": 7.9, "duration": "1 temporada"},
    {"id": "s4", "title": "ImpÃ©rio do Crime", "description": "A ascensÃ£o de um sindicato do crime organizado",
     "type": "series", "genres": ["Drama", "Crime"], "year": 2023, "rating": 9.3, "duration": "4 temporadas"},
    {"id": "ch1", "title": "Canal Premium", "description": "Entretenimento ao vivo 24/7",
     "type": "live", "genres": ["Entretenimento"], "year": 2024, "rating": 8.5, "duration": "24/7"},
    {"id": "ch2", "title": "Canal NotÃ­cias", "description": "NotÃ­cias em tempo real",
     "type": "live", "genres": ["NotÃ­cias"], "year": 2024, "rating": 8.0, "duration": "24/7"},
    {"id": "ch3", "title": "Canal Esportes", "description": "TransmissÃµes esportivas ao vivo",
     "type": "live", "genres": ["Esportes"], "year": 2024, "rating": 9.0, "duration": "24/7"},
]

class ServiceAImpl(services_pb2_grpc.ServiceAServicer):
    def GetContent(self, request, context):
        """Retorna catÃ¡logo de conteÃºdo filtrado por tipo e gÃªnero"""
        start = time.time()
        try:
            # Filtrar por tipo
            content_type = request.type.lower() if request.type else "all"
            filtered = CONTENT_CATALOG if content_type == "all" else [
                c for c in CONTENT_CATALOG if c["type"] == content_type
            ]
            
            # Filtrar por gÃªnero
            if request.genre:
                filtered = [c for c in filtered if request.genre in c["genres"]]
            
            # Aplicar limite
            limit = request.limit if request.limit > 0 else len(filtered)
            filtered = filtered[:limit]
            
            # Construir resposta
            items = [
                services_pb2.ContentItem(
                    id=c["id"],
                    title=c["title"],
                    description=c["description"],
                    thumbnail=f"/api/thumbnails/{c['id']}.jpg",
                    type=c["type"],
                    genres=c["genres"],
                    year=c["year"],
                    rating=c["rating"],
                    duration=c["duration"]
                )
                for c in filtered
            ]
            
            CONTENT_ITEMS_RETURNED.labels(content_type=content_type).inc(len(items))
            REQUEST_COUNT.labels(method='GetContent', status='success').inc()
            
            return services_pb2.ContentResponse(items=items, total=len(items))
            
        except Exception as e:
            REQUEST_COUNT.labels(method='GetContent', status='error').inc()
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"Error: {str(e)}")
            return services_pb2.ContentResponse()
        finally:
            REQUEST_LATENCY.labels(method='GetContent').observe(time.time() - start)

def serve():
    # Start Prometheus metrics server
    metrics_port = int(os.environ.get("METRICS_PORT", "9101"))
    start_http_server(metrics_port)
    print(f"Metrics server started on :{metrics_port}", flush=True)
    
    port = int(os.environ.get("PORT", "50051"))
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    services_pb2_grpc.add_ServiceAServicer_to_server(ServiceAImpl(), server)
    server.add_insecure_port(f"[::]:{port}")
    server.start()
    print(f"Service A listening on :{port}", flush=True)
    try:
        while True:
            time.sleep(86400)
    except KeyboardInterrupt:
        server.stop(0)

if __name__ == "__main__":
    serve()

```

services/a_py/proto/services.proto
```
// Contrato de comunicaÃ§Ã£o gRPC para aplicaÃ§Ã£o de streaming
// Service A: CatÃ¡logo de conteÃºdo (filmes, sÃ©ries, canais ao vivo)
// Service B: Metadados e recomendaÃ§Ãµes

syntax = "proto3";

package pspd;

// Service A: CatÃ¡logo de conteÃºdo
message ContentRequest {
  string type = 1; // "movies", "series", "live", "all"
  int32 limit = 2;
  string genre = 3;
}

message ContentItem {
  string id = 1;
  string title = 2;
  string description = 3;
  string thumbnail = 4;
  string type = 5;
  repeated string genres = 6;
  int32 year = 7;
  float rating = 8;
  string duration = 9;
}

message ContentResponse {
  repeated ContentItem items = 1;
  int32 total = 2;
}

service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}

// Service B: Metadados e recomendaÃ§Ãµes (streaming)
message MetadataRequest {
  string content_id = 1;
  string user_id = 2;
}

message MetadataItem {
  string key = 1;
  string value = 2;
  float relevance_score = 3;
}

service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}

```

proto/services.proto
```
// Contrato de comunicaÃ§Ã£o gRPC para aplicaÃ§Ã£o de streaming
// Service A: CatÃ¡logo de conteÃºdo (filmes, sÃ©ries, canais ao vivo)
// Service B: Metadados e recomendaÃ§Ãµes

syntax = "proto3";

package pspd;

// Service A: CatÃ¡logo de conteÃºdo
message ContentRequest {
  string type = 1; // "movies", "series", "live", "all"
  int32 limit = 2;
  string genre = 3;
}

message ContentItem {
  string id = 1;
  string title = 2;
  string description = 3;
  string thumbnail = 4;
  string type = 5;
  repeated string genres = 6;
  int32 year = 7;
  float rating = 8;
  string duration = 9;
}

message ContentResponse {
  repeated ContentItem items = 1;
  int32 total = 2;
}

service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}

// Service B: Metadados e recomendaÃ§Ãµes (streaming)
message MetadataRequest {
  string content_id = 1;
  string user_id = 2;
}

message MetadataItem {
  string key = 1;
  string value = 2;
  float relevance_score = 3;
}

service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}

```

scripts/expose_backend.sh
```
#!/bin/bash

# Script helper para expor backend e configurar integraÃ§Ã£o com frontend
# Uso: ./scripts/expose_backend.sh [ngrok|local]

set -e

MODE="${1:-ngrok}"

echo "ğŸš€ Configurando integraÃ§Ã£o Frontend + Backend + MÃ©tricas"
echo ""

# Verificar se cluster estÃ¡ rodando
echo "ğŸ“‹ Verificando cluster..."
if ! kubectl cluster-info &> /dev/null; then
    echo "âŒ Cluster Kubernetes nÃ£o estÃ¡ acessÃ­vel"
    echo "   Execute: minikube start"
    exit 1
fi

# Verificar se namespace pspd existe
if ! kubectl get namespace pspd &> /dev/null; then
    echo "âŒ Namespace 'pspd' nÃ£o existe"
    echo "   Execute: kubectl apply -f k8s/"
    exit 1
fi

# Verificar se pods estÃ£o rodando
echo "ğŸ“‹ Verificando pods..."
PODS_READY=$(kubectl get pods -n pspd --no-headers | grep -c "Running" || true)
if [ "$PODS_READY" -lt 3 ]; then
    echo "âš ï¸  Nem todos os pods estÃ£o Running"
    kubectl get pods -n pspd
    echo ""
    read -p "Continuar mesmo assim? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# Verificar se Prometheus estÃ¡ rodando
echo "ğŸ“‹ Verificando Prometheus..."
if ! kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus &> /dev/null; then
    echo "âš ï¸  Prometheus nÃ£o encontrado no namespace 'monitoring'"
    echo "   Para instalar: helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace"
fi

echo ""
echo "âœ… VerificaÃ§Ãµes concluÃ­das!"
echo ""

# FunÃ§Ã£o para matar processos na saÃ­da
cleanup() {
    echo ""
    echo "ğŸ›‘ Encerrando processos..."
    jobs -p | xargs -r kill 2>/dev/null || true
    exit 0
}

trap cleanup SIGINT SIGTERM

if [ "$MODE" == "ngrok" ]; then
    echo "ğŸŒ Modo: Ngrok (Frontend Vercel + Backend Kubernetes)"
    echo ""
    
    # Verificar se ngrok estÃ¡ instalado
    if ! command -v ngrok &> /dev/null; then
        echo "âŒ Ngrok nÃ£o estÃ¡ instalado"
        echo ""
        echo "InstalaÃ§Ã£o:"
        echo "  1. Baixe: https://ngrok.com/download"
        echo "  2. OU: snap install ngrok"
        echo "  3. Configure auth token: ngrok config add-authtoken <token>"
        echo ""
        exit 1
    fi
    
    echo "ğŸ“¡ Iniciando port-forward do Gateway P (porta 8080)..."
    kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
    PF_PID=$!
    
    # Aguardar port-forward estar pronto
    sleep 3
    
    echo "ğŸŒ Expondo backend com Ngrok..."
    ngrok http 8080 > /dev/null 2>&1 &
    NGROK_PID=$!
    
    # Aguardar Ngrok iniciar
    sleep 3
    
    # Pegar URL pÃºblica do Ngrok
    NGROK_URL=$(curl -s http://localhost:4040/api/tunnels | grep -o '"public_url":"https://[^"]*' | cut -d'"' -f4 | head -1)
    
    if [ -z "$NGROK_URL" ]; then
        echo "âŒ Falha ao obter URL do Ngrok"
        echo "   Verifique se Ngrok estÃ¡ rodando: curl http://localhost:4040/api/tunnels"
        exit 1
    fi
    
    echo ""
    echo "âœ… Backend exposto publicamente!"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“‹ ConfiguraÃ§Ã£o do Frontend (Vercel):"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "1. Acesse: https://vercel.com/seu-usuario/streaming-app-design/settings/environment-variables"
    echo ""
    echo "2. Adicione variÃ¡vel de ambiente:"
    echo "   Key:   NEXT_PUBLIC_API_URL"
    echo "   Value: $NGROK_URL"
    echo "   Environment: Production"
    echo ""
    echo "3. Redeploy o projeto na Vercel"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ§ª Testar Backend:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "curl \"$NGROK_URL/api/content?type=all\""
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“Š Acessar Monitoramento:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "Em NOVOS terminais, execute:"
    echo ""
    echo "# Prometheus:"
    echo "kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
    echo "â†’ http://localhost:9090"
    echo ""
    echo "# Grafana:"
    echo "kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
    echo "â†’ http://localhost:3000"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "â³ Processos rodando... (Ctrl+C para encerrar)"
    echo ""
    
    # Manter script rodando
    wait

elif [ "$MODE" == "local" ]; then
    echo "ğŸ’» Modo: Frontend Local + Backend Local"
    echo ""
    
    # Verificar se frontend existe
    if [ ! -d "../streaming-app-design" ]; then
        echo "âŒ Pasta do frontend nÃ£o encontrada: ../streaming-app-design"
        echo ""
        echo "Clone o repositÃ³rio:"
        echo "  git clone <repo-url> ../streaming-app-design"
        exit 1
    fi
    
    # Verificar se node_modules existe
    if [ ! -d "../streaming-app-design/node_modules" ]; then
        echo "ğŸ“¦ Instalando dependÃªncias do frontend..."
        cd ../streaming-app-design
        if command -v pnpm &> /dev/null; then
            pnpm install
        else
            npm install
        fi
        cd - > /dev/null
    fi
    
    # Criar .env.local
    echo "ğŸ“ Configurando .env.local..."
    cat > ../streaming-app-design/.env.local << 'EOF'
NEXT_PUBLIC_API_URL=http://localhost:8080
EOF
    
    echo "ğŸ“¡ Iniciando port-forward do Gateway P (porta 8080)..."
    kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
    PF_PID=$!
    
    # Aguardar port-forward estar pronto
    sleep 3
    
    echo "ğŸš€ Iniciando frontend Next.js..."
    cd ../streaming-app-design
    
    if command -v pnpm &> /dev/null; then
        pnpm dev > /dev/null 2>&1 &
    else
        npm run dev > /dev/null 2>&1 &
    fi
    NEXT_PID=$!
    cd - > /dev/null
    
    # Aguardar Next.js iniciar
    echo "â³ Aguardando Next.js iniciar..."
    sleep 5
    
    echo ""
    echo "âœ… Frontend e Backend rodando!"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸŒ Acessos:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "Frontend:  http://localhost:3000"
    echo "Backend:   http://localhost:8080"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“Š Acessar Monitoramento:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "Em NOVOS terminais, execute:"
    echo ""
    echo "# Prometheus:"
    echo "kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
    echo "â†’ http://localhost:9090"
    echo ""
    echo "# Grafana:"
    echo "kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
    echo "â†’ http://localhost:3000"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "â³ Processos rodando... (Ctrl+C para encerrar)"
    echo ""
    
    # Manter script rodando
    wait

else
    echo "âŒ Modo invÃ¡lido: $MODE"
    echo ""
    echo "Uso:"
    echo "  ./scripts/expose_backend.sh ngrok   # Expor com Ngrok (para Vercel)"
    echo "  ./scripts/expose_backend.sh local   # Rodar frontend localmente"
    exit 1
fi

```

scripts/run_scenario_comparison.sh
```
#!/bin/bash
# Script para executar todos os cenÃ¡rios e gerar anÃ¡lise comparativa

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  Executando Todos os CenÃ¡rios                                â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Executar cada cenÃ¡rio
for i in {1..5}; do
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  CENÃRIO $i/5"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    if [ -f "$PROJECT_DIR/test/scenario_$i/run_all.sh" ]; then
        "$PROJECT_DIR/test/scenario_$i/run_all.sh" || echo -e "${YELLOW}âš ï¸  CenÃ¡rio $i falhou${NC}"
    else
        echo -e "${YELLOW}âš ï¸  Script nÃ£o encontrado: test/scenario_$i/run_all.sh${NC}"
    fi
    
    echo ""
    [ "$i" != "5" ] && sleep 10
done

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  ğŸ“Š Gerando AnÃ¡lise Comparativa                             â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Gerar comparaÃ§Ã£o
python3 "$SCRIPT_DIR/compare_scenarios.py"

echo ""
echo -e "${GREEN}âœ… Todos os cenÃ¡rios executados e comparados!${NC}"
echo ""
echo "ğŸ“ Resultados em: test_results/"
echo "ï¿½ï¿½ ComparaÃ§Ã£o em: test_results/scenario-comparison/"

```

scripts/stable_port_forward.sh
```
#!/bin/bash

# Script para manter port-forward ativo durante testes longos
# Uso: ./scripts/stable_port_forward.sh [porta_local] [porta_remote]

NAMESPACE="pspd"
SERVICE="p-svc"
LOCAL_PORT=${1:-8080}
REMOTE_PORT=${2:-80}
LOG_FILE="/tmp/pf_stable.log"

echo "ğŸ”— Iniciando port-forward estÃ¡vel"
echo "   Namespace: $NAMESPACE"
echo "   Service: $SERVICE"
echo "   Port: $LOCAL_PORT:$REMOTE_PORT"
echo "   Log: $LOG_FILE"
echo ""

# FunÃ§Ã£o para iniciar port-forward
start_pf() {
    echo "[$(date '+%H:%M:%S')] Iniciando port-forward..." | tee -a $LOG_FILE
    kubectl port-forward -n $NAMESPACE svc/$SERVICE $LOCAL_PORT:$REMOTE_PORT >> $LOG_FILE 2>&1 &
    PF_PID=$!
    echo $PF_PID > /tmp/pf_stable.pid
    echo "[$(date '+%H:%M:%S')] PID: $PF_PID" | tee -a $LOG_FILE
}

# FunÃ§Ã£o para verificar se port-forward estÃ¡ ativo
check_pf() {
    if [ -f /tmp/pf_stable.pid ]; then
        PID=$(cat /tmp/pf_stable.pid)
        if ps -p $PID > /dev/null 2>&1; then
            return 0  # EstÃ¡ rodando
        fi
    fi
    return 1  # NÃ£o estÃ¡ rodando
}

# Limpar processos antigos
pkill -f "kubectl port-forward.*$SERVICE" 2>/dev/null || true
sleep 1

# Iniciar port-forward
start_pf
sleep 3

# Loop de monitoramento
echo "[$(date '+%H:%M:%S')] Monitorando port-forward (Ctrl+C para parar)..."
echo "   Para parar: kill \$(cat /tmp/pf_stable.pid)"
echo ""
echo "ğŸ’¡ Durante testes de spike Ã© normal que o port-forward caia"
echo "   O script reiniciarÃ¡ automaticamente"
echo ""

RESTART_COUNT=0
RESTART_DELAY=2

while true; do
    if ! check_pf; then
        RESTART_COUNT=$((RESTART_COUNT + 1))
        
        # Aumentar delay progressivo apÃ³s muitos restarts
        if [ $RESTART_COUNT -gt 10 ]; then
            RESTART_DELAY=5
        elif [ $RESTART_COUNT -gt 5 ]; then
            RESTART_DELAY=3
        fi
        
        echo "[$(date '+%H:%M:%S')] âš ï¸  Port-forward caiu! Reiniciando em ${RESTART_DELAY}s (#$RESTART_COUNT)..." | tee -a $LOG_FILE
        sleep $RESTART_DELAY
        
        # Limpar processos Ã³rfÃ£os
        pkill -f "kubectl port-forward.*$SERVICE" 2>/dev/null || true
        sleep 1
        
        start_pf
        sleep 3
    fi
    sleep 5
done
```

scripts/open_monitoring.sh
```
#!/bin/bash
# Script para abrir Grafana e Prometheus em segundo plano

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  Abrindo Grafana e Prometheus                                â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Verificar se o namespace existe
if ! kubectl get namespace monitoring > /dev/null 2>&1; then
    echo "âŒ Namespace 'monitoring' nÃ£o encontrado"
    echo "   Execute primeiro: helm install prometheus ..."
    exit 1
fi

# Verificar se os pods estÃ£o rodando
echo "ğŸ” Verificando pods..."
GRAFANA_POD=$(kubectl get pods -n monitoring -l "app.kubernetes.io/name=grafana" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
PROMETHEUS_POD=$(kubectl get pods -n monitoring -l "app.kubernetes.io/name=prometheus" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

if [ -z "$GRAFANA_POD" ]; then
    echo "âŒ Pod do Grafana nÃ£o encontrado"
    echo "   Verifique: kubectl get pods -n monitoring"
    exit 1
fi

if [ -z "$PROMETHEUS_POD" ]; then
    echo "âŒ Pod do Prometheus nÃ£o encontrado"
    echo "   Verifique: kubectl get pods -n monitoring"
    exit 1
fi

echo "âœ… Pods encontrados:"
echo "   Grafana: $GRAFANA_POD"
echo "   Prometheus: $PROMETHEUS_POD"
echo ""

# Matar port-forwards antigos
echo "ğŸ§¹ Limpando port-forwards antigos..."
pkill -f "port-forward.*monitoring.*3000" 2>/dev/null || true
pkill -f "port-forward.*monitoring.*9090" 2>/dev/null || true
sleep 1

# Iniciar port-forwards em background
echo "ğŸš€ Iniciando port-forwards..."
echo ""

kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 > /dev/null 2>&1 &
GRAFANA_PID=$!
sleep 2

kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 > /dev/null 2>&1 &
PROMETHEUS_PID=$!
sleep 2

# Verificar se estÃ£o rodando
if ps -p $GRAFANA_PID > /dev/null && ps -p $PROMETHEUS_PID > /dev/null; then
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  âœ… Port-forwards iniciados com sucesso!                     â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "ğŸŒ Acesse:"
    echo ""
    echo "   ğŸ“Š Grafana:    http://localhost:3000"
    echo "      Login: admin"
    echo "      Senha: admin (serÃ¡ pedido para trocar no primeiro login)"
    echo ""
    echo "   ğŸ“ˆ Prometheus: http://localhost:9090"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "ğŸ’¡ Dicas:"
    echo "   â€¢ Port-forwards estÃ£o rodando em background (PIDs: $GRAFANA_PID, $PROMETHEUS_PID)"
    echo "   â€¢ Para parar: pkill -f 'port-forward.*monitoring'"
    echo "   â€¢ Ou feche este terminal"
    echo ""
    echo "ğŸ”§ Troubleshooting:"
    echo "   â€¢ Se nÃ£o carregar, aguarde 10-20 segundos"
    echo "   â€¢ Verifique: kubectl get pods -n monitoring"
    echo "   â€¢ Logs: kubectl logs -n monitoring $GRAFANA_POD"
    echo ""
    
    # Manter script rodando
    echo "â³ Mantendo port-forwards ativos... (Ctrl+C para parar)"
    wait $GRAFANA_PID $PROMETHEUS_PID
else
    echo "âŒ Falha ao iniciar port-forwards"
    echo "   Verifique: kubectl get svc -n monitoring"
    exit 1
fi

```

scripts/compare_scenarios.py
```
#!/usr/bin/env python3
"""
Script para gerar anÃ¡lise comparativa entre cenÃ¡rios com grÃ¡ficos.
Compara performance, custo e escalabilidade dos 5 cenÃ¡rios.
"""

import json
import re
import os
import sys
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import numpy as np

# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o
plt.rcParams['figure.figsize'] = (16, 10)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3

SCENARIO_NAMES = {
    '1': 'S1: Base (HPA)',
    '2': 'S2: 2 RÃ©plicas',
    '3': 'S3: DistribuÃ­do',
    '4': 'S4: Recursos -50%',
    '5': 'S5: Sem HPA'
}

SCENARIO_COLORS = {
    '1': '#3498db',      # Azul
    '2': '#2ecc71',  # Verde
    '3': '#e74c3c',  # Vermelho
    '4': '#f39c12', # Laranja
    '5': '#9b59b6'     # Roxo
}


def find_result_dirs(base_dir: Path) -> Dict[str, Path]:
    """Encontra diretÃ³rios de resultados dos cenÃ¡rios."""
    result_dirs = {}
    
    test_results_dir = base_dir / "test_results"
    if not test_results_dir.exists():
        return result_dirs
    
    for scenario_key in SCENARIO_NAMES.keys():
        scenario_dir = test_results_dir / f"scenario_{scenario_key}"
        if scenario_dir.exists():
            result_dirs[scenario_key] = scenario_dir
    
    return result_dirs


def parse_output_file(file_path: Path) -> Dict[str, Any]:
    """Parse do arquivo output.txt do k6."""
    metrics = {}
    
    if not file_path.exists():
        return metrics
    
    with open(file_path) as f:
        content = f.read()
    
    # Remover quebras de linha no meio das linhas de mÃ©tricas
    content = re.sub(r'\n\s+', ' ', content)
    
    # Throughput
    throughput_match = re.search(r'http_reqs.*?([\d.]+)/s', content)
    if throughput_match:
        metrics['throughput'] = float(throughput_match.group(1))
    
    # Total requests
    total_match = re.search(r'http_reqs.*?(\d+)', content)
    if total_match:
        metrics['total_requests'] = int(total_match.group(1))
    
    # LatÃªncia mÃ©dia
    avg_match = re.search(r'http_req_duration.*?avg=([\d.\-]+)ms', content)
    if avg_match:
        metrics['latency_avg'] = float(avg_match.group(1))
    
    # LatÃªncia P95
    p95_match = re.search(r'http_req_duration.*?p\(95\)=([\d.\-]+)ms', content)
    if p95_match:
        metrics['latency_p95'] = float(p95_match.group(1))
    
    # LatÃªncia P99
    p99_match = re.search(r'http_req_duration.*?p\(99\)=([\d.\-]+)ms', content)
    if p99_match:
        metrics['latency_p99'] = float(p99_match.group(1))
    
    # Taxa de falha
    failed_match = re.search(r'http_req_failed.*?([\d.]+)%', content)
    if failed_match:
        metrics['failure_rate'] = float(failed_match.group(1))
    else:
        metrics['failure_rate'] = 0.0
    
    # Success rate (fallback)
    checks_match = re.search(r'checks.*?([\d.]+)%', content)
    if checks_match:
        metrics['success_rate'] = float(checks_match.group(1))
    else:
        metrics['success_rate'] = 100.0 - metrics.get('failure_rate', 0.0)
    
    # VUs
    vus_match = re.search(r'vus_max.*?(\d+)', content)
    if vus_match:
        metrics['max_vus'] = int(vus_match.group(1))
    
    return metrics


def parse_hpa_status(file_path: Path) -> Dict[str, Dict[str, int]]:
    """Parse do arquivo hpa-status-post.txt."""
    hpa_data = {}
    
    if not file_path.exists():
        return hpa_data
    
    with open(file_path) as f:
        content = f.read().replace('\n', ' ')
    
    parts = content.split()
    
    for hpa_name in ['a-hpa', 'b-hpa', 'p-hpa']:
        try:
            idx = parts.index(hpa_name)
            numbers = []
            for i in range(idx + 1, min(idx + 20, len(parts))):
                try:
                    num = int(parts[i])
                    numbers.append(num)
                    if len(numbers) == 3:
                        break
                except ValueError:
                    continue
            
            if len(numbers) >= 3:
                hpa_data[hpa_name] = {
                    'min': numbers[0],
                    'max': numbers[1],
                    'replicas': numbers[2]
                }
        except (ValueError, IndexError):
            continue
    
    return hpa_data


def collect_scenario_data(result_dirs: Dict[str, Path]) -> Dict[str, Dict]:
    """Coleta dados de todos os cenÃ¡rios."""
    scenarios_data = {}
    
    for scenario_key, result_dir in result_dirs.items():
        scenario_data = {
            'baseline': {},
            'ramp': {},
            'spike': {},
            'soak': {},
            'hpa': {}
        }
        
        # Parse de cada teste
        for test_name in ['baseline', 'ramp', 'spike', 'soak']:
            test_dir = result_dir / test_name
            output_file = test_dir / 'output.txt'
            
            if output_file.exists():
                scenario_data[test_name] = parse_output_file(output_file)
            
            # HPA data (apenas spike para comparaÃ§Ã£o)
            if test_name == 'spike':
                hpa_file = test_dir / 'hpa-status-post.txt'
                if hpa_file.exists():
                    scenario_data['hpa'] = parse_hpa_status(hpa_file)
        
        scenarios_data[scenario_key] = scenario_data
    
    return scenarios_data


def plot_latency_comparison(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 1: ComparaÃ§Ã£o de latÃªncia P95 entre cenÃ¡rios."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # P95 por teste
    tests = ['baseline', 'ramp', 'spike', 'soak']
    x = np.arange(len(tests))
    width = 0.15
    
    for i, (scenario_key, scenario_name) in enumerate(SCENARIO_NAMES.items()):
        if scenario_key not in scenarios_data:
            continue
        
        p95_values = []
        for test in tests:
            p95 = scenarios_data[scenario_key][test].get('latency_p95', 0)
            p95_values.append(p95)
        
        offset = width * (i - 2)
        ax1.bar(x + offset, p95_values, width, 
                label=scenario_name, 
                color=SCENARIO_COLORS[scenario_key],
                alpha=0.8)
    
    ax1.set_xlabel('Tipo de Teste')
    ax1.set_ylabel('LatÃªncia P95 (ms)')
    ax1.set_title('LatÃªncia P95 por CenÃ¡rio e Teste')
    ax1.set_xticks(x)
    ax1.set_xticklabels(tests)
    ax1.legend(fontsize=8)
    ax1.grid(True, alpha=0.3)
    
    # LatÃªncia mÃ©dia durante spike
    scenarios = []
    spike_p95 = []
    colors = []
    
    for scenario_key, scenario_name in SCENARIO_NAMES.items():
        if scenario_key not in scenarios_data:
            continue
        
        p95 = scenarios_data[scenario_key]['spike'].get('latency_p95', 0)
        scenarios.append(scenario_name)
        spike_p95.append(p95)
        colors.append(SCENARIO_COLORS[scenario_key])
    
    bars = ax2.barh(scenarios, spike_p95, color=colors, alpha=0.8)
    ax2.set_xlabel('LatÃªncia P95 (ms)')
    ax2.set_title('LatÃªncia P95 durante Spike Test')
    ax2.grid(True, alpha=0.3, axis='x')
    
    # Adicionar valores nas barras
    for bar in bars:
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2, 
                f'{width:.0f}ms', 
                ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_dir / '01_scenario_latency_comparison.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '01_scenario_latency_comparison.png'}")


def plot_throughput_comparison(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 2: ComparaÃ§Ã£o de throughput entre cenÃ¡rios."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Throughput por teste
    tests = ['baseline', 'ramp', 'spike', 'soak']
    x = np.arange(len(tests))
    width = 0.15
    
    for i, (scenario_key, scenario_name) in enumerate(SCENARIO_NAMES.items()):
        if scenario_key not in scenarios_data:
            continue
        
        throughput_values = []
        for test in tests:
            throughput = scenarios_data[scenario_key][test].get('throughput', 0)
            throughput_values.append(throughput)
        
        offset = width * (i - 2)
        ax1.bar(x + offset, throughput_values, width, 
                label=scenario_name, 
                color=SCENARIO_COLORS[scenario_key],
                alpha=0.8)
    
    ax1.set_xlabel('Tipo de Teste')
    ax1.set_ylabel('Throughput (req/s)')
    ax1.set_title('Throughput por CenÃ¡rio e Teste')
    ax1.set_xticks(x)
    ax1.set_xticklabels(tests)
    ax1.legend(fontsize=8)
    ax1.grid(True, alpha=0.3)
    
    # Throughput mÃ©dio geral
    scenarios = []
    avg_throughput = []
    colors = []
    
    for scenario_key, scenario_name in SCENARIO_NAMES.items():
        if scenario_key not in scenarios_data:
            continue
        
        throughputs = [
            scenarios_data[scenario_key][test].get('throughput', 0)
            for test in tests
        ]
        avg = np.mean([t for t in throughputs if t > 0])
        
        scenarios.append(scenario_name)
        avg_throughput.append(avg)
        colors.append(SCENARIO_COLORS[scenario_key])
    
    bars = ax2.barh(scenarios, avg_throughput, color=colors, alpha=0.8)
    ax2.set_xlabel('Throughput MÃ©dio (req/s)')
    ax2.set_title('Throughput MÃ©dio Geral')
    ax2.grid(True, alpha=0.3, axis='x')
    
    for bar in bars:
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f}', 
                ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_dir / '02_scenario_throughput_comparison.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '02_scenario_throughput_comparison.png'}")


def plot_hpa_scaling(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 3: ComparaÃ§Ã£o de scaling HPA durante spike."""
    scenarios = []
    a_replicas = []
    b_replicas = []
    p_replicas = []
    colors = []
    
    for scenario_key, scenario_name in SCENARIO_NAMES.items():
        if scenario_key not in scenarios_data:
            continue
        
        hpa_data = scenarios_data[scenario_key].get('hpa', {})
        
        scenarios.append(scenario_name.replace('S', '\nS'))
        a_replicas.append(hpa_data.get('a-hpa', {}).get('replicas', 0))
        b_replicas.append(hpa_data.get('b-hpa', {}).get('replicas', 0))
        p_replicas.append(hpa_data.get('p-hpa', {}).get('replicas', 0))
        colors.append(SCENARIO_COLORS[scenario_key])
    
    x = np.arange(len(scenarios))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    bars1 = ax.bar(x - width, a_replicas, width, label='Service A', alpha=0.8, color='#3498db')
    bars2 = ax.bar(x, b_replicas, width, label='Service B', alpha=0.8, color='#2ecc71')
    bars3 = ax.bar(x + width, p_replicas, width, label='Gateway P', alpha=0.8, color='#e74c3c')
    
    ax.set_xlabel('CenÃ¡rio')
    ax.set_ylabel('NÃºmero de RÃ©plicas')
    ax.set_title('Escalamento HPA durante Spike Test (200 VUs)')
    ax.set_xticks(x)
    ax.set_xticklabels(scenarios, fontsize=9)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}',
                       ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(output_dir / '03_scenario_hpa_scaling.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '03_scenario_hpa_scaling.png'}")


def plot_success_rate(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 4: Taxa de sucesso por cenÃ¡rio."""
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    axes = axes.flatten()
    
    tests = ['baseline', 'ramp', 'spike', 'soak']
    
    for idx, test in enumerate(tests):
        scenarios = []
        success_rates = []
        colors = []
        
        for scenario_key, scenario_name in SCENARIO_NAMES.items():
            if scenario_key not in scenarios_data:
                continue
            
            success_rate = scenarios_data[scenario_key][test].get('success_rate', 0)
            scenarios.append(scenario_name)
            success_rates.append(success_rate)
            colors.append(SCENARIO_COLORS[scenario_key])
        
        bars = axes[idx].barh(scenarios, success_rates, color=colors, alpha=0.8)
        axes[idx].set_xlabel('Taxa de Sucesso (%)')
        axes[idx].set_title(f'Taxa de Sucesso - {test.upper()}')
        axes[idx].set_xlim(0, 105)
        axes[idx].grid(True, alpha=0.3, axis='x')
        
        # Adicionar valores
        for bar in bars:
            width = bar.get_width()
            axes[idx].text(width, bar.get_y() + bar.get_height()/2, 
                          f'{width:.1f}%', 
                          ha='left', va='center', fontsize=8)
        
        # Linha de referÃªncia em 95%
        axes[idx].axvline(x=95, color='red', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.tight_layout()
    plt.savefig(output_dir / '04_scenario_success_rate.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '04_scenario_success_rate.png'}")


def plot_cost_analysis(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 5: AnÃ¡lise de custo estimado (pod*min)."""
    scenarios = []
    baseline_pods = []
    spike_pods = []
    avg_pods = []
    colors = []
    
    # Estimativas baseadas em rÃ©plicas iniciais e HPA
    cost_estimates = {
        '1-base': {'baseline': 3, 'spike': 11, 'avg': 6},
        '2-replicas': {'baseline': 6, 'spike': 13, 'avg': 8},
        '3-distribution': {'baseline': 9, 'spike': 15, 'avg': 11},
        '4-resources': {'baseline': 3, 'spike': 18, 'avg': 9},
        '5-no-hpa': {'baseline': 11, 'spike': 11, 'avg': 11}
    }
    
    for scenario_key, scenario_name in SCENARIO_NAMES.items():
        if scenario_key not in scenarios_data:
            continue
        
        est = cost_estimates.get(scenario_key, {'baseline': 3, 'spike': 11, 'avg': 6})
        
        scenarios.append(scenario_name)
        baseline_pods.append(est['baseline'])
        spike_pods.append(est['spike'])
        avg_pods.append(est['avg'])
        colors.append(SCENARIO_COLORS[scenario_key])
    
    x = np.arange(len(scenarios))
    width = 0.25
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Pods por fase
    bars1 = ax1.bar(x - width, baseline_pods, width, label='Baseline', alpha=0.8)
    bars2 = ax1.bar(x, spike_pods, width, label='Spike', alpha=0.8)
    bars3 = ax1.bar(x + width, avg_pods, width, label='MÃ©dia', alpha=0.8)
    
    ax1.set_xlabel('CenÃ¡rio')
    ax1.set_ylabel('NÃºmero de Pods')
    ax1.set_title('Pods Ativos por Fase')
    ax1.set_xticks(x)
    ax1.set_xticklabels([s.replace('S', '\nS') for s in scenarios], fontsize=9)
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Valores nas barras
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}',
                    ha='center', va='bottom', fontsize=8)
    
    # Custo total estimado (pod*hora para teste completo ~30min)
    total_cost = []
    for est in [cost_estimates.get(key, {'avg': 6}) for key in SCENARIO_NAMES.keys() 
                if key in scenarios_data]:
        # 27 minutos de teste * pods mÃ©dios
        cost = est['avg'] * 27 / 60  # pod-horas
        total_cost.append(cost)
    
    bars = ax2.barh(scenarios, total_cost, color=colors, alpha=0.8)
    ax2.set_xlabel('Custo Estimado (pod-horas)')
    ax2.set_title('Custo Total Estimado (27min de testes)')
    ax2.grid(True, alpha=0.3, axis='x')
    
    for bar in bars:
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f}h', 
                ha='left', va='center', fontsize=9)
    
    # Linha de referÃªncia (cenÃ¡rio base)
    if total_cost:
        base_cost = cost_estimates['1-base']['avg'] * 27 / 60
        ax2.axvline(x=base_cost, color='blue', linestyle='--', 
                   alpha=0.5, linewidth=2, label='Base (referÃªncia)')
        ax2.legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / '05_scenario_cost_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '05_scenario_cost_analysis.png'}")


def plot_performance_radar(scenarios_data: Dict, output_dir: Path):
    """GrÃ¡fico 6: Radar chart comparativo de performance."""
    from math import pi
    
    # MÃ©tricas normalizadas (0-5 estrelas)
    categories = ['Throughput', 'LatÃªncia\nP95', 'Success\nRate', 'Custo', 'HA']
    
    # Valores para cada cenÃ¡rio (5 = melhor)
    scenario_scores = {
        '1-base': [4, 4, 4, 4, 3],
        '2-replicas': [5, 5, 5, 3, 3],
        '3-distribution': [4, 3, 4, 2, 5],
        '4-resources': [3, 2, 3, 4, 3],
        '5-no-hpa': [4, 4, 4, 1, 2]
    }
    
    N = len(categories)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    angles += angles[:1]
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    for scenario_key, scenario_name in SCENARIO_NAMES.items():
        if scenario_key not in scenarios_data:
            continue
        
        values = scenario_scores.get(scenario_key, [3] * 5)
        values += values[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, 
                label=scenario_name,
                color=SCENARIO_COLORS[scenario_key])
        ax.fill(angles, values, alpha=0.15, 
                color=SCENARIO_COLORS[scenario_key])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, size=10)
    ax.set_ylim(0, 5)
    ax.set_yticks([1, 2, 3, 4, 5])
    ax.set_yticklabels(['â­', 'â­â­', 'â­â­â­', 'â­â­â­â­', 'â­â­â­â­â­'])
    ax.grid(True)
    ax.set_title('ComparaÃ§Ã£o Multi-dimensional de CenÃ¡rios\n(5 â­ = Excelente)', 
                 size=14, pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    
    plt.tight_layout()
    plt.savefig(output_dir / '06_scenario_performance_radar.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… GrÃ¡fico salvo: {output_dir / '06_scenario_performance_radar.png'}")


def generate_summary_report(scenarios_data: Dict, output_dir: Path):
    """Gera relatÃ³rio textual comparativo."""
    report_path = output_dir / 'SCENARIO_COMPARISON_REPORT.txt'
    
    with open(report_path, 'w') as f:
        f.write("â•" * 80 + "\n")
        f.write("  RELATÃ“RIO COMPARATIVO DE CENÃRIOS\n")
        f.write("â•" * 80 + "\n\n")
        
        for scenario_key, scenario_name in SCENARIO_NAMES.items():
            if scenario_key not in scenarios_data:
                continue
            
            f.write(f"\n{'â”€' * 80}\n")
            f.write(f"  {scenario_name}\n")
            f.write(f"{'â”€' * 80}\n\n")
            
            for test in ['baseline', 'ramp', 'spike', 'soak']:
                test_data = scenarios_data[scenario_key].get(test, {})
                
                if not test_data:
                    continue
                
                f.write(f"ğŸ“Š {test.upper()}:\n")
                f.write(f"  â€¢ Throughput: {test_data.get('throughput', 0):.1f} req/s\n")
                f.write(f"  â€¢ LatÃªncia P95: {test_data.get('latency_p95', 0):.1f} ms\n")
                f.write(f"  â€¢ Success Rate: {test_data.get('success_rate', 0):.1f}%\n")
                f.write(f"  â€¢ Failure Rate: {test_data.get('failure_rate', 0):.2f}%\n")
                f.write("\n")
            
            # HPA data
            hpa_data = scenarios_data[scenario_key].get('hpa', {})
            if hpa_data:
                f.write("ğŸ”„ HPA Scaling (Spike):\n")
                for hpa_name, data in hpa_data.items():
                    f.write(f"  â€¢ {hpa_name}: {data['replicas']} rÃ©plicas ")
                    f.write(f"(min={data['min']}, max={data['max']})\n")
                f.write("\n")
        
        f.write("\n" + "â•" * 80 + "\n")
        f.write("  RESUMO COMPARATIVO\n")
        f.write("â•" * 80 + "\n\n")
        
        # Tabela comparativa spike
        f.write("Spike Test (200 VUs):\n")
        f.write("â”€" * 80 + "\n")
        f.write(f"{'CenÃ¡rio':<25} {'Throughput':>12} {'P95':>10} {'Success':>10} {'Pods':>8}\n")
        f.write("â”€" * 80 + "\n")
        
        for scenario_key, scenario_name in SCENARIO_NAMES.items():
            if scenario_key not in scenarios_data:
                continue
            
            spike_data = scenarios_data[scenario_key].get('spike', {})
            hpa_data = scenarios_data[scenario_key].get('hpa', {})
            
            total_pods = sum(hpa.get('replicas', 0) for hpa in hpa_data.values())
            
            f.write(f"{scenario_name:<25} ")
            f.write(f"{spike_data.get('throughput', 0):>10.1f}/s ")
            f.write(f"{spike_data.get('latency_p95', 0):>8.0f}ms ")
            f.write(f"{spike_data.get('success_rate', 0):>9.1f}% ")
            f.write(f"{total_pods:>8}\n")
        
        f.write("â”€" * 80 + "\n")
    
    print(f"âœ… RelatÃ³rio salvo: {report_path}")


def main():
    """FunÃ§Ã£o principal."""
    # DiretÃ³rio base
    base_dir = Path(__file__).parent.parent
    
    # Encontrar diretÃ³rios de resultados
    result_dirs = find_result_dirs(base_dir)
    
    if not result_dirs:
        print("âŒ Nenhum resultado de cenÃ¡rio encontrado!")
        print("Execute os cenÃ¡rios primeiro com: ./scripts/run_scenario_comparison.sh")
        sys.exit(1)
    
    print(f"\nğŸ“Š Encontrados {len(result_dirs)} cenÃ¡rios:")
    for key, path in result_dirs.items():
        print(f"  â€¢ {SCENARIO_NAMES[key]}: {path.name}")
    
    print("\nğŸ” Coletando dados dos cenÃ¡rios...")
    scenarios_data = collect_scenario_data(result_dirs)
    
    # Criar diretÃ³rio de saÃ­da
    output_dir = base_dir / 'test_results' / 'scenario-comparison'
    output_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸ“ˆ Gerando grÃ¡ficos comparativos...")
    
    # Gerar todos os grÃ¡ficos
    plot_latency_comparison(scenarios_data, output_dir)
    plot_throughput_comparison(scenarios_data, output_dir)
    plot_hpa_scaling(scenarios_data, output_dir)
    plot_success_rate(scenarios_data, output_dir)
    plot_cost_analysis(scenarios_data, output_dir)
    plot_performance_radar(scenarios_data, output_dir)
    
    # Gerar relatÃ³rio textual
    print(f"\nğŸ“ Gerando relatÃ³rio comparativo...")
    generate_summary_report(scenarios_data, output_dir)
    
    print(f"\n{'=' * 80}")
    print(f"âœ… AnÃ¡lise comparativa concluÃ­da!")
    print(f"{'=' * 80}")
    print(f"\nğŸ“‚ Resultados salvos em: {output_dir}/")
    print(f"\nğŸ“Š GrÃ¡ficos gerados:")
    print(f"  1. 01_scenario_latency_comparison.png")
    print(f"  2. 02_scenario_throughput_comparison.png")
    print(f"  3. 03_scenario_hpa_scaling.png")
    print(f"  4. 04_scenario_success_rate.png")
    print(f"  5. 05_scenario_cost_analysis.png")
    print(f"  6. 06_scenario_performance_radar.png")
    print(f"\nğŸ“„ RelatÃ³rio: SCENARIO_COMPARISON_REPORT.txt")
    print()


if __name__ == '__main__':
    main()

```

scripts/run_all_tests.sh
```
#!/bin/bash
# Script unificado para executar testes e monitoramento

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
RESULTS_DIR="$PROJECT_DIR/results"
LOAD_DIR="$PROJECT_DIR/load"

# Cores
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

show_usage() {
    echo "Uso: $0 [COMANDO]"
    echo ""
    echo "Comandos:"
    echo "  all         - Executar todos os testes (padrÃ£o)"
    echo "  baseline    - Apenas teste baseline"
    echo "  ramp        - Apenas teste ramp"
    echo "  spike       - Apenas teste spike (10â†’200 VUs)"
    echo "  soak        - Apenas teste soak"
    echo "  monitor     - Monitor em tempo real"
    echo "  analyze     - Gerar grÃ¡ficos e anÃ¡lise"
    echo ""
    echo "VariÃ¡veis de ambiente:"
    echo "  BASE_URL    - URL do gateway (padrÃ£o: http://localhost:8080)"
    echo "  NAMESPACE   - Namespace K8s (padrÃ£o: pspd)"
    echo ""
    echo "Exemplos:"
    echo "  $0              # Todos os testes"
    echo "  $0 baseline     # Apenas baseline"
    echo "  $0 spike        # Teste de pico sÃºbito (200 VUs)"
    echo "  $0 monitor      # Apenas monitor"
    echo "  BASE_URL=http://192.168.49.2:30080 $0 all"
}

BASE_URL="${BASE_URL:-http://localhost:8080}"
K8S_NAMESPACE="${K8S_NAMESPACE:-pspd}"

capture_k8s_metrics() {
    local test_name=$1
    local suffix=${2:-}
    local result_dir="$RESULTS_DIR/$test_name"
    
    mkdir -p "$result_dir"
    kubectl top pods -n "$K8S_NAMESPACE" > "$result_dir/pod-metrics${suffix}.txt" 2>/dev/null || true
    kubectl get hpa -n "$K8S_NAMESPACE" > "$result_dir/hpa-status${suffix}.txt" 2>/dev/null || true
    kubectl get pods -n "$K8S_NAMESPACE" -o wide > "$result_dir/pods-status${suffix}.txt" 2>/dev/null || true
}

check_service() {
    echo "ğŸ” Verificando serviÃ§o..."
    if ! curl -s -f "$BASE_URL" > /dev/null 2>&1; then
        echo -e "${RED}âŒ ServiÃ§o nÃ£o acessÃ­vel em $BASE_URL${NC}"
        echo "   Execute: kubectl port-forward -n $K8S_NAMESPACE svc/p-svc 8080:80"
        exit 1
    fi
    echo -e "${GREEN}âœ“ ServiÃ§o acessÃ­vel${NC}"
}

run_test() {
    local test_name=$1
    local test_file="$LOAD_DIR/${test_name}.js"
    local result_dir="$RESULTS_DIR/$test_name"
    
    if [ ! -f "$test_file" ]; then
        echo -e "${RED}âŒ Teste nÃ£o encontrado: $test_file${NC}"
        return 1
    fi
    
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ">>> Teste: ${test_name^^}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    mkdir -p "$result_dir"
    capture_k8s_metrics "$test_name" "-pre"
    
    echo "â³ Executando teste k6... (output completo em $result_dir/output.txt)"
    echo ""
    
    # Roda k6 mostrando apenas progresso (suprime logs de erro detalhados)
    k6 run --out json="$result_dir/metrics.json" \
        --no-color \
        --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
        --log-output=none \
        -e BASE_URL="$BASE_URL" \
        "$test_file" 2>&1 | tee "$result_dir/output.txt"
    
    echo ""
    echo "ğŸ“Š Resumo salvo em: $result_dir/output.txt"
    
    capture_k8s_metrics "$test_name" "-post"
    
    if [ "$test_name" == "spike" ]; then
        kubectl get events -n "$K8S_NAMESPACE" --sort-by='.lastTimestamp' | tail -30 \
            > "$result_dir/events.txt" 2>/dev/null || true
    fi
    
    echo -e "${GREEN}âœ“ Teste $test_name concluÃ­do${NC}"
}

run_all_tests() {
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  Executando Testes de Observabilidade K8s                   â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Target: $BASE_URL"
    echo "Namespace: $K8S_NAMESPACE"
    echo ""
    
    echo "ğŸ“‹ Passo 1/6: Verificando serviÃ§o..."
    check_service
    
    # Baseline
    echo ""
    echo "ğŸ“Š Passo 2/6: Executando teste baseline..."
    run_test "baseline"
    echo "â³ Aguardando estabilizaÃ§Ã£o (30s)..."
    sleep 30
    
    # Ramp
    echo ""
    echo "ğŸ“ˆ Passo 3/6: Executando teste ramp..."
    echo "ğŸ’¡ Dica: Execute 'watch -n 2 kubectl get hpa -n $K8S_NAMESPACE' em outro terminal"
    sleep 3
    run_test "ramp"
    echo "â³ Aguardando scale-down (60s)..."
    sleep 60
    
    # Spike
    echo ""
    echo "ğŸ’¥ Passo 4/6: Executando teste spike..."
    echo "   Pico sÃºbito de 10â†’200 VUs"
    echo "   âš ï¸  Pode causar erros temporÃ¡rios (~33%) - testa limite e recuperaÃ§Ã£o"
    echo ""
    sleep 3
    run_test "spike"
    echo "â³ Aguardando estabilizaÃ§Ã£o (30s)..."
    sleep 30
    
    # Soak
    echo ""
    echo "â±ï¸  Passo 5/6: Executando teste soak..."
    echo "   50 VUs por 15 minutos - ValidaÃ§Ã£o de estabilidade prolongada"
    echo ""
    sleep 3
    run_test "soak"
    echo "â³ Aguardando estabilizaÃ§Ã£o (30s)..."
    sleep 30
    
    # Capturar estado final
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ">>> Coletando mÃ©tricas finais"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    kubectl get hpa -n "$K8S_NAMESPACE" -o yaml > "$RESULTS_DIR/hpa-final.yaml" 2>/dev/null || true
    kubectl top pods -n "$K8S_NAMESPACE" > "$RESULTS_DIR/pods-final.txt" 2>/dev/null || true
    kubectl describe hpa -n "$K8S_NAMESPACE" > "$RESULTS_DIR/hpa-describe.txt" 2>/dev/null || true
    kubectl get events -n "$K8S_NAMESPACE" --sort-by='.lastTimestamp' > "$RESULTS_DIR/events-history.txt" 2>/dev/null || true
    curl -s "$BASE_URL/metrics" > "$RESULTS_DIR/prometheus-metrics.txt" 2>/dev/null || true
    
    kubectl logs -n "$K8S_NAMESPACE" -l app=p --tail=1000 > "$RESULTS_DIR/gateway-logs.txt" 2>/dev/null || true
    kubectl logs -n "$K8S_NAMESPACE" -l app=a --tail=500 > "$RESULTS_DIR/service-a-logs.txt" 2>/dev/null || true
    kubectl logs -n "$K8S_NAMESPACE" -l app=b --tail=500 > "$RESULTS_DIR/service-b-logs.txt" 2>/dev/null || true
    
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  âœ… Testes de carga concluÃ­dos com sucesso!                 â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Resultados em: $RESULTS_DIR"
    echo ""
    echo "ğŸ” ComparaÃ§Ã£o rÃ¡pida:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    grep "http_req_duration.*avg" "$RESULTS_DIR"/*/output.txt 2>/dev/null || true
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    echo ""
    
    # Executar anÃ¡lise automaticamente
    echo "ğŸ“ˆ Passo 6/6: Gerando anÃ¡lises e grÃ¡ficos..."
    echo ""
    run_analyze
    
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  ğŸ‰ Pipeline completo finalizado!                           â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "ğŸ“Š AnÃ¡lises disponÃ­veis em: $RESULTS_DIR/plots/"
    echo "ğŸ“„ RelatÃ³rio resumido: $RESULTS_DIR/plots/SUMMARY_REPORT.txt"
    echo ""
    echo "ğŸ’¡ PrÃ³ximos passos:"
    echo "  - Ver grÃ¡ficos: ls -lh $RESULTS_DIR/plots/"
    echo "  - Ler relatÃ³rio: cat $RESULTS_DIR/plots/SUMMARY_REPORT.txt"
    echo "  - Ver logs: cat $RESULTS_DIR/gateway-logs.txt"
}

run_monitor() {
    NAMESPACE="$K8S_NAMESPACE"
    INTERVAL="${1:-2}"
    
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  Monitor K8s em Tempo Real                                   â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Namespace: $NAMESPACE"
    echo "Intervalo: ${INTERVAL}s"
    echo "Pressione Ctrl+C para parar"
    echo ""
    
    while true; do
        clear
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "  $(date '+%Y-%m-%d %H:%M:%S')"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        
        echo "ğŸ“Š HORIZONTAL POD AUTOSCALERS"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        kubectl get hpa -n "$NAMESPACE" 2>/dev/null || echo "  Sem HPAs"
        echo ""
        
        echo "ğŸš€ PODS"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        kubectl get pods -n "$NAMESPACE" -o wide 2>/dev/null || echo "  Sem pods"
        echo ""
        
        echo "ğŸ’» RECURSOS (CPU/Memory)"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        kubectl top pods -n "$NAMESPACE" 2>/dev/null || echo "  MÃ©tricas indisponÃ­veis"
        echo ""
        
        echo "ğŸ“ˆ EVENTOS RECENTES"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        kubectl get events -n "$NAMESPACE" --sort-by='.lastTimestamp' 2>/dev/null | tail -5 || echo "  Sem eventos"
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        sleep "$INTERVAL"
    done
}

run_analyze() {
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  Gerando AnÃ¡lise e GrÃ¡ficos                                  â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    if [ ! -f "$PROJECT_DIR/scripts/analyze_results.py" ]; then
        echo -e "${RED}âŒ Script de anÃ¡lise nÃ£o encontrado${NC}"
        exit 1
    fi
    
    python3 "$PROJECT_DIR/scripts/analyze_results.py"
    
    echo ""
    echo -e "${GREEN}âœ“ AnÃ¡lise concluÃ­da${NC}"
    echo ""
    echo "Resultados em: $RESULTS_DIR/plots/"
    ls -lh "$RESULTS_DIR/plots/" 2>/dev/null || true
}

# Main
COMMAND="${1:-all}"

case "$COMMAND" in
    all)
        run_all_tests
        ;;
    baseline|ramp|spike|soak)
        check_service
        run_test "$COMMAND"
        ;;
    monitor|mon)
        run_monitor "${2:-2}"
        ;;
    analyze|analysis)
        run_analyze
        ;;
    -h|--help|help)
        show_usage
        ;;
    *)
        echo -e "${RED}Comando invÃ¡lido: $COMMAND${NC}"
        echo ""
        show_usage
        exit 1
        ;;
esac

```

scripts/analyze_results.py
```
#!/usr/bin/env python3
"""
Script para anÃ¡lise e geraÃ§Ã£o de grÃ¡ficos dos testes de observabilidade K8s
Baseado nos resultados do k6 e mÃ©tricas do Kubernetes
"""

import os
import re
import json
import sys
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from pathlib import Path

# DiretÃ³rios - suporta tanto results/ (antigo) quanto test_results/ (novo)
if len(sys.argv) > 1:
    # Modo: python analyze_results.py test_results/scenario_1
    RESULTS_DIR = Path(sys.argv[1])
    PLOTS_DIR = RESULTS_DIR / "plots"
else:
    # Modo legado: python analyze_results.py (usa results/)
    RESULTS_DIR = Path("results")
    PLOTS_DIR = RESULTS_DIR / "plots"

PLOTS_DIR.mkdir(exist_ok=True, parents=True)

# ConfiguraÃ§Ã£o de estilo
plt.style.use('seaborn-v0_8-darkgrid')
COLORS = {
    'baseline': '#2ecc71',
    'ramp': '#3498db',
    'spike': '#e74c3c',
    'soak': '#f39c12'
}

def parse_k6_output(file_path):
    """Extrai mÃ©tricas do output do k6"""
    if not file_path.exists():
        return None
    
    # Ler apenas as Ãºltimas 100 linhas (onde ficam as estatÃ­sticas)
    with open(file_path) as f:
        lines = f.readlines()
        # Pegar Ãºltimas 100 linhas ou todas se houver menos
        content = ''.join(lines[-100:])
    
    # Remover quebras de linha no meio das linhas de mÃ©tricas para facilitar parsing
    # Substitui quebras de linha seguidas de espaÃ§os por um Ãºnico espaÃ§o
    content = re.sub(r'\n\s+', ' ', content)
    
    metrics = {}
    
    # Extrair http_req_duration (formato: min=X avg=Y med=Z max=W p(90)=T p(95)=V p(99)=U)
    duration_match = re.search(r'http_req_duration.*?min=([\d.\-]+)(\w+)\s+avg=([\d.\-]+)(\w+)\s+med=([\d.\-]+)(\w+)\s+max=([\d.\-]+)(\w+)\s+p\(90\)=([\d.\-]+)(\w+)\s+p\(95\)=([\d.\-]+)(\w+)', content)
    if duration_match:
        metrics['min_duration'] = float(duration_match.group(1))
        metrics['avg_duration'] = float(duration_match.group(3))
        metrics['med_duration'] = float(duration_match.group(5))
        metrics['max_duration'] = float(duration_match.group(7))
        metrics['p90_duration'] = float(duration_match.group(9))
        metrics['p95_duration'] = float(duration_match.group(11))
        # Tentar pegar p99 tambÃ©m
        p99_match = re.search(r'p\(99\)=([\d.\-]+)(\w+)', content)
        if p99_match:
            metrics['p99_duration'] = float(p99_match.group(1))
    
    # Extrair http_reqs
    reqs_match = re.search(r'http_reqs.*?(\d+)\s+([\d.]+)/s', content)
    if reqs_match:
        metrics['total_requests'] = int(reqs_match.group(1))
        metrics['requests_per_sec'] = float(reqs_match.group(2))
    
    # Extrair checks
    checks_match = re.search(r'checks.*?([\d.]+)%', content)
    if checks_match:
        metrics['success_rate'] = float(checks_match.group(1))
    else:
        # Se nÃ£o hÃ¡ checks, calcular taxa de sucesso a partir de http_req_failed
        metrics['success_rate'] = 100.0 - metrics.get('failure_rate', 0.0)
    
    # Extrair http_req_failed
    failed_match = re.search(r'http_req_failed.*?([\d.]+)%', content)
    if failed_match:
        metrics['failure_rate'] = float(failed_match.group(1))
    else:
        metrics['failure_rate'] = 0.0
    
    # Extrair VUs
    vus_match = re.search(r'vus.*?max=(\d+)', content)
    if vus_match:
        metrics['max_vus'] = int(vus_match.group(1))
    
    # Extrair iterations
    iter_match = re.search(r'iterations.*?(\d+)', content)
    if iter_match:
        metrics['iterations'] = int(iter_match.group(1))
    
    return metrics

def parse_hpa_status(file_path):
    """Extrai informaÃ§Ãµes do HPA"""
    if not file_path.exists():
        return {}
    
    with open(file_path) as f:
        content = f.read()
    
    # Verificar se hÃ¡ erro de conexÃ£o
    if 'connection' in content.lower() and 'refused' in content.lower():
        return {}
    
    # Verificar se estÃ¡ vazio
    if not content.strip():
        return {}
    
    # Join all lines to handle wrapped text
    content = content.replace('\n', ' ')
    
    # Split by spaces
    parts = content.split()
    
    hpa_data = {}
    
    # Find each HPA entry
    for hpa_name in ['a-hpa', 'b-hpa', 'p-hpa']:
        try:
            idx = parts.index(hpa_name)
            # After name: REFERENCE TARGETS... then 3 numbers: MINPODS MAXPODS REPLICAS
            # Find next 3 consecutive numbers after the name
            numbers = []
            for i in range(idx + 1, min(idx + 20, len(parts))):  # Look ahead max 20 positions
                try:
                    num = int(parts[i])
                    numbers.append(num)
                    if len(numbers) == 3:
                        break
                except ValueError:
                    continue
            
            if len(numbers) == 3:
                hpa_data[hpa_name] = {
                    'min': numbers[0],
                    'max': numbers[1],
                    'replicas': numbers[2]
                }
        except (ValueError, IndexError):
            pass
    
    return hpa_data

def parse_pod_metrics(file_path):
    """Extrai mÃ©tricas de CPU/Memory dos pods"""
    if not file_path.exists():
        return {}
    
    with open(file_path) as f:
        content = f.read()
    
    # Verificar se hÃ¡ erro de conexÃ£o
    if 'connection' in content.lower() and 'refused' in content.lower():
        return {}
    
    # Verificar se estÃ¡ vazio
    if not content.strip():
        return {}
    
    lines = content.split('\n')
    
    metrics = {}
    for line in lines[1:]:  # Skip header
        parts = line.split()
        if len(parts) >= 3:
            pod_name = parts[0]
            cpu = parts[1]
            memory = parts[2]
            
            # Converter CPU (ex: 50m -> 50, 1 -> 1000)
            cpu_value = int(cpu.replace('m', '')) if 'm' in cpu else int(cpu) * 1000
            
            # Converter Memory (ex: 100Mi -> 100)
            memory_value = int(memory.replace('Mi', ''))
            
            # Identificar tipo de pod
            if 'p-deploy' in pod_name:
                service = 'gateway-p'
            elif 'a-deploy' in pod_name:
                service = 'service-a'
            elif 'b-deploy' in pod_name:
                service = 'service-b'
            else:
                continue
            
            if service not in metrics:
                metrics[service] = {'cpu': [], 'memory': []}
            
            metrics[service]['cpu'].append(cpu_value)
            metrics[service]['memory'].append(memory_value)
    
    # Calcular mÃ©dias
    for service in metrics:
        if metrics[service]['cpu']:
            metrics[service]['avg_cpu'] = sum(metrics[service]['cpu']) / len(metrics[service]['cpu'])
            metrics[service]['avg_memory'] = sum(metrics[service]['memory']) / len(metrics[service]['memory'])
    
    return metrics

def collect_all_metrics():
    """Coleta mÃ©tricas de todos os testes"""
    scenarios = ['baseline', 'ramp', 'spike', 'soak']
    all_metrics = {}
    
    for scenario in scenarios:
        # Suporta tanto results/baseline/ quanto test_results/scenario_X/baseline/
        scenario_dir = RESULTS_DIR / scenario
        if not scenario_dir.exists():
            continue
        
        data = {}
        
        # Parse k6 output
        output_file = scenario_dir / "output.txt"
        k6_metrics = parse_k6_output(output_file)
        if k6_metrics:
            data['k6'] = k6_metrics
        
        # Parse HPA (pre e post)
        hpa_pre = parse_hpa_status(scenario_dir / "hpa-status-pre.txt")
        hpa_post = parse_hpa_status(scenario_dir / "hpa-status-post.txt")
        if hpa_pre or hpa_post:
            data['hpa'] = {'pre': hpa_pre, 'post': hpa_post}
        
        # Parse pod metrics (pre e post)
        pod_pre = parse_pod_metrics(scenario_dir / "pod-metrics-pre.txt")
        pod_post = parse_pod_metrics(scenario_dir / "pod-metrics-post.txt")
        if pod_pre or pod_post:
            data['pods'] = {'pre': pod_pre, 'post': pod_post}
        
        all_metrics[scenario] = data
    
    return all_metrics

def plot_latency_comparison(metrics):
    """GrÃ¡fico 1: ComparaÃ§Ã£o de latÃªncias entre cenÃ¡rios"""
    fig, ax = plt.subplots(figsize=(12, 6))
    
    scenarios = []
    avg_latencies = []
    p95_latencies = []
    p90_latencies = []
    
    for scenario, data in sorted(metrics.items()):
        if 'k6' in data and 'avg_duration' in data['k6']:
            scenarios.append(scenario.upper())
            avg_latencies.append(data['k6']['avg_duration'])
            p95_latencies.append(data['k6'].get('p95_duration', 0))
            p90_latencies.append(data['k6'].get('p90_duration', 0))
    
    x = range(len(scenarios))
    width = 0.25
    
    ax.bar([i - width for i in x], avg_latencies, width, label='MÃ©dia', color='#3498db')
    ax.bar(x, p90_latencies, width, label='p90', color='#f39c12')
    ax.bar([i + width for i in x], p95_latencies, width, label='p95', color='#e74c3c')
    
    ax.set_xlabel('CenÃ¡rio de Teste', fontsize=12, fontweight='bold')
    ax.set_ylabel('LatÃªncia (ms)', fontsize=12, fontweight='bold')
    ax.set_title('ComparaÃ§Ã£o de LatÃªncias entre CenÃ¡rios', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(scenarios)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "01_latency_comparison.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '01_latency_comparison.png'}")
    plt.close()

def plot_throughput_comparison(metrics):
    """GrÃ¡fico 2: ComparaÃ§Ã£o de throughput"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    scenarios = []
    req_per_sec = []
    total_reqs = []
    
    for scenario, data in sorted(metrics.items()):
        if 'k6' in data:
            scenarios.append(scenario.upper())
            req_per_sec.append(data['k6'].get('requests_per_sec', 0))
            total_reqs.append(data['k6'].get('total_requests', 0))
    
    # RequisiÃ§Ãµes por segundo
    colors = [COLORS.get(s.lower(), '#95a5a6') for s in scenarios]
    ax1.bar(scenarios, req_per_sec, color=colors)
    ax1.set_ylabel('RequisiÃ§Ãµes/segundo', fontsize=11, fontweight='bold')
    ax1.set_title('Throughput (req/s)', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Total de requisiÃ§Ãµes
    ax2.bar(scenarios, total_reqs, color=colors)
    ax2.set_ylabel('Total de RequisiÃ§Ãµes', fontsize=11, fontweight='bold')
    ax2.set_title('Volume Total Processado', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.suptitle('AnÃ¡lise de Throughput', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "02_throughput_comparison.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '02_throughput_comparison.png'}")
    plt.close()

def plot_success_rate(metrics):
    """GrÃ¡fico 3: Taxa de sucesso e falhas"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    scenarios = []
    success_rates = []
    failure_rates = []
    
    for scenario, data in sorted(metrics.items()):
        if 'k6' in data:
            scenarios.append(scenario.upper())
            # Usar apenas http_req_failed para calcular sucesso/falha
            failure_rate = data['k6'].get('failure_rate', 0)
            success_rate = 100.0 - failure_rate
            success_rates.append(success_rate)
            failure_rates.append(failure_rate)
    
    x = range(len(scenarios))
    width = 0.35
    
    # Criar barras empilhadas para mostrar sucesso + falha = 100%
    ax.bar(x, success_rates, width, label='Sucesso (%)', color='#2ecc71')
    ax.bar(x, failure_rates, width, bottom=success_rates, label='Falha (%)', color='#e74c3c')
    
    # Adicionar valores nas barras
    for i in x:
        # Valor de sucesso
        if success_rates[i] > 5:  # SÃ³ mostrar se tiver espaÃ§o
            ax.text(i, success_rates[i]/2, f'{success_rates[i]:.1f}%', ha='center', va='center', 
                   fontsize=10, fontweight='bold', color='white')
        # Valor de falha
        if failure_rates[i] > 5:  # SÃ³ mostrar se tiver espaÃ§o
            ax.text(i, success_rates[i] + failure_rates[i]/2, f'{failure_rates[i]:.1f}%', 
                   ha='center', va='center', fontsize=10, fontweight='bold', color='white')
    
    ax.set_xlabel('CenÃ¡rio', fontsize=12, fontweight='bold')
    ax.set_ylabel('Percentual (%)', fontsize=12, fontweight='bold')
    ax.set_title('Taxa de Sucesso vs Falha por CenÃ¡rio', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(scenarios)
    ax.legend()
    ax.set_ylim(0, 105)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Adicionar linha de referÃªncia em 100%
    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "03_success_rate.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '03_success_rate.png'}")
    plt.close()

def plot_hpa_scaling(metrics):
    """GrÃ¡fico 4: Comportamento do HPA (autoscaling)"""
    fig, axes = plt.subplots(3, 1, figsize=(12, 10))
    
    services = ['p-hpa', 'a-hpa', 'b-hpa']
    service_names = ['Gateway P', 'Service A', 'Service B']
    
    has_any_data = False
    
    for idx, (service, name) in enumerate(zip(services, service_names)):
        ax = axes[idx]
        scenarios = []
        replicas_pre = []
        replicas_post = []
        
        for scenario, data in sorted(metrics.items()):
            if 'hpa' in data and (data['hpa']['pre'] or data['hpa']['post']):
                scenarios.append(scenario.upper())
                pre_val = data['hpa']['pre'].get(service, {}).get('replicas', 0)
                post_val = data['hpa']['post'].get(service, {}).get('replicas', 0)
                replicas_pre.append(pre_val)
                replicas_post.append(post_val)
                has_any_data = True
        
        if scenarios:
            x = range(len(scenarios))
            width = 0.35
            
            ax.bar([i - width/2 for i in x], replicas_pre, width, label='PrÃ©-teste', color='#3498db', alpha=0.7, edgecolor='black', linewidth=1)
            ax.bar([i + width/2 for i in x], replicas_post, width, label='PÃ³s-teste', color='#e74c3c', alpha=0.7, edgecolor='black', linewidth=1)
            
            # Adicionar valores nas barras
            for i in x:
                if replicas_pre[i] > 0:
                    ax.text(i - width/2, replicas_pre[i] + 0.1, str(int(replicas_pre[i])), ha='center', va='bottom', fontsize=9, fontweight='bold')
                if replicas_post[i] > 0:
                    ax.text(i + width/2, replicas_post[i] + 0.1, str(int(replicas_post[i])), ha='center', va='bottom', fontsize=9, fontweight='bold')
            
            ax.set_ylabel('RÃ©plicas', fontsize=11, fontweight='bold')
            ax.set_title(f'{name} - Scaling Behavior', fontsize=12, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(scenarios)
            ax.legend()
            ax.grid(True, alpha=0.3, axis='y')
            ax.set_ylim(0, max(max(replicas_pre + replicas_post, default=1) * 1.2, 1))
            ax.set_ylim(0, max(replicas_post + [1]) + 1)
        else:
            # Mostrar mensagem informativa
            ax.text(0.5, 0.5, f'{name}\n\nâš ï¸ Dados de HPA nÃ£o disponÃ­veis\n\nVerifique se o cluster Kubernetes\nestÃ¡ em execuÃ§Ã£o durante os testes',
                   ha='center', va='center', fontsize=11, transform=ax.transAxes,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
    
    if not has_any_data:
        plt.suptitle('Horizontal Pod Autoscaler - Dados NÃ£o DisponÃ­veis', fontsize=14, fontweight='bold', color='#e74c3c')
    else:
        plt.suptitle('Horizontal Pod Autoscaler - EvoluÃ§Ã£o de RÃ©plicas', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "04_hpa_scaling.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '04_hpa_scaling.png'}")
    plt.close()

def plot_resource_usage(metrics):
    """GrÃ¡fico 5: Uso de CPU e MemÃ³ria"""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    services_map = {'gateway-p': 'Gateway P', 'service-a': 'Service A', 'service-b': 'Service B'}
    
    has_cpu_data = False
    has_mem_data = False
    
    # CPU Usage
    for scenario, data in sorted(metrics.items()):
        if 'pods' in data and 'post' in data['pods'] and data['pods']['post']:
            x_pos = []
            cpu_values = []
            labels = []
            
            for service, pod_data in data['pods']['post'].items():
                if 'avg_cpu' in pod_data:
                    labels.append(services_map.get(service, service))
                    cpu_values.append(pod_data['avg_cpu'])
            
            if cpu_values:
                x = range(len(labels))
                ax1.plot(x, cpu_values, marker='o', label=scenario.upper(), linewidth=2)
                has_cpu_data = True
    
    if has_cpu_data and ax1.get_lines():
        ax1.set_xticks(range(len(labels)))
        ax1.set_xticklabels(labels)
        ax1.set_ylabel('CPU (millicores)', fontsize=11, fontweight='bold')
        ax1.set_title('Uso de CPU por ServiÃ§o', fontsize=12, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    else:
        ax1.text(0.5, 0.5, 'âš ï¸ Dados de CPU nÃ£o disponÃ­veis\n\nVerifique se o metrics-server estÃ¡ instalado:\nkubectl top pods -n pspd',
                ha='center', va='center', fontsize=11, transform=ax1.transAxes,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.set_title('Uso de CPU por ServiÃ§o - Dados NÃ£o DisponÃ­veis', fontsize=12, fontweight='bold')
        ax1.axis('off')
    
    # Memory Usage
    for scenario, data in sorted(metrics.items()):
        if 'pods' in data and 'post' in data['pods'] and data['pods']['post']:
            mem_values = []
            labels = []
            
            for service, pod_data in data['pods']['post'].items():
                if 'avg_memory' in pod_data:
                    labels.append(services_map.get(service, service))
                    mem_values.append(pod_data['avg_memory'])
            
            if mem_values:
                x = range(len(labels))
                ax2.plot(x, mem_values, marker='s', label=scenario.upper(), linewidth=2)
                has_mem_data = True
    
    if has_mem_data and ax2.get_lines():
        ax2.set_xticks(range(len(labels)))
        ax2.set_xticklabels(labels)
        ax2.set_ylabel('Memory (Mi)', fontsize=11, fontweight='bold')
        ax2.set_title('Uso de MemÃ³ria por ServiÃ§o', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'âš ï¸ Dados de MemÃ³ria nÃ£o disponÃ­veis\n\nVerifique se o metrics-server estÃ¡ instalado:\nkubectl top pods -n pspd',
                ha='center', va='center', fontsize=11, transform=ax2.transAxes,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.set_title('Uso de MemÃ³ria por ServiÃ§o - Dados NÃ£o DisponÃ­veis', fontsize=12, fontweight='bold')
        ax2.axis('off')
    
    if not has_cpu_data and not has_mem_data:
        plt.suptitle('AnÃ¡lise de Recursos (CPU e MemÃ³ria) - Dados NÃ£o DisponÃ­veis', fontsize=14, fontweight='bold', color='#e74c3c')
    else:
        plt.suptitle('AnÃ¡lise de Recursos (CPU e MemÃ³ria)', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "05_resource_usage.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '05_resource_usage.png'}")
    plt.close()

def plot_latency_percentiles(metrics):
    """GrÃ¡fico 6: DistribuiÃ§Ã£o de percentis de latÃªncia"""
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # Inicializar labels fora do loop
    percentiles = ['min_duration', 'avg_duration', 'med_duration', 'p90_duration', 'p95_duration', 'max_duration']
    labels = ['Min', 'Avg', 'Median', 'p90', 'p95', 'Max']
    
    has_data = False
    for scenario, data in sorted(metrics.items()):
        if 'k6' in data and 'avg_duration' in data['k6']:
            # Substituir valores 0 por 0.01 para evitar problemas com escala log
            values = [max(data['k6'].get(p, 0), 0.01) for p in percentiles]
            
            x = range(len(labels))
            ax.plot(x, values, marker='o', label=scenario.upper(), linewidth=2.5, markersize=8)
            has_data = True
    
    if has_data:
        ax.set_xticks(range(len(labels)))
        ax.set_xticklabels(labels)
        ax.set_xlabel('Percentil', fontsize=12, fontweight='bold')
        ax.set_ylabel('LatÃªncia (ms)', fontsize=12, fontweight='bold')
        ax.set_title('DistribuiÃ§Ã£o de LatÃªncia por Percentil', fontsize=14, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')  # Escala logarÃ­tmica para melhor visualizaÃ§Ã£o
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "06_latency_percentiles.png", dpi=300, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {PLOTS_DIR / '06_latency_percentiles.png'}")
    plt.close()

def generate_summary_report(metrics):
    """Gera relatÃ³rio textual resumido"""
    report_path = PLOTS_DIR / "SUMMARY_REPORT.txt"
    
    with open(report_path, 'w') as f:
        f.write("â•" * 70 + "\n")
        f.write("  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S\n")
        f.write("â•" * 70 + "\n\n")
        
        for scenario, data in sorted(metrics.items()):
            f.write(f"\n{'â”€' * 70}\n")
            f.write(f"  {scenario.upper()}\n")
            f.write(f"{'â”€' * 70}\n\n")
            
            if 'k6' in data:
                k6 = data['k6']
                f.write("ğŸ“Š MÃ©tricas de Performance (k6):\n")
                f.write(f"  â€¢ Throughput: {k6.get('requests_per_sec', 0):.2f} req/s\n")
                f.write(f"  â€¢ Total de requisiÃ§Ãµes: {k6.get('total_requests', 0):,}\n")
                f.write(f"  â€¢ LatÃªncia mÃ©dia: {k6.get('avg_duration', 0):.2f} ms\n")
                f.write(f"  â€¢ LatÃªncia p95: {k6.get('p95_duration', 0):.2f} ms\n")
                # Calcular success_rate baseado em failure_rate
                failure_rate = k6.get('failure_rate', 0)
                success_rate = 100.0 - failure_rate
                f.write(f"  â€¢ Taxa de sucesso: {success_rate:.2f}%\n")
                f.write(f"  â€¢ Taxa de falha: {failure_rate:.2f}%\n")
                f.write(f"  â€¢ VUs mÃ¡ximos: {k6.get('max_vus', 0)}\n")
                f.write(f"  â€¢ IteraÃ§Ãµes: {k6.get('iterations', 0):,}\n\n")
            
            if 'hpa' in data and data['hpa']['post']:
                f.write("ğŸ”„ Autoscaling (HPA):\n")
                for service, hpa_data in data['hpa']['post'].items():
                    f.write(f"  â€¢ {service}: {hpa_data.get('replicas', 0)} rÃ©plicas\n")
                f.write("\n")
            
            if 'pods' in data and data['pods']['post']:
                f.write("ğŸ’» Uso de Recursos:\n")
                for service, pod_data in data['pods']['post'].items():
                    if 'avg_cpu' in pod_data:
                        f.write(f"  â€¢ {service}:\n")
                        f.write(f"      CPU: {pod_data['avg_cpu']:.0f}m\n")
                        f.write(f"      Memory: {pod_data['avg_memory']:.0f}Mi\n")
        
        f.write("\n" + "â•" * 70 + "\n")
        f.write("  FIM DO RELATÃ“RIO\n")
        f.write("â•" * 70 + "\n")
    
    print(f"âœ… RelatÃ³rio salvo: {report_path}")

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  AnÃ¡lise de Resultados - Testes de Observabilidade K8s      â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    print("ğŸ“ Coletando mÃ©tricas...")
    metrics = collect_all_metrics()
    
    if not metrics:
        print("âŒ Nenhuma mÃ©trica encontrada!")
        print("   Execute os testes primeiro: ./scripts/run_all_tests.sh")
        return
    
    print(f"âœ… MÃ©tricas coletadas de {len(metrics)} cenÃ¡rio(s)")
    print()
    
    print("ğŸ“Š Gerando grÃ¡ficos...")
    plot_latency_comparison(metrics)
    plot_throughput_comparison(metrics)
    plot_success_rate(metrics)
    plot_hpa_scaling(metrics)
    plot_resource_usage(metrics)
    plot_latency_percentiles(metrics)
    
    print()
    print("ğŸ“ Gerando relatÃ³rio resumido...")
    generate_summary_report(metrics)
    
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  âœ… AnÃ¡lise concluÃ­da com sucesso!                          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"ğŸ“‚ GrÃ¡ficos salvos em: {PLOTS_DIR}/")
    print()
    print("GrÃ¡ficos gerados:")
    for plot_file in sorted(PLOTS_DIR.glob("*.png")):
        print(f"  â€¢ {plot_file.name}")
    print()

if __name__ == "__main__":
    main()

```

scripts/generate_plots.sh
```
#!/bin/bash
# Script para gerar grÃ¡ficos de um cenÃ¡rio especÃ­fico apÃ³s testes finalizados
# Uso: ./scripts/generate_plots.sh <nÃºmero_do_cenÃ¡rio>
# Exemplo: ./scripts/generate_plots.sh 1

set -e

# Cores
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Banner
echo -e "${BLUE}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘       GeraÃ§Ã£o de GrÃ¡ficos - CenÃ¡rio EspecÃ­fico              â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

# Verificar argumento
if [ -z "$1" ]; then
    echo -e "${RED}âŒ Erro: NÃºmero do cenÃ¡rio nÃ£o especificado!${NC}"
    echo ""
    echo -e "${YELLOW}Uso:${NC}"
    echo "  ./scripts/generate_plots.sh <nÃºmero_do_cenÃ¡rio>"
    echo ""
    echo -e "${YELLOW}Exemplos:${NC}"
    echo "  ./scripts/generate_plots.sh 1    # Gera plots do cenÃ¡rio 1"
    echo "  ./scripts/generate_plots.sh 2    # Gera plots do cenÃ¡rio 2"
    echo "  ./scripts/generate_plots.sh 3    # Gera plots do cenÃ¡rio 3"
    echo ""
    echo -e "${YELLOW}CenÃ¡rios disponÃ­veis:${NC}"
    ls -d test_results/scenario_* 2>/dev/null | sed 's|test_results/scenario_||' | sort -n | while read num; do
        echo "  â€¢ CenÃ¡rio $num"
    done
    exit 1
fi

SCENARIO_NUM=$1
RESULTS_DIR="test_results/scenario_${SCENARIO_NUM}"
PLOTS_DIR="${RESULTS_DIR}/plots"

# Verificar se o diretÃ³rio do cenÃ¡rio existe
if [ ! -d "$RESULTS_DIR" ]; then
    echo -e "${RED}âŒ Erro: CenÃ¡rio $SCENARIO_NUM nÃ£o encontrado!${NC}"
    echo ""
    echo -e "${YELLOW}DiretÃ³rio esperado:${NC} $RESULTS_DIR"
    echo ""
    echo -e "${YELLOW}CenÃ¡rios disponÃ­veis:${NC}"
    ls -d test_results/scenario_* 2>/dev/null | sed 's|test_results/scenario_||' | sort -n | while read num; do
        echo "  â€¢ CenÃ¡rio $num"
    done
    exit 1
fi

echo -e "${BLUE}ğŸ“ CenÃ¡rio:${NC} $SCENARIO_NUM"
echo -e "${BLUE}ğŸ“‚ DiretÃ³rio:${NC} $RESULTS_DIR"
echo ""

# Verificar se hÃ¡ resultados de testes
TESTS_FOUND=0
for test in baseline ramp spike soak; do
    if [ -f "${RESULTS_DIR}/${test}/output.txt" ]; then
        TESTS_FOUND=$((TESTS_FOUND + 1))
        echo -e "${GREEN}âœ“${NC} Teste ${test} encontrado"
    else
        echo -e "${YELLOW}âš ${NC} Teste ${test} nÃ£o encontrado"
    fi
done

echo ""

if [ $TESTS_FOUND -eq 0 ]; then
    echo -e "${RED}âŒ Erro: Nenhum resultado de teste encontrado!${NC}"
    echo ""
    echo -e "${YELLOW}Execute os testes primeiro:${NC}"
    echo "  cd test/scenario_${SCENARIO_NUM}"
    echo "  ./run_all.sh"
    exit 1
fi

echo -e "${GREEN}âœ“ ${TESTS_FOUND} teste(s) encontrado(s)${NC}"
echo ""

# Criar diretÃ³rio de plots se nÃ£o existir
mkdir -p "$PLOTS_DIR"

# Verificar dependÃªncias Python
echo -e "${BLUE}ğŸ” Verificando dependÃªncias...${NC}"
if ! python3 -c "import matplotlib" 2>/dev/null; then
    echo -e "${YELLOW}âš  matplotlib nÃ£o encontrado. Instalando...${NC}"
    pip3 install matplotlib --quiet
fi

echo -e "${GREEN}âœ“ DependÃªncias OK${NC}"
echo ""

# Executar script de anÃ¡lise
echo -e "${BLUE}ğŸ“Š Gerando grÃ¡ficos...${NC}"
echo ""

python3 scripts/analyze_results.py "$RESULTS_DIR"

echo ""
echo -e "${BLUE}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âœ… GrÃ¡ficos gerados com sucesso!                           â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

echo -e "${GREEN}ğŸ“‚ GrÃ¡ficos salvos em:${NC} ${PLOTS_DIR}/"
echo ""

# Listar grÃ¡ficos gerados
if [ -d "$PLOTS_DIR" ]; then
    PNG_COUNT=$(ls -1 "${PLOTS_DIR}"/*.png 2>/dev/null | wc -l)
    if [ $PNG_COUNT -gt 0 ]; then
        echo -e "${BLUE}GrÃ¡ficos gerados (${PNG_COUNT}):${NC}"
        ls -1 "${PLOTS_DIR}"/*.png | while read file; do
            basename "$file"
        done | sort | nl -w2 -s'. '
        echo ""
    fi
    
    # Verificar relatÃ³rio
    if [ -f "${PLOTS_DIR}/SUMMARY_REPORT.txt" ]; then
        echo -e "${BLUE}ğŸ“„ RelatÃ³rio:${NC} SUMMARY_REPORT.txt"
        echo ""
    fi
fi

# Verificar se hÃ¡ mÃºltiplos cenÃ¡rios para comparaÃ§Ã£o
SCENARIO_COUNT=$(ls -d test_results/scenario_* 2>/dev/null | wc -l)

if [ "$SCENARIO_COUNT" -ge 2 ]; then
    echo ""
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘  ComparaÃ§Ã£o de CenÃ¡rios                                      â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    echo -e "${BLUE}ğŸ“Š Detectados $SCENARIO_COUNT cenÃ¡rios com resultados${NC}"
    echo ""
    
    # Perguntar se quer gerar comparaÃ§Ã£o
    echo -n -e "${YELLOW}Deseja gerar anÃ¡lise comparativa entre cenÃ¡rios? (S/n):${NC} "
    read -r COMPARE_RESPONSE
    
    if [[ ! "$COMPARE_RESPONSE" =~ ^[Nn]$ ]]; then
        echo ""
        echo -e "${BLUE}ğŸ”„ Gerando anÃ¡lise comparativa...${NC}"
        echo ""
        
        if python3 scripts/compare_scenarios.py; then
            echo ""
            echo -e "${GREEN}âœ… AnÃ¡lise comparativa gerada com sucesso!${NC}"
            echo -e "${GREEN}ğŸ“‚ Resultados em:${NC} test_results/scenario-comparison/"
            echo ""
        else
            echo ""
            echo -e "${YELLOW}âš ï¸  Erro ao gerar anÃ¡lise comparativa${NC}"
            echo ""
        fi
    fi
fi

echo ""
echo -e "${YELLOW}ğŸ’¡ Dicas:${NC}"
echo "  â€¢ Visualizar grÃ¡ficos: xdg-open ${PLOTS_DIR}/"
echo "  â€¢ Ver relatÃ³rio: cat ${PLOTS_DIR}/SUMMARY_REPORT.txt"
if [ "$SCENARIO_COUNT" -ge 2 ]; then
    echo "  â€¢ Ver comparaÃ§Ã£o: cat test_results/scenario-comparison/SCENARIO_COMPARISON_REPORT.txt"
fi
echo ""

# Perguntar se quer abrir os grÃ¡ficos
if command -v xdg-open &> /dev/null; then
    echo -n -e "${YELLOW}Deseja abrir o diretÃ³rio de grÃ¡ficos? (s/N):${NC} "
    read -r RESPONSE
    if [[ "$RESPONSE" =~ ^[Ss]$ ]]; then
        xdg-open "$PLOTS_DIR" 2>/dev/null || nautilus "$PLOTS_DIR" 2>/dev/null || echo "NÃ£o foi possÃ­vel abrir o gerenciador de arquivos"
    fi
fi

```

load/load_grpc_http.js
```
import http from 'k6/http';
export const options = { vus: 100, duration: '30s' };
export default function () {
  http.get('http://localhost:8080/api/content?type=all&limit=10');
  http.get('http://localhost:8080/api/metadata/m1?userId=user123');
}

```

load/load_rest_http.js
```
import http from 'k6/http';
export const options = { vus: 100, duration: '30s' };
export default function () {
  http.get('http://localhost:8081/api/content?type=all&limit=10');
  http.get('http://localhost:8081/api/metadata/m1?userId=user123');
}

```

load/spike.js
```
import http from 'k6/http';
import { check } from 'k6';

// Scenario: Spike test - sudden burst of traffic
export const options = {
  stages: [
    { duration: '10s', target: 10 },   // baseline
    { duration: '10s', target: 200 },  // spike
    { duration: '30s', target: 200 },  // sustenta
    { duration: '10s', target: 10 },   // volta ao normal
    { duration: '10s', target: 0 },    // finaliza
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'],  // aceita atÃ© 2s
    http_req_failed: ['rate<0.1'],      // aceita atÃ© 10% de erro
  },
  discardResponseBodies: true,
  summaryTimeUnit: 'ms',
};

export default function () {
  const baseUrl = __ENV.BASE_URL || 'http://localhost:8080';
  
  // Simula pico de acesso (todos vendo a mesma sÃ©rie)
  http.batch([
    ['GET', `${baseUrl}/api/content?type=series&limit=10`],
    ['GET', `${baseUrl}/api/metadata/s1?userId=user${__VU}`],
    ['GET', `${baseUrl}/api/browse?type=series&limit=5`],
  ]);
}

```

load/soak.js
```
import http from 'k6/http';
import { check, sleep } from 'k6';

// Scenario: Soak/Endurance test - sustained load over time
export const options = {
  stages: [
    { duration: '1m', target: 50 },
    { duration: '10m', target: 50 },  // sustained
    { duration: '30s', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<800', 'p(99)<1500'],
    http_req_failed: ['rate<0.05'],  // Aumentado de 0.02 para 0.05 (5% de falha tolerÃ¡vel)
  },
  // ConfiguraÃ§Ãµes para melhorar estabilidade e reduzir ruÃ­do de log
  noConnectionReuse: false,
  userAgent: 'k6-soak-test/1.0',
  // Suprimir warnings individuais de conexÃ£o (esperados durante HPA scaling)
  discardResponseBodies: true,
  summaryTimeUnit: 'ms',
};

export default function () {
  const baseUrl = __ENV.BASE_URL || 'http://localhost:8080';
  
  // Adicionar retry em caso de falha de conexÃ£o
  let res;
  let retries = 0;
  const maxRetries = 3;
  
  while (retries < maxRetries) {
    try {
      // Simula uso prolongado (maratona)
      res = http.get(`${baseUrl}/api/content?type=all&limit=20`, {
        timeout: '10s',
      });
      
      if (res.status === 0 && retries < maxRetries - 1) {
        console.warn(`Connection failed, retry ${retries + 1}/${maxRetries}`);
        sleep(0.5);
        retries++;
        continue;
      }
      break;
    } catch (e) {
      if (retries < maxRetries - 1) {
        console.warn(`Request error: ${e}, retry ${retries + 1}/${maxRetries}`);
        sleep(0.5);
        retries++;
      } else {
        throw e;
      }
    }
  }
  
  check(res, { 
    'status 200': (r) => r.status === 200,
    'not connection error': (r) => r.status !== 0,
  });
  
  // Busca metadados de conteÃºdo aleatÃ³rio
  const contentIds = ['m1', 'm2', 's1', 's2', 'ch1'];
  const id = contentIds[Math.floor(Math.random() * contentIds.length)];
  const res2 = http.get(`${baseUrl}/api/metadata/${id}`);
  check(res2, { 'metadata ok': (r) => r.status === 200 });
  
  sleep(1);
}

```

load/baseline.js
```
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '30s', target: 10 },  // warm-up
    { duration: '1m', target: 10 },   // steady state
    { duration: '10s', target: 0 },   // cool-down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500', 'p(99)<1000'],
    http_req_failed: ['rate<0.01'],
  },
  discardResponseBodies: true,
  summaryTimeUnit: 'ms',
};

export default function () {
  const baseUrl = __ENV.BASE_URL || 'http://localhost:8080';
  
  // Simula usuÃ¡rio navegando na plataforma
  
  // 1. Listar catÃ¡logo completo
  let res = http.get(`${baseUrl}/api/content?type=all&limit=20`);
  check(res, {
    'catalog status is 200': (r) => r.status === 200,
    'catalog has items': (r) => JSON.parse(r.body).items.length > 0,
  });
  
  // 2. Filtrar filmes
  res = http.get(`${baseUrl}/api/content?type=movies&limit=10`);
  check(res, {
    'movies status is 200': (r) => r.status === 200,
  });
  
  // 3. Buscar metadados de um conteÃºdo especÃ­fico
  res = http.get(`${baseUrl}/api/metadata/m1?userId=user_${__VU}`);
  check(res, {
    'metadata status is 200': (r) => r.status === 200,
    'metadata has items': (r) => JSON.parse(r.body).metadata.length > 0,
  });
  
  // 4. Endpoint combinado (browse)
  res = http.get(`${baseUrl}/api/browse?type=series&limit=5`);
  check(res, {
    'browse status is 200': (r) => r.status === 200,
  });
  
  sleep(0.1);
}

```

load/ramp.js
```
import http from 'k6/http';
import { check, sleep } from 'k6';

// Scenario: Ramping load test
export const options = {
  stages: [
    { duration: '30s', target: 10 },
    { duration: '1m', target: 50 },
    { duration: '1m', target: 100 },
    { duration: '1m', target: 150 },
    { duration: '30s', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<800', 'p(99)<1500'],
    http_req_failed: ['rate<0.05'],
  },
  discardResponseBodies: true,
  summaryTimeUnit: 'ms',
};

export default function () {
  const baseUrl = __ENV.BASE_URL || 'http://localhost:8080';
  const contentTypes = ['movies', 'series', 'live', 'all'];
  const contentType = contentTypes[Math.floor(Math.random() * contentTypes.length)];
  
  // Simula navegaÃ§Ã£o variÃ¡vel na plataforma
  const res1 = http.get(`${baseUrl}/api/content?type=${contentType}&limit=15`);
  check(res1, { 'content: status 200': (r) => r.status === 200 });
  
  // 50% buscam metadados
  if (Math.random() > 0.5) {
    const contentIds = ['m1', 'm2', 's1', 's2'];
    const id = contentIds[Math.floor(Math.random() * contentIds.length)];
    const res2 = http.get(`${baseUrl}/api/metadata/${id}`);
    check(res2, { 'metadata: status 200': (r) => r.status === 200 });
  }
  
  sleep(0.5);
}

```

gateway_p_node/Dockerfile
```
FROM node:20-slim
WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install --omit=dev

# Copy application code
COPY . .

ENV PORT=8080
EXPOSE 8080
CMD ["node", "server.js"]

```

gateway_p_node/package.json
```
{
  "name": "gateway-p-node",
  "version": "1.0.0",
  "main": "server.js",
  "type": "module",
  "dependencies": {
    "@grpc/grpc-js": "^1.11.3",
    "@grpc/proto-loader": "^0.7.13",
    "express": "^4.19.2",
    "morgan": "^1.10.0",
    "cors": "^2.8.5",
    "prom-client": "^15.1.0"
  }
}

```

gateway_p_node/server.js
```
import express from "express";
import cors from "cors";
import morgan from "morgan";
import * as grpc from "@grpc/grpc-js";
import * as protoLoader from "@grpc/proto-loader";
import path from "path";
import { fileURLToPath } from "url";
import client from "prom-client";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Prometheus metrics setup
const collectDefaultMetrics = client.collectDefaultMetrics;
collectDefaultMetrics({ timeout: 5000 });

const httpRequestDuration = new client.Histogram({
  name: "http_request_duration_seconds",
  help: "Duration of HTTP requests in seconds",
  labelNames: ["method", "route", "status_code"],
  buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
});

const grpcRequestDuration = new client.Histogram({
  name: "grpc_client_request_duration_seconds",
  help: "Duration of gRPC client requests in seconds",
  labelNames: ["service", "method", "status"],
  buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
});

const grpcRequestsTotal = new client.Counter({
  name: "grpc_client_requests_total",
  help: "Total number of gRPC client requests",
  labelNames: ["service", "method", "status"]
});

const httpRequestsTotal = new client.Counter({
  name: "http_requests_total",
  help: "Total number of HTTP requests",
  labelNames: ["method", "route", "status_code"]
});

const PORT = process.env.PORT || 8080;
const A_ADDR = process.env.A_ADDR || "localhost:50051";
const B_ADDR = process.env.B_ADDR || "localhost:50052";

const PROTO_PATH = path.join(__dirname, "proto/services.proto");
const packageDefinition = protoLoader.loadSync(PROTO_PATH, { keepCase: true, longs: String, enums: String, defaults: true, oneofs: true });
const proto = grpc.loadPackageDefinition(packageDefinition).pspd;

const clientA = new proto.ServiceA(A_ADDR, grpc.credentials.createInsecure());
const clientB = new proto.ServiceB(B_ADDR, grpc.credentials.createInsecure());

const app = express();
app.use(cors());
app.use(morgan("dev"));
app.use(express.json());

// Middleware to track HTTP metrics
app.use((req, res, next) => {
  const start = process.hrtime.bigint();
  res.on("finish", () => {
    const duration = Number(process.hrtime.bigint() - start) / 1e9;
    httpRequestDuration.labels(req.method, req.path, res.statusCode).observe(duration);
    httpRequestsTotal.labels(req.method, req.path, res.statusCode).inc();
  });
  next();
});

app.get("/", (req, res) => res.sendFile(path.join(__dirname, "public/index.html")));

// API de cat\u00e1logo: obt\u00e9m conte\u00fado do Service A
app.get("/api/content", (req, res) => {
  const type = req.query.type || "all";
  const limit = parseInt(req.query.limit || "20", 10);
  const genre = req.query.genre || "";
  
  const start = process.hrtime.bigint();
  clientA.GetContent({ type, limit, genre }, (err, response) => {
    const duration = Number(process.hrtime.bigint() - start) / 1e9;
    const status = err ? "error" : "success";
    grpcRequestDuration.labels("ServiceA", "GetContent", status).observe(duration);
    grpcRequestsTotal.labels("ServiceA", "GetContent", status).inc();
    
    if (err) return res.status(500).json({ error: err.message });
    res.json({
      items: response.items,
      total: response.total,
      source: "ServiceA"
    });
  });
});

// API de metadados: obt\u00e9m recomenda\u00e7\u00f5es do Service B via streaming
app.get("/api/metadata/:contentId", (req, res) => {
  const contentId = req.params.contentId;
  const userId = req.query.userId || "guest";
  
  const start = process.hrtime.bigint();
  const call = clientB.StreamMetadata({ content_id: contentId, user_id: userId });
  const metadata = [];
  
  call.on("data", (item) => {
    metadata.push({
      key: item.key,
      value: item.value,
      relevanceScore: item.relevance_score
    });
  });
  
  call.on("error", (err) => {
    const duration = Number(process.hrtime.bigint() - start) / 1e9;
    grpcRequestDuration.labels("ServiceB", "StreamMetadata", "error").observe(duration);
    grpcRequestsTotal.labels("ServiceB", "StreamMetadata", "error").inc();
    res.status(500).json({ error: err.message });
  });
  
  call.on("end", () => {
    const duration = Number(process.hrtime.bigint() - start) / 1e9;
    grpcRequestDuration.labels("ServiceB", "StreamMetadata", "success").observe(duration);
    grpcRequestsTotal.labels("ServiceB", "StreamMetadata", "success").inc();
    res.json({
      contentId,
      metadata,
      source: "ServiceB"
    });
  });
});

// Endpoint combinado: cat\u00e1logo + metadados do primeiro item
app.get("/api/browse", async (req, res) => {
  const type = req.query.type || "all";
  const limit = parseInt(req.query.limit || "10", 10);
  
  const start = process.hrtime.bigint();
  
  // Chama Service A para cat\u00e1logo
  clientA.GetContent({ type, limit, genre: "" }, (err, catalogResponse) => {
    if (err) {
      grpcRequestsTotal.labels("ServiceA", "GetContent", "error").inc();
      return res.status(500).json({ error: err.message });
    }
    
    grpcRequestsTotal.labels("ServiceA", "GetContent", "success").inc();
    
    // Se houver itens, busca metadados do primeiro via Service B
    if (catalogResponse.items.length > 0) {
      const firstItem = catalogResponse.items[0];
      const metaCall = clientB.StreamMetadata({ 
        content_id: firstItem.id, 
        user_id: "guest" 
      });
      const metadata = [];
      
      metaCall.on("data", (item) => metadata.push({
        key: item.key,
        value: item.value,
        relevanceScore: item.relevance_score
      }));
      
      metaCall.on("error", (err) => {
        grpcRequestsTotal.labels("ServiceB", "StreamMetadata", "error").inc();
      });
      
      metaCall.on("end", () => {
        const duration = Number(process.hrtime.bigint() - start) / 1e9;
        grpcRequestsTotal.labels("ServiceB", "StreamMetadata", "success").inc();
        httpRequestDuration.labels("GET", "/api/browse", 200).observe(duration);
        
        res.json({
          catalog: catalogResponse.items,
          total: catalogResponse.total,
          featuredMetadata: metadata,
          processingTime: `${(duration * 1000).toFixed(2)}ms`
        });
      });
    } else {
      const duration = Number(process.hrtime.bigint() - start) / 1e9;
      res.json({
        catalog: [],
        total: 0,
        featuredMetadata: [],
        processingTime: `${(duration * 1000).toFixed(2)}ms`
      });
    }
  });
});

app.get("/healthz", (_, res) => res.send("ok"));

app.get("/metrics", async (req, res) => {
  res.set("Content-Type", client.register.contentType);
  res.end(await client.register.metrics());
});

app.listen(PORT, () => {
  console.log(`Gateway P listening on :${PORT}`);
  console.log(`Using A at ${A_ADDR} and B at ${B_ADDR}`);
});

```

gateway_p_node/proto/services.proto
```
// Contrato de comunicaÃ§Ã£o gRPC para aplicaÃ§Ã£o de streaming
// Service A: CatÃ¡logo de conteÃºdo (filmes, sÃ©ries, canais ao vivo)
// Service B: Metadados e recomendaÃ§Ãµes

syntax = "proto3";

package pspd;

// Service A: CatÃ¡logo de conteÃºdo
message ContentRequest {
  string type = 1; // "movies", "series", "live", "all"
  int32 limit = 2;
  string genre = 3;
}

message ContentItem {
  string id = 1;
  string title = 2;
  string description = 3;
  string thumbnail = 4;
  string type = 5;
  repeated string genres = 6;
  int32 year = 7;
  float rating = 8;
  string duration = 9;
}

message ContentResponse {
  repeated ContentItem items = 1;
  int32 total = 2;
}

service ServiceA {
  rpc GetContent(ContentRequest) returns (ContentResponse);
}

// Service B: Metadados e recomendaÃ§Ãµes (streaming)
message MetadataRequest {
  string content_id = 1;
  string user_id = 2;
}

message MetadataItem {
  string key = 1;
  string value = 2;
  float relevance_score = 3;
}

service ServiceB {
  rpc StreamMetadata(MetadataRequest) returns (stream MetadataItem);
}

```

gateway_p_node/public/index.html
```
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>PSPD Streaming Platform</title>
    <style>
      body {
        font-family: system-ui, -apple-system, sans-serif;
        margin: 0;
        padding: 2rem;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        color: #eee;
      }
      h1 {
        text-align: center;
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
      }
      .subtitle {
        text-align: center;
        color: #888;
        margin-bottom: 3rem;
      }
      .container {
        max-width: 1200px;
        margin: 0 auto;
      }
      .section {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        padding: 2rem;
        border-radius: 16px;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }
      h2 {
        margin-top: 0;
        color: #4a9eff;
      }
      .controls {
        display: flex;
        gap: 1rem;
        margin-bottom: 1rem;
        flex-wrap: wrap;
      }
      input, select, button {
        padding: 0.75rem 1rem;
        border-radius: 8px;
        border: 1px solid rgba(255, 255, 255, 0.2);
        background: rgba(255, 255, 255, 0.1);
        color: #eee;
        font-size: 1rem;
      }
      button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border: none;
        cursor: pointer;
        font-weight: 600;
        transition: transform 0.2s;
      }
      button:hover {
        transform: translateY(-2px);
      }
      button:active {
        transform: translateY(0);
      }
      pre {
        background: #000;
        padding: 1rem;
        border-radius: 8px;
        overflow-x: auto;
        max-height: 400px;
        overflow-y: auto;
      }
      .grid {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 1rem;
        margin-top: 1rem;
      }
      .card {
        background: rgba(255, 255, 255, 0.08);
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid rgba(255, 255, 255, 0.1);
      }
      .card h3 {
        margin: 0 0 0.5rem 0;
        color: #4a9eff;
        font-size: 1.1rem;
      }
      .card p {
        margin: 0.25rem 0;
        color: #aaa;
        font-size: 0.9rem;
      }
      .badge {
        display: inline-block;
        padding: 0.25rem 0.5rem;
        background: rgba(74, 158, 255, 0.2);
        border-radius: 4px;
        font-size: 0.75rem;
        margin-right: 0.25rem;
        margin-top: 0.25rem;
      }
      .metadata-list {
        list-style: none;
        padding: 0;
      }
      .metadata-list li {
        padding: 0.5rem;
        background: rgba(255, 255, 255, 0.05);
        margin-bottom: 0.5rem;
        border-radius: 4px;
      }
      .score {
        float: right;
        color: #4a9eff;
        font-weight: bold;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>ğŸ¬ PSPD Streaming Platform</h1>
      <p class="subtitle">API de Streaming com gRPC Â· Service A (CatÃ¡logo) Â· Service B (Metadados)</p>
      
      <!-- Service A: CatÃ¡logo -->
      <div class="section">
        <h2>ğŸ“º CatÃ¡logo de ConteÃºdo (Service A - Unary gRPC)</h2>
        <div class="controls">
          <select id="contentType">
            <option value="all">Todos</option>
            <option value="movies">Filmes</option>
            <option value="series">SÃ©ries</option>
            <option value="live">Ao Vivo</option>
          </select>
          <input id="contentLimit" type="number" value="10" min="1" max="50" placeholder="Limite">
          <input id="contentGenre" placeholder="GÃªnero (opcional)">
          <button onclick="loadCatalog()">ğŸ” Buscar CatÃ¡logo</button>
        </div>
        <div id="catalogGrid" class="grid"></div>
        <pre id="catalogRaw" style="display:none"></pre>
      </div>

      <!-- Service B: Metadados -->
      <div class="section">
        <h2>ğŸ¯ Metadados e RecomendaÃ§Ãµes (Service B - Streaming gRPC)</h2>
        <div class="controls">
          <select id="metadataContentId">
            <option value="m1">A Jornada Infinita (m1)</option>
            <option value="m2">Segredos do Passado (m2)</option>
            <option value="m3">Risadas na Cidade (m3)</option>
            <option value="m4">O Ãšltimo GuardiÃ£o (m4)</option>
            <option value="s1">DimensÃµes Paralelas (s1)</option>
            <option value="s2">Cidade Sombria (s2)</option>
          </select>
          <input id="metadataUserId" placeholder="User ID" value="user123">
          <button onclick="loadMetadata()">ğŸ“Š Buscar Metadados</button>
        </div>
        <ul id="metadataList" class="metadata-list"></ul>
        <pre id="metadataRaw" style="display:none"></pre>
      </div>

      <!-- API Combinada -->
      <div class="section">
        <h2>ğŸš€ Browse (API Combinada: A + B)</h2>
        <div class="controls">
          <select id="browseType">
            <option value="all">Todos</option>
            <option value="movies">Filmes</option>
            <option value="series">SÃ©ries</option>
          </select>
          <input id="browseLimit" type="number" value="5" min="1" max="20">
          <button onclick="loadBrowse()">âš¡ Carregar Browse</button>
        </div>
        <pre id="browseOutput"></pre>
      </div>
    </div>

    <script>
      async function loadCatalog() {
        const type = document.getElementById('contentType').value;
        const limit = document.getElementById('contentLimit').value;
        const genre = document.getElementById('contentGenre').value;
        
        const url = `/api/content?type=${type}&limit=${limit}${genre ? '&genre=' + genre : ''}`;
        const response = await fetch(url);
        const data = await response.json();
        
        // Renderizar grid de cards
        const grid = document.getElementById('catalogGrid');
        grid.innerHTML = data.items.map(item => `
          <div class="card">
            <h3>${item.title}</h3>
            <p><strong>${item.type === 'movie' ? 'ğŸ¬ Filme' : item.type === 'series' ? 'ğŸ“º SÃ©rie' : 'ğŸ“¡ Ao Vivo'}</strong></p>
            <p>${item.description.substring(0, 80)}...</p>
            <p>â­ ${item.rating} | ${item.year}</p>
            <div>
              ${item.genres.map(g => `<span class="badge">${g}</span>`).join('')}
            </div>
          </div>
        `).join('');
        
        // Mostrar JSON tambÃ©m
        document.getElementById('catalogRaw').textContent = JSON.stringify(data, null, 2);
      }

      async function loadMetadata() {
        const contentId = document.getElementById('metadataContentId').value;
        const userId = document.getElementById('metadataUserId').value;
        
        const url = `/api/metadata/${contentId}?userId=${userId}`;
        const response = await fetch(url);
        const data = await response.json();
        
        // Renderizar lista de metadados
        const list = document.getElementById('metadataList');
        list.innerHTML = data.metadata.map(item => `
          <li>
            <strong>${item.key}:</strong> ${item.value}
            <span class="score">${(item.relevance_score * 100).toFixed(0)}%</span>
          </li>
        `).join('');
        
        // Mostrar JSON tambÃ©m
        document.getElementById('metadataRaw').textContent = JSON.stringify(data, null, 2);
      }

      async function loadBrowse() {
        const type = document.getElementById('browseType').value;
        const limit = document.getElementById('browseLimit').value;
        
        const url = `/api/browse?type=${type}&limit=${limit}`;
        const response = await fetch(url);
        const data = await response.json();
        
        document.getElementById('browseOutput').textContent = JSON.stringify(data, null, 2);
      }

      // Carregar catÃ¡logo inicial
      window.addEventListener('load', () => loadCatalog());
    </script>
  </body>
</html>


```

test/run_all_scenarios.sh
```
#!/bin/bash
# Executar todos os testes de todos os cenÃ¡rios

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
TEST_DIR="$PROJECT_ROOT/test"

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  EXECUÃ‡ÃƒO COMPLETA DE TODOS OS CENÃRIOS                        â•‘"
echo "â•‘  5 CenÃ¡rios Ã— 4 Testes = 20 ExecuÃ§Ãµes                          â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Array com os cenÃ¡rios
SCENARIOS=(1 2 3 4 5)
START_TIME=$(date +%s)

# Contador de sucessos e falhas
TOTAL_SCENARIOS=${#SCENARIOS[@]}
SUCCESS_COUNT=0
FAILED_SCENARIOS=()

# Executar cada cenÃ¡rio
for scenario in "${SCENARIOS[@]}"; do
    SCENARIO_DIR="$TEST_DIR/scenario_${scenario}"
    
    echo ""
    echo "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®"
    echo "â”‚  CENÃRIO $scenario de $TOTAL_SCENARIOS"
    echo "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    echo ""
    
    if [ -f "$SCENARIO_DIR/run_all.sh" ]; then
        SCENARIO_START=$(date +%s)
        
        # Executar run_all.sh do cenÃ¡rio
        bash "$SCENARIO_DIR/run_all.sh"
        EXIT_CODE=$?
        
        SCENARIO_END=$(date +%s)
        SCENARIO_DURATION=$((SCENARIO_END - SCENARIO_START))
        
        if [ $EXIT_CODE -eq 0 ]; then
            echo ""
            echo "âœ… CenÃ¡rio $scenario concluÃ­do com sucesso em ${SCENARIO_DURATION}s"
            ((SUCCESS_COUNT++))
        else
            echo ""
            echo "âŒ CenÃ¡rio $scenario falhou (exit code: $EXIT_CODE)"
            FAILED_SCENARIOS+=($scenario)
        fi
    else
        echo "âš ï¸  Arquivo run_all.sh nÃ£o encontrado em $SCENARIO_DIR"
        FAILED_SCENARIOS+=($scenario)
    fi
    
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
done

# Calcular tempo total
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
MINUTES=$((TOTAL_DURATION / 60))
SECONDS=$((TOTAL_DURATION % 60))

# RelatÃ³rio final
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  RELATÃ“RIO FINAL                                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š EstatÃ­sticas:"
echo "   â€¢ CenÃ¡rios executados: $TOTAL_SCENARIOS"
echo "   â€¢ Sucessos: $SUCCESS_COUNT"
echo "   â€¢ Falhas: $((TOTAL_SCENARIOS - SUCCESS_COUNT))"
echo "   â€¢ Tempo total: ${MINUTES}m ${SECONDS}s"
echo ""

if [ ${#FAILED_SCENARIOS[@]} -eq 0 ]; then
    echo "âœ… TODOS OS CENÃRIOS CONCLUÃDOS COM SUCESSO!"
else
    echo "âŒ CenÃ¡rios que falharam: ${FAILED_SCENARIOS[*]}"
    echo ""
    echo "Para reexecutar um cenÃ¡rio especÃ­fico:"
    for failed in "${FAILED_SCENARIOS[@]}"; do
        echo "   bash test/scenario_${failed}/run_all.sh"
    done
fi

echo ""
echo "ğŸ“ Resultados salvos em:"
for scenario in "${SCENARIOS[@]}"; do
    echo "   â€¢ test_results/scenario_${scenario}/"
done

echo ""
echo "ğŸ“Š Para gerar grÃ¡ficos de um cenÃ¡rio especÃ­fico:"
echo "   ./scripts/generate_plots.sh <NÃšMERO_DO_CENÃRIO>"
echo ""

# Exit com cÃ³digo apropriado
if [ ${#FAILED_SCENARIOS[@]} -eq 0 ]; then
    exit 0
else
    exit 1
fi

```

test/scenario_4/00_setup.sh
```
#!/bin/bash
# Script auxiliar: Setup do Scenario 4 (Limited Resources)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

echo "ğŸ”§ Setup Scenario 4: Limited Resources"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Verificar cluster Kubernetes
source "$SCRIPT_DIR/../common/k8s_check.sh"
if ! check_kubernetes_cluster; then
    echo "âŒ Falha na verificaÃ§Ã£o do cluster Kubernetes"
    exit 1
fi
echo ""

# Limpar
kubectl delete namespace pspd 2>/dev/null || true
sleep 5

# Deploy
kubectl apply -f "$PROJECT_ROOT/k8s/namespace.yaml"
kubectl apply -f "$PROJECT_ROOT/k8s/scenarios/scenario4-resources/"
sleep 10

# Aguardar
kubectl wait --for=condition=ready pod -l app=a -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=b -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=p -n pspd --timeout=60s

echo "âœ… Pods prontos:"
kubectl get pods -n pspd

# Port-forward
pkill -f "port-forward.*pspd.*p-svc" 2>/dev/null || true
sleep 1
kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
sleep 5

# Testar com retry (atÃ© 10 tentativas)
echo "ğŸ§ª Testando conectividade..."
for i in {1..10}; do
    if curl -s --max-time 2 http://localhost:8080/api/content?type=all > /dev/null 2>&1; then
        echo "âœ… Gateway OK (http://localhost:8080)"
        exit 0
    fi
    echo "   Tentativa $i/10 falhou, aguardando..."
    sleep 2
done

echo "âŒ Falha apÃ³s 10 tentativas"
echo "   Verifique se os pods estÃ£o rodando: kubectl get pods -n pspd"
echo "   Verifique logs: kubectl logs -n pspd -l app=p"
exit 1

```

test/scenario_4/baseline.sh
```
#!/bin/bash
# Teste: Baseline (Scenario 4)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

mkdir -p "$RESULTS_DIR/baseline"

echo "ğŸ“Š Executando: Baseline Test (Scenario 4)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/baseline/metrics.json" \
  "$PROJECT_ROOT/load/baseline.js" | tee "$RESULTS_DIR/baseline/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/baseline/"

```

test/scenario_4/spike.sh
```
#!/bin/bash
# Teste: Spike (Scenario 4)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

mkdir -p "$RESULTS_DIR/spike"

echo "ğŸ“Š Executando: Spike Test (Scenario 4)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/spike/metrics.json" \
  "$PROJECT_ROOT/load/spike.js" | tee "$RESULTS_DIR/spike/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/spike/"

```

test/scenario_4/soak.sh
```
#!/bin/bash
# Teste: Soak (Scenario 4)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

mkdir -p "$RESULTS_DIR/soak"

echo "ğŸ“Š Executando: Soak Test (Scenario 4)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/soak/metrics.json" \
  "$PROJECT_ROOT/load/soak.js" | tee "$RESULTS_DIR/soak/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/soak/"

```

test/scenario_4/run_all.sh
```
#!/bin/bash
# Executar todos os testes do Scenario 1

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

echo "ğŸš€ SCENARIO 4: Limited Resources (1 replica + 50% CPU/Mem + HPA 1-15)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Array com os testes a executar
TESTS=("baseline" "ramp" "spike" "soak")

# Executar cada teste com setup antes
for test in "${TESTS[@]}"; do
    echo ""
    echo "ğŸ“‹ Executando setup para teste: $test"
    bash "$SCRIPT_DIR/00_setup.sh" || { echo "âŒ Setup falhou para $test"; exit 1; }
    
    echo ""
    echo "ğŸ§ª Executando teste: $test"
    bash "$SCRIPT_DIR/${test}.sh" || { echo "âš ï¸  Teste $test falhou"; }
done
\necho ""
echo "ğŸ“Š Gerando grÃ¡ficos de anÃ¡lise..."
python3 "$PROJECT_ROOT/scripts/analyze_results.py" "$RESULTS_DIR"

echo ""
echo "âœ… TODOS OS TESTES CONCLUÃDOS!"
echo "ğŸ“ Resultados em: $RESULTS_DIR"
ls -lh "$RESULTS_DIR"

```

test/scenario_4/ramp.sh
```
#!/bin/bash
# Teste: Ramp (Scenario 4)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_4"

mkdir -p "$RESULTS_DIR/ramp"

echo "ğŸ“Š Executando: Ramp Test (Scenario 4)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/ramp/metrics.json" \
  "$PROJECT_ROOT/load/ramp.js" | tee "$RESULTS_DIR/ramp/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/ramp/"

```

test/scenario_3/00_setup.sh
```
#!/bin/bash
# Script auxiliar: Setup do Scenario 3 (Distribution)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

echo "ğŸ”§ Setup Scenario 3: Distribution"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Verificar cluster Kubernetes
source "$SCRIPT_DIR/../common/k8s_check.sh"
if ! check_kubernetes_cluster; then
    echo "âŒ Falha na verificaÃ§Ã£o do cluster Kubernetes"
    exit 1
fi
echo ""

# Limpar
kubectl delete namespace pspd 2>/dev/null || true
sleep 5

# Deploy
kubectl apply -f "$PROJECT_ROOT/k8s/namespace.yaml"
kubectl apply -f "$PROJECT_ROOT/k8s/scenarios/scenario3-distribution/"
sleep 10

# Aguardar
kubectl wait --for=condition=ready pod -l app=a -n pspd --timeout=120s
kubectl wait --for=condition=ready pod -l app=b -n pspd --timeout=120s
kubectl wait --for=condition=ready pod -l app=p -n pspd --timeout=120s

echo "âœ… Pods prontos:"
kubectl get pods -n pspd

# Port-forward
pkill -f "port-forward.*pspd.*p-svc" 2>/dev/null || true
sleep 1
kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
sleep 5

# Testar com retry (atÃ© 10 tentativas)
echo "ğŸ§ª Testando conectividade..."
for i in {1..10}; do
    if curl -s --max-time 2 http://localhost:8080/api/content?type=all > /dev/null 2>&1; then
        echo "âœ… Gateway OK (http://localhost:8080)"
        exit 0
    fi
    echo "   Tentativa $i/10 falhou, aguardando..."
    sleep 2
done

echo "âŒ Falha apÃ³s 10 tentativas"
echo "   Verifique se os pods estÃ£o rodando: kubectl get pods -n pspd"
echo "   Verifique logs: kubectl logs -n pspd -l app=p"
exit 1

```

test/scenario_3/baseline.sh
```
#!/bin/bash
# Teste: Baseline (Scenario 3)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

mkdir -p "$RESULTS_DIR/baseline"

echo "ğŸ“Š Executando: Baseline Test (Scenario 3)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/baseline/metrics.json" \
  "$PROJECT_ROOT/load/baseline.js" | tee "$RESULTS_DIR/baseline/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/baseline/"

```

test/scenario_3/spike.sh
```
#!/bin/bash
# Teste: Spike (Scenario 3)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

mkdir -p "$RESULTS_DIR/spike"

echo "ğŸ“Š Executando: Spike Test (Scenario 3)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/spike/metrics.json" \
  "$PROJECT_ROOT/load/spike.js" | tee "$RESULTS_DIR/spike/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/spike/"

```

test/scenario_3/soak.sh
```
#!/bin/bash
# Teste: Soak (Scenario 3)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

mkdir -p "$RESULTS_DIR/soak"

echo "ğŸ“Š Executando: Soak Test (Scenario 3)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/soak/metrics.json" \
  "$PROJECT_ROOT/load/soak.js" | tee "$RESULTS_DIR/soak/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/soak/"

```

test/scenario_3/run_all.sh
```
#!/bin/bash
# Executar todos os testes do Scenario 1

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

echo "ğŸš€ SCENARIO 3: Distribution (3 replicas + anti-affinity + HPA 3-12)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Array com os testes a executar
TESTS=("baseline" "ramp" "spike" "soak")

# Executar cada teste com setup antes
for test in "${TESTS[@]}"; do
    echo ""
    echo "ğŸ“‹ Executando setup para teste: $test"
    bash "$SCRIPT_DIR/00_setup.sh" || { echo "âŒ Setup falhou para $test"; exit 1; }
    
    echo ""
    echo "ğŸ§ª Executando teste: $test"
    bash "$SCRIPT_DIR/${test}.sh" || { echo "âš ï¸  Teste $test falhou"; }
done
\necho ""
echo "ğŸ“Š Gerando grÃ¡ficos de anÃ¡lise..."
python3 "$PROJECT_ROOT/scripts/analyze_results.py" "$RESULTS_DIR"

echo ""
echo "âœ… TODOS OS TESTES CONCLUÃDOS!"
echo "ğŸ“ Resultados em: $RESULTS_DIR"
ls -lh "$RESULTS_DIR"

```

test/scenario_3/ramp.sh
```
#!/bin/bash
# Teste: Ramp (Scenario 3)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_3"

mkdir -p "$RESULTS_DIR/ramp"

echo "ğŸ“Š Executando: Ramp Test (Scenario 3)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/ramp/metrics.json" \
  "$PROJECT_ROOT/load/ramp.js" | tee "$RESULTS_DIR/ramp/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/ramp/"

```

test/scenario_1/00_setup.sh
```
#!/bin/bash
# Script auxiliar: Setup do Scenario 1 (Baseline)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

echo "ğŸ”§ Setup Scenario 1: Baseline"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Verificar cluster Kubernetes
source "$SCRIPT_DIR/../common/k8s_check.sh"
if ! check_kubernetes_cluster; then
    echo "âŒ Falha na verificaÃ§Ã£o do cluster Kubernetes"
    exit 1
fi
echo ""

# Limpar
kubectl delete namespace pspd 2>/dev/null || true
sleep 5

# Deploy
kubectl apply -f "$PROJECT_ROOT/k8s/namespace.yaml"
kubectl apply -f "$PROJECT_ROOT/k8s/scenarios/scenario1-base/"
sleep 10

# Aguardar
kubectl wait --for=condition=ready pod -l app=a -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=b -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=p -n pspd --timeout=60s

echo "âœ… Pods prontos:"
kubectl get pods -n pspd

# Port-forward
pkill -f "port-forward.*pspd.*p-svc" 2>/dev/null || true
sleep 1
kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
sleep 5

# Testar com retry (atÃ© 10 tentativas)
echo "ğŸ§ª Testando conectividade..."
for i in {1..10}; do
    if curl -s --max-time 2 http://localhost:8080/api/content?type=all > /dev/null 2>&1; then
        echo "âœ… Gateway OK (http://localhost:8080)"
        exit 0
    fi
    echo "   Tentativa $i/10 falhou, aguardando..."
    sleep 2
done

echo "âŒ Falha apÃ³s 10 tentativas"
echo "   Verifique se os pods estÃ£o rodando: kubectl get pods -n pspd"
echo "   Verifique logs: kubectl logs -n pspd -l app=p"
exit 1

```

test/scenario_1/baseline.sh
```
#!/bin/bash
# Teste: Baseline (Scenario 1)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

mkdir -p "$RESULTS_DIR/baseline"

echo "ğŸ“Š Executando: Baseline Test (Scenario 1)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/baseline/metrics.json" \
  "$PROJECT_ROOT/load/baseline.js" | tee "$RESULTS_DIR/baseline/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/baseline/"

```

test/scenario_1/spike.sh
```
#!/bin/bash
# Teste: Spike (Scenario 1)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

mkdir -p "$RESULTS_DIR/spike"

echo "ğŸ“Š Executando: Spike Test (Scenario 1)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/spike/metrics.json" \
  "$PROJECT_ROOT/load/spike.js" | tee "$RESULTS_DIR/spike/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/spike/"

```

test/scenario_1/soak.sh
```
#!/bin/bash
# Teste: Soak (Scenario 1)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

mkdir -p "$RESULTS_DIR/soak"

echo "ğŸ“Š Executando: Soak Test (Scenario 1)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/soak/metrics.json" \
  "$PROJECT_ROOT/load/soak.js" | tee "$RESULTS_DIR/soak/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/soak/"

```

test/scenario_1/run_all.sh
```
#!/bin/bash
# Executar todos os testes do Scenario 1

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

echo "ğŸš€ SCENARIO 1: Baseline (1 replica + HPA 1-10)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Array com os testes a executar
TESTS=("baseline" "ramp" "spike" "soak")

# Executar cada teste com setup antes
for test in "${TESTS[@]}"; do
    echo ""
    echo "ğŸ“‹ Executando setup para teste: $test"
    bash "$SCRIPT_DIR/00_setup.sh" || { echo "âŒ Setup falhou para $test"; exit 1; }
    
    echo ""
    echo "ğŸ§ª Executando teste: $test"
    bash "$SCRIPT_DIR/${test}.sh" || { echo "âš ï¸  Teste $test falhou"; }
done

echo ""
echo "ğŸ“Š Gerando grÃ¡ficos de anÃ¡lise..."
python3 "$PROJECT_ROOT/scripts/analyze_results.py" "$RESULTS_DIR"

echo ""
echo "âœ… TODOS OS TESTES CONCLUÃDOS!"
echo "ğŸ“ Resultados em: $RESULTS_DIR"
ls -lh "$RESULTS_DIR"

```

test/scenario_1/ramp.sh
```
#!/bin/bash
# Teste: Ramp (Scenario 1)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_1"

mkdir -p "$RESULTS_DIR/ramp"

echo "ğŸ“Š Executando: Ramp Test (Scenario 1)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/ramp/metrics.json" \
  "$PROJECT_ROOT/load/ramp.js" | tee "$RESULTS_DIR/ramp/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/ramp/"

```

test/scenario_5/00_setup.sh
```
#!/bin/bash
# Script auxiliar: Setup do Scenario 5 (No HPA)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

echo "ğŸ”§ Setup Scenario 5: No HPA"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Verificar cluster Kubernetes
source "$SCRIPT_DIR/../common/k8s_check.sh"
if ! check_kubernetes_cluster; then
    echo "âŒ Falha na verificaÃ§Ã£o do cluster Kubernetes"
    exit 1
fi
echo ""

# Limpar
kubectl delete namespace pspd 2>/dev/null || true
sleep 5

# Deploy
kubectl apply -f "$PROJECT_ROOT/k8s/namespace.yaml"
kubectl apply -f "$PROJECT_ROOT/k8s/scenarios/scenario5-no-hpa/"
sleep 10

# Aguardar
kubectl wait --for=condition=ready pod -l app=a -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=b -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=p -n pspd --timeout=60s

echo "âœ… Pods prontos:"
kubectl get pods -n pspd

# Port-forward
pkill -f "port-forward.*pspd.*p-svc" 2>/dev/null || true
sleep 1
kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
sleep 5

# Testar com retry (atÃ© 10 tentativas)
echo "ğŸ§ª Testando conectividade..."
for i in {1..10}; do
    if curl -s --max-time 2 http://localhost:8080/api/content?type=all > /dev/null 2>&1; then
        echo "âœ… Gateway OK (http://localhost:8080)"
        exit 0
    fi
    echo "   Tentativa $i/10 falhou, aguardando..."
    sleep 2
done

echo "âŒ Falha apÃ³s 10 tentativas"
echo "   Verifique se os pods estÃ£o rodando: kubectl get pods -n pspd"
echo "   Verifique logs: kubectl logs -n pspd -l app=p"
exit 1

```

test/scenario_5/baseline.sh
```
#!/bin/bash
# Teste: Baseline (Scenario 5)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

mkdir -p "$RESULTS_DIR/baseline"

echo "ğŸ“Š Executando: Baseline Test (Scenario 5)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/baseline/metrics.json" \
  "$PROJECT_ROOT/load/baseline.js" | tee "$RESULTS_DIR/baseline/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/baseline/"

```

test/scenario_5/spike.sh
```
#!/bin/bash
# Teste: Spike (Scenario 5)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

mkdir -p "$RESULTS_DIR/spike"

echo "ğŸ“Š Executando: Spike Test (Scenario 5)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/spike/metrics.json" \
  "$PROJECT_ROOT/load/spike.js" | tee "$RESULTS_DIR/spike/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/spike/"

```

test/scenario_5/soak.sh
```
#!/bin/bash
# Teste: Soak (Scenario 5)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

mkdir -p "$RESULTS_DIR/soak"

echo "ğŸ“Š Executando: Soak Test (Scenario 5)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/soak/metrics.json" \
  "$PROJECT_ROOT/load/soak.js" | tee "$RESULTS_DIR/soak/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/soak/"

```

test/scenario_5/run_all.sh
```
#!/bin/bash
# Executar todos os testes do Scenario 1

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

echo "ğŸš€ SCENARIO 5: No HPA (5 fixed replicas, no autoscaling)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Array com os testes a executar
TESTS=("baseline" "ramp" "spike" "soak")

# Executar cada teste com setup antes
for test in "${TESTS[@]}"; do
    echo ""
    echo "ğŸ“‹ Executando setup para teste: $test"
    bash "$SCRIPT_DIR/00_setup.sh" || { echo "âŒ Setup falhou para $test"; exit 1; }
    
    echo ""
    echo "ğŸ§ª Executando teste: $test"
    bash "$SCRIPT_DIR/${test}.sh" || { echo "âš ï¸  Teste $test falhou"; }
done
\necho ""
echo "ğŸ“Š Gerando grÃ¡ficos de anÃ¡lise..."
python3 "$PROJECT_ROOT/scripts/analyze_results.py" "$RESULTS_DIR"

echo ""
echo "âœ… TODOS OS TESTES CONCLUÃDOS!"
echo "ğŸ“ Resultados em: $RESULTS_DIR"
ls -lh "$RESULTS_DIR"

```

test/scenario_5/ramp.sh
```
#!/bin/bash
# Teste: Ramp (Scenario 5)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_5"

mkdir -p "$RESULTS_DIR/ramp"

echo "ğŸ“Š Executando: Ramp Test (Scenario 5)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/ramp/metrics.json" \
  "$PROJECT_ROOT/load/ramp.js" | tee "$RESULTS_DIR/ramp/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/ramp/"

```

test/common/k8s_check.sh
```
#!/bin/bash
# Script auxiliar: FunÃ§Ãµes compartilhadas para verificaÃ§Ã£o de cluster

check_kubernetes_cluster() {
    echo "ğŸ” Verificando cluster Kubernetes..."
    
    # Verificar se kubectl estÃ¡ instalado
    if ! command -v kubectl &> /dev/null; then
        echo "âŒ kubectl nÃ£o encontrado!"
        echo "   Instale o kubectl: https://kubernetes.io/docs/tasks/tools/"
        return 1
    fi
    
    # Verificar se minikube estÃ¡ instalado
    if ! command -v minikube &> /dev/null; then
        echo "âŒ minikube nÃ£o encontrado!"
        echo "   Instale o minikube: https://minikube.sigs.k8s.io/docs/start/"
        return 1
    fi
    
    # Verificar status do minikube
    local minikube_status=$(minikube status --format='{{.Host}}' 2>/dev/null)
    
    if [ "$minikube_status" != "Running" ]; then
        echo "âš ï¸  Minikube nÃ£o estÃ¡ rodando"
        echo "ğŸš€ Iniciando Minikube..."
        
        if minikube start; then
            echo "âœ… Minikube iniciado com sucesso"
        else
            echo "âŒ Falha ao iniciar Minikube"
            return 1
        fi
    fi
    
    # Verificar se o API server estÃ¡ respondendo
    if ! kubectl cluster-info &>/dev/null; then
        echo "âš ï¸  Cluster nÃ£o estÃ¡ respondendo, atualizando contexto..."
        
        if minikube update-context; then
            echo "âœ… Contexto atualizado"
        else
            echo "âŒ Falha ao atualizar contexto"
            return 1
        fi
        
        # Tentar novamente apÃ³s atualizar contexto
        if ! kubectl cluster-info &>/dev/null; then
            echo "âŒ Cluster ainda nÃ£o estÃ¡ acessÃ­vel"
            echo "   Tentando reiniciar..."
            
            if minikube start; then
                echo "âœ… Cluster reiniciado com sucesso"
            else
                echo "âŒ Falha ao acessar cluster Kubernetes"
                return 1
            fi
        fi
    fi
    
    # Verificar addons necessÃ¡rios
    echo "ğŸ”Œ Verificando addons do Minikube..."
    
    # Verificar ingress
    if ! minikube addons list | grep -q "ingress.*enabled"; then
        echo "âš ï¸  Addon ingress nÃ£o estÃ¡ habilitado"
        echo "ğŸ”§ Habilitando ingress..."
        minikube addons enable ingress
    fi
    
    # Verificar metrics-server (opcional, mas Ãºtil)
    if ! minikube addons list | grep -q "metrics-server.*enabled"; then
        echo "ğŸ’¡ Dica: Habilite metrics-server para mÃ©tricas de recursos"
        echo "   Execute: minikube addons enable metrics-server"
    fi
    
    echo "âœ… Cluster Kubernetes estÃ¡ pronto!"
    kubectl cluster-info | grep "Kubernetes control plane"
    
    return 0
}

# Verificar se estÃ¡ sendo executado diretamente ou sendo sourced
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    # Executado diretamente - rodar verificaÃ§Ã£o
    check_kubernetes_cluster
else
    # Sendo sourced - apenas definir a funÃ§Ã£o
    :
fi

```

test/scenario_2/00_setup.sh
```
#!/bin/bash
# Script auxiliar: Setup do Scenario 2 (Warm Start)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

echo "ğŸ”§ Setup Scenario 2: Warm Start"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Verificar cluster Kubernetes
source "$SCRIPT_DIR/../common/k8s_check.sh"
if ! check_kubernetes_cluster; then
    echo "âŒ Falha na verificaÃ§Ã£o do cluster Kubernetes"
    exit 1
fi
echo ""

# Limpar
kubectl delete namespace pspd 2>/dev/null || true
sleep 5

# Deploy
kubectl apply -f "$PROJECT_ROOT/k8s/namespace.yaml"
kubectl apply -f "$PROJECT_ROOT/k8s/scenarios/scenario2-replicas/"
sleep 10

# Aguardar
kubectl wait --for=condition=ready pod -l app=a -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=b -n pspd --timeout=60s
kubectl wait --for=condition=ready pod -l app=p -n pspd --timeout=60s

echo "âœ… Pods prontos:"
kubectl get pods -n pspd

# Port-forward
pkill -f "port-forward.*pspd.*p-svc" 2>/dev/null || true
sleep 1
kubectl port-forward -n pspd svc/p-svc 8080:80 > /dev/null 2>&1 &
sleep 5

# Testar com retry (atÃ© 10 tentativas)
echo "ğŸ§ª Testando conectividade..."
for i in {1..10}; do
    if curl -s --max-time 2 http://localhost:8080/api/content?type=all > /dev/null 2>&1; then
        echo "âœ… Gateway OK (http://localhost:8080)"
        exit 0
    fi
    echo "   Tentativa $i/10 falhou, aguardando..."
    sleep 2
done

echo "âŒ Falha apÃ³s 10 tentativas"
echo "   Verifique se os pods estÃ£o rodando: kubectl get pods -n pspd"
echo "   Verifique logs: kubectl logs -n pspd -l app=p"
exit 1

```

test/scenario_2/baseline.sh
```
#!/bin/bash
# Teste: Baseline (Scenario 2)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

mkdir -p "$RESULTS_DIR/baseline"

echo "ğŸ“Š Executando: Baseline Test (Scenario 2)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-pre.txt" 2>&1 || true

k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/baseline/metrics.json" \
  "$PROJECT_ROOT/load/baseline.js" | tee "$RESULTS_DIR/baseline/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/baseline/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/baseline/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/baseline/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/baseline/"

```

test/scenario_2/spike.sh
```
#!/bin/bash
# Teste: Spike (Scenario 2)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

mkdir -p "$RESULTS_DIR/spike"

echo "ğŸ“Š Executando: Spike Test (Scenario 2)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/spike/metrics.json" \
  "$PROJECT_ROOT/load/spike.js" | tee "$RESULTS_DIR/spike/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/spike/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/spike/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/spike/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/spike/"

```

test/scenario_2/soak.sh
```
#!/bin/bash
# Teste: Soak (Scenario 2)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

mkdir -p "$RESULTS_DIR/soak"

echo "ğŸ“Š Executando: Soak Test (Scenario 2)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/soak/metrics.json" \
  "$PROJECT_ROOT/load/soak.js" | tee "$RESULTS_DIR/soak/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/soak/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/soak/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/soak/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/soak/"

```

test/scenario_2/run_all.sh
```
#!/bin/bash
# Executar todos os testes do Scenario 1

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

echo "ğŸš€ SCENARIO 2: Warm Start (2 replicas + HPA 2-10)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Array com os testes a executar
TESTS=("baseline" "ramp" "spike" "soak")

# Executar cada teste com setup antes
for test in "${TESTS[@]}"; do
    echo ""
    echo "ğŸ“‹ Executando setup para teste: $test"
    bash "$SCRIPT_DIR/00_setup.sh" || { echo "âŒ Setup falhou para $test"; exit 1; }
    
    echo ""
    echo "ğŸ§ª Executando teste: $test"
    bash "$SCRIPT_DIR/${test}.sh" || { echo "âš ï¸  Teste $test falhou"; }
done
\necho ""
echo "ğŸ“Š Gerando grÃ¡ficos de anÃ¡lise..."
python3 "$PROJECT_ROOT/scripts/analyze_results.py" "$RESULTS_DIR"

echo ""
echo "âœ… TODOS OS TESTES CONCLUÃDOS!"
echo "ğŸ“ Resultados em: $RESULTS_DIR"
ls -lh "$RESULTS_DIR"

```

test/scenario_2/ramp.sh
```
#!/bin/bash
# Teste: Ramp (Scenario 2)

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test_results/scenario_2"

mkdir -p "$RESULTS_DIR/ramp"

echo "ğŸ“Š Executando: Ramp Test (Scenario 2)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# MÃ©tricas PRE
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-pre.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-pre.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-pre.txt" 2>&1 || true

# Executar teste
k6 run --log-output=none \
  --summary-trend-stats="min,avg,med,max,p(90),p(95),p(99)" \
  --out json="$RESULTS_DIR/ramp/metrics.json" \
  "$PROJECT_ROOT/load/ramp.js" | tee "$RESULTS_DIR/ramp/output.txt"

# MÃ©tricas POST
kubectl top pods -n pspd > "$RESULTS_DIR/ramp/pod-metrics-post.txt" 2>&1 || true
kubectl get hpa -n pspd > "$RESULTS_DIR/ramp/hpa-status-post.txt" 2>&1 || true
kubectl get pods -n pspd -o wide > "$RESULTS_DIR/ramp/pods-status-post.txt" 2>&1 || true

echo ""
echo "âœ… Resultados salvos em: $RESULTS_DIR/ramp/"

```

test_results/scenario_4/plots/SUMMARY_REPORT.txt
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  BASELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 1901.80 req/s
  â€¢ Total de requisiÃ§Ãµes: 189,381
  â€¢ LatÃªncia mÃ©dia: 3.94 ms
  â€¢ LatÃªncia p95: 8.33 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 10
  â€¢ IteraÃ§Ãµes: 189,381

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 26Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 27Mi
  â€¢ gateway-p:
      CPU: 11m
      Memory: 28Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  RAMP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 201.89 req/s
  â€¢ Total de requisiÃ§Ãµes: 48,304
  â€¢ LatÃªncia mÃ©dia: 1.45 ms
  â€¢ LatÃªncia p95: 2.12 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 149
  â€¢ IteraÃ§Ãµes: 32,138

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 26Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 26Mi
  â€¢ gateway-p:
      CPU: 7m
      Memory: 36Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SOAK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 93.54 req/s
  â€¢ Total de requisiÃ§Ãµes: 64,322
  â€¢ LatÃªncia mÃ©dia: 1.52 ms
  â€¢ LatÃªncia p95: 2.25 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 50
  â€¢ IteraÃ§Ãµes: 32,161

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 26Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 26Mi
  â€¢ gateway-p:
      CPU: 6m
      Memory: 34Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SPIKE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 2762.30 req/s
  â€¢ Total de requisiÃ§Ãµes: 192,618
  â€¢ LatÃªncia mÃ©dia: 84.78 ms
  â€¢ LatÃªncia p95: 270.22 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 200
  â€¢ IteraÃ§Ãµes: 64,206

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 16m
      Memory: 27Mi
  â€¢ service-b:
      CPU: 15m
      Memory: 26Mi
  â€¢ gateway-p:
      CPU: 21m
      Memory: 23Mi

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FIM DO RELATÃ“RIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

test_results/scenario_3/plots/SUMMARY_REPORT.txt
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  BASELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 2192.55 req/s
  â€¢ Total de requisiÃ§Ãµes: 218,257
  â€¢ LatÃªncia mÃ©dia: 3.42 ms
  â€¢ LatÃªncia p95: 7.59 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 10
  â€¢ IteraÃ§Ãµes: 218,257

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 3 rÃ©plicas
  â€¢ b-hpa: 3 rÃ©plicas
  â€¢ p-hpa: 3 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 26Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 25Mi
  â€¢ gateway-p:
      CPU: 12m
      Memory: 20Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  RAMP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 200.85 req/s
  â€¢ Total de requisiÃ§Ãµes: 48,003
  â€¢ LatÃªncia mÃ©dia: 2.00 ms
  â€¢ LatÃªncia p95: 3.52 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 149
  â€¢ IteraÃ§Ãµes: 32,071

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 3 rÃ©plicas
  â€¢ b-hpa: 3 rÃ©plicas
  â€¢ p-hpa: 3 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 25Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 25Mi
  â€¢ gateway-p:
      CPU: 7m
      Memory: 23Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SOAK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 93.49 req/s
  â€¢ Total de requisiÃ§Ãµes: 64,252
  â€¢ LatÃªncia mÃ©dia: 2.06 ms
  â€¢ LatÃªncia p95: 3.44 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 50
  â€¢ IteraÃ§Ãµes: 32,126

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 3 rÃ©plicas
  â€¢ b-hpa: 3 rÃ©plicas
  â€¢ p-hpa: 3 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 25Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 25Mi
  â€¢ gateway-p:
      CPU: 6m
      Memory: 24Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SPIKE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 487.99 req/s
  â€¢ Total de requisiÃ§Ãµes: 48,308
  â€¢ LatÃªncia mÃ©dia: 391.15 ms
  â€¢ LatÃªncia p95: 1123.35 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 200
  â€¢ IteraÃ§Ãµes: 16,055

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 3 rÃ©plicas
  â€¢ b-hpa: 3 rÃ©plicas
  â€¢ p-hpa: 3 rÃ©plicas


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FIM DO RELATÃ“RIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

test_results/scenario_1/plots/SUMMARY_REPORT.txt
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  BASELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 1286.60 req/s
  â€¢ Total de requisiÃ§Ãµes: 128,225
  â€¢ LatÃªncia mÃ©dia: 5.87 ms
  â€¢ LatÃªncia p95: 15.11 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 10
  â€¢ IteraÃ§Ãµes: 128,225

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 3m
      Memory: 33Mi
  â€¢ service-b:
      CPU: 3m
      Memory: 37Mi
  â€¢ gateway-p:
      CPU: 17m
      Memory: 45Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  RAMP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 201.93 req/s
  â€¢ Total de requisiÃ§Ãµes: 48,276
  â€¢ LatÃªncia mÃ©dia: 1.70 ms
  â€¢ LatÃªncia p95: 2.44 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 149
  â€¢ IteraÃ§Ãµes: 32,110

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 37Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 30Mi
  â€¢ gateway-p:
      CPU: 7m
      Memory: 46Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SOAK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 93.35 req/s
  â€¢ Total de requisiÃ§Ãµes: 64,244
  â€¢ LatÃªncia mÃ©dia: 2.07 ms
  â€¢ LatÃªncia p95: 3.52 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 50
  â€¢ IteraÃ§Ãµes: 32,122

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 31Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 29Mi
  â€¢ gateway-p:
      CPU: 8m
      Memory: 43Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SPIKE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 2782.48 req/s
  â€¢ Total de requisiÃ§Ãµes: 194,082
  â€¢ LatÃªncia mÃ©dia: 82.62 ms
  â€¢ LatÃªncia p95: 270.61 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 200
  â€¢ IteraÃ§Ãµes: 64,694

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 1 rÃ©plicas
  â€¢ b-hpa: 1 rÃ©plicas
  â€¢ p-hpa: 1 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 24m
      Memory: 33Mi
  â€¢ service-b:
      CPU: 24m
      Memory: 34Mi
  â€¢ gateway-p:
      CPU: 28m
      Memory: 32Mi

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FIM DO RELATÃ“RIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

test_results/scenario_5/plots/SUMMARY_REPORT.txt
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  BASELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 998.37 req/s
  â€¢ Total de requisiÃ§Ãµes: 99,266
  â€¢ LatÃªncia mÃ©dia: 7.53 ms
  â€¢ LatÃªncia p95: 18.74 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 10
  â€¢ IteraÃ§Ãµes: 99,266

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 4m
      Memory: 24Mi
  â€¢ service-b:
      CPU: 4m
      Memory: 24Mi
  â€¢ gateway-p:
      CPU: 17m
      Memory: 19Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  RAMP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 195.61 req/s
  â€¢ Total de requisiÃ§Ãµes: 46,816
  â€¢ LatÃªncia mÃ©dia: 9.03 ms
  â€¢ LatÃªncia p95: 36.84 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 149
  â€¢ IteraÃ§Ãµes: 31,320

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 4m
      Memory: 25Mi
  â€¢ service-b:
      CPU: 5m
      Memory: 25Mi
  â€¢ gateway-p:
      CPU: 13m
      Memory: 21Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SOAK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 93.54 req/s
  â€¢ Total de requisiÃ§Ãµes: 64,328
  â€¢ LatÃªncia mÃ©dia: 1.49 ms
  â€¢ LatÃªncia p95: 2.17 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 50
  â€¢ IteraÃ§Ãµes: 32,164

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 1m
      Memory: 26Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 25Mi
  â€¢ gateway-p:
      CPU: 5m
      Memory: 24Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SPIKE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 146.13 req/s
  â€¢ Total de requisiÃ§Ãµes: 12,147
  â€¢ LatÃªncia mÃ©dia: 2048.17 ms
  â€¢ LatÃªncia p95: 215.56 ms
  â€¢ Taxa de sucesso: 96.63%
  â€¢ Taxa de falha: 3.37%
  â€¢ VUs mÃ¡ximos: 200
  â€¢ IteraÃ§Ãµes: 4,048


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FIM DO RELATÃ“RIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

test_results/scenario_2/plots/SUMMARY_REPORT.txt
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RELATÃ“RIO DE ANÃLISE - TESTES DE OBSERVABILIDADE K8S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  BASELINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 1898.48 req/s
  â€¢ Total de requisiÃ§Ãµes: 188,805
  â€¢ LatÃªncia mÃ©dia: 3.96 ms
  â€¢ LatÃªncia p95: 8.95 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 10
  â€¢ IteraÃ§Ãµes: 188,805

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 2 rÃ©plicas
  â€¢ b-hpa: 2 rÃ©plicas
  â€¢ p-hpa: 2 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 28Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 26Mi
  â€¢ gateway-p:
      CPU: 11m
      Memory: 26Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  RAMP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 200.53 req/s
  â€¢ Total de requisiÃ§Ãµes: 47,949
  â€¢ LatÃªncia mÃ©dia: 3.14 ms
  â€¢ LatÃªncia p95: 3.55 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 149
  â€¢ IteraÃ§Ãµes: 31,975

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 2 rÃ©plicas
  â€¢ b-hpa: 2 rÃ©plicas
  â€¢ p-hpa: 2 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 28Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 28Mi
  â€¢ gateway-p:
      CPU: 7m
      Memory: 33Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SOAK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 93.55 req/s
  â€¢ Total de requisiÃ§Ãµes: 64,278
  â€¢ LatÃªncia mÃ©dia: 1.83 ms
  â€¢ LatÃªncia p95: 2.99 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 50
  â€¢ IteraÃ§Ãµes: 32,139

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 2 rÃ©plicas
  â€¢ b-hpa: 2 rÃ©plicas
  â€¢ p-hpa: 2 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 2m
      Memory: 25Mi
  â€¢ service-b:
      CPU: 2m
      Memory: 26Mi
  â€¢ gateway-p:
      CPU: 6m
      Memory: 27Mi

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SPIKE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š MÃ©tricas de Performance (k6):
  â€¢ Throughput: 2584.55 req/s
  â€¢ Total de requisiÃ§Ãµes: 179,832
  â€¢ LatÃªncia mÃ©dia: 88.05 ms
  â€¢ LatÃªncia p95: 279.78 ms
  â€¢ Taxa de sucesso: 100.00%
  â€¢ Taxa de falha: 0.00%
  â€¢ VUs mÃ¡ximos: 200
  â€¢ IteraÃ§Ãµes: 59,944

ğŸ”„ Autoscaling (HPA):
  â€¢ a-hpa: 2 rÃ©plicas
  â€¢ b-hpa: 2 rÃ©plicas
  â€¢ p-hpa: 2 rÃ©plicas

ğŸ’» Uso de Recursos:
  â€¢ service-a:
      CPU: 25m
      Memory: 31Mi
  â€¢ service-b:
      CPU: 24m
      Memory: 30Mi
  â€¢ gateway-p:
      CPU: 32m
      Memory: 37Mi

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  FIM DO RELATÃ“RIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

