# Monitoramento K8s - Projeto PSPD

AplicaÃ§Ã£o gRPC distribuÃ­da (Pâ†’A,B) com Prometheus/Grafana em cluster multi-node Kubernetes.

---

## âš¡ Comandos Essenciais

### Setup Inicial (uma vez)
```bash
# Criar cluster (1 master + 2 workers)
./scripts/setup_multinode_cluster.sh

# Deploy aplicaÃ§Ã£o + monitoramento
kubectl apply -f k8s/
kubectl apply -f k8s/monitoring/
```

### Executar Testes
```bash
# Testes de carga (baseline, ramp, spike, soak)
./scripts/run_all_tests.sh all

# AnÃ¡lise comparativa de 5 cenÃ¡rios (2-3 horas)
./scripts/run_scenario_comparison.sh --all
```

### Acessar Dashboards
```bash
# Grafana (admin/admin)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# â†’ http://localhost:3000

# Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# â†’ http://localhost:9090
```

---

## ğŸ“Š Arquitetura

```
HTTP Request â†’ Gateway P (Node.js)
                   â†“ gRPC
              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
              â†“         â†“
         Service A  Service B
         (Python)   (Python streaming)
```

**Cluster K8s**: 1 master + 2 workers  
**Monitoramento**: Prometheus + Grafana  
**Autoscaling**: HPA configurado (CPU 70%, 1-10 replicas)

---

## ğŸ§ª Testes de Carga (k6)

| Teste | DuraÃ§Ã£o | VUs | Objetivo |
|-------|---------|-----|----------|
| **baseline** | 2min | 10 | Linha de base |
| **ramp** | 4.5min | 10â†’150 | Testar HPA |
| **spike** | 1.5min | 10â†’200â†’10 | ResiliÃªncia |
| **soak** | 11.5min | 50 | Estabilidade |

### MÃ©tricas Coletadas
- LatÃªncia (p50, p95, p99)
- Throughput (req/s)
- Taxa de sucesso/erro
- Scaling HPA (rÃ©plicas)
- CPU/MemÃ³ria por pod

### Resultados
```
results/
â”œâ”€â”€ baseline/output.txt
â”œâ”€â”€ ramp/output.txt
â”œâ”€â”€ spike/output.txt
â”œâ”€â”€ soak/output.txt
â””â”€â”€ plots/
    â”œâ”€â”€ 01_latency_comparison.png
    â”œâ”€â”€ 02_throughput_comparison.png
    â”œâ”€â”€ 03_success_rate.png
    â”œâ”€â”€ 04_hpa_scaling.png
    â”œâ”€â”€ 05_resource_usage.png
    â””â”€â”€ 06_latency_percentiles.png
```

---

## ğŸ¯ CenÃ¡rios de Teste

5 configuraÃ§Ãµes diferentes para anÃ¡lise comparativa:

| # | Nome | DescriÃ§Ã£o | Foco |
|---|------|-----------|------|
| 1 | **base** | HPA padrÃ£o, 1 rÃ©plica inicial | Baseline |
| 2 | **replicas** | 2 rÃ©plicas iniciais | Warm start |
| 3 | **distribution** | Anti-affinity forÃ§ada | Alta disponibilidade |
| 4 | **resources** | CPU/Mem -50% | Recursos limitados |
| 5 | **no-hpa** | RÃ©plicas fixas (3/5) | Sem autoscaling |

### Executar CenÃ¡rios
```bash
# Todos os cenÃ¡rios (2-3 horas)
./scripts/run_scenario_comparison.sh --all

# Apenas gerar grÃ¡ficos comparativos
./scripts/run_scenario_comparison.sh --compare

# Menu interativo (escolher 1 cenÃ¡rio)
./scripts/run_scenario_comparison.sh
```

### SaÃ­da Esperada
```
scenario-comparison/
â”œâ”€â”€ 01_scenario_latency_comparison.png
â”œâ”€â”€ 02_scenario_throughput_comparison.png
â”œâ”€â”€ 03_scenario_hpa_scaling.png
â”œâ”€â”€ 04_scenario_success_rate.png
â”œâ”€â”€ 05_scenario_cost_analysis.png
â”œâ”€â”€ 06_scenario_performance_radar.png
â””â”€â”€ SCENARIO_COMPARISON_REPORT.txt
```

---

## ğŸ“ˆ MÃ©tricas Prometheus

### MÃ©tricas Customizadas

**Gateway P (8080/metrics)**:
- `http_requests_total{method,route,status_code}`
- `http_request_duration_seconds{method,route,status_code}`
- `grpc_client_requests_total{service,method,status}`
- `grpc_client_request_duration_seconds{service,method,status}`

**Service A (9101/metrics)**:
- `grpc_server_requests_total{method,status}`
- `grpc_server_request_duration_seconds{method}`

**Service B (9102/metrics)**:
- `grpc_server_requests_total{method,status}`
- `grpc_server_request_duration_seconds{method}`
- `grpc_server_stream_items_total{method}`

### Queries PromQL Ãšteis
```promql
# Taxa de requisiÃ§Ãµes HTTP
rate(http_requests_total{app="p"}[1m])

# LatÃªncia P95 do Gateway
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{app="p"}[1m]))

# Taxa de erros
rate(http_requests_total{app="p",status_code=~"5.."}[1m])

# Chamadas gRPC do Gateway
rate(grpc_client_requests_total{app="p"}[1m])

# LatÃªncia do Service A
histogram_quantile(0.95, rate(grpc_server_request_duration_seconds_bucket{app="a"}[1m]))
```

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
.
â”œâ”€â”€ k8s/                          # Manifests Kubernetes
â”‚   â”œâ”€â”€ a.yaml                    # Deployment + Service A
â”‚   â”œâ”€â”€ b.yaml                    # Deployment + Service B
â”‚   â”œâ”€â”€ p.yaml                    # Deployment + Service Gateway P
â”‚   â”œâ”€â”€ monitoring/               # HPA + ServiceMonitors
â”‚   â””â”€â”€ scenarios/                # 5 cenÃ¡rios de teste
â”‚       â”œâ”€â”€ scenario1-base/
â”‚       â”œâ”€â”€ scenario2-replicas/
â”‚       â”œâ”€â”€ scenario3-distribution/
â”‚       â”œâ”€â”€ scenario4-resources/
â”‚       â””â”€â”€ scenario5-no-hpa/
â”‚
â”œâ”€â”€ services/                     # CÃ³digo dos microserviÃ§os
â”‚   â”œâ”€â”€ a_py/                     # Service A (Python gRPC)
â”‚   â”œâ”€â”€ b_py/                     # Service B (Python gRPC streaming)
â”‚   â””â”€â”€ gateway_p_node/           # Gateway P (Node.js + Express)
â”‚
â”œâ”€â”€ load/                         # Scripts k6
â”‚   â”œâ”€â”€ baseline.js
â”‚   â”œâ”€â”€ ramp.js
â”‚   â”œâ”€â”€ spike.js
â”‚   â””â”€â”€ soak.js
â”‚
â””â”€â”€ scripts/                      # AutomaÃ§Ã£o
    â”œâ”€â”€ setup_multinode_cluster.sh
    â”œâ”€â”€ run_all_tests.sh
    â””â”€â”€ run_scenario_comparison.sh
```

---

## ğŸ”§ Comandos Ãšteis

### Cluster
```bash
# Status do cluster
minikube status
kubectl get nodes

# Ver pods
kubectl get pods -n pspd
kubectl get pods -n monitoring

# Logs
kubectl logs -n pspd -l app=p
kubectl logs -n pspd -l app=a
```

### Testes Manuais
```bash
# Port-forward do Gateway P
kubectl port-forward -n pspd svc/p-svc 8080:80

# Testar endpoints
curl http://localhost:8080/a/hello?name=teste
curl http://localhost:8080/b/numbers?count=5
```

### MÃ©tricas
```bash
# Ver mÃ©tricas do Service A
kubectl port-forward -n pspd svc/a-svc 9101:9101
curl http://localhost:9101/metrics | grep grpc_server

# Verificar targets no Prometheus
# â†’ http://localhost:9090/targets
# Procurar: serviceMonitor/pspd/service-a-monitor/0
```

### Limpeza
```bash
# Deletar namespace
kubectl delete namespace pspd

# Parar cluster
minikube stop

# Deletar cluster
minikube delete
```

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **`docs/METRICAS_PROMETHEUS.md`** - Detalhes de todas as mÃ©tricas e queries PromQL
- **`k8s/scenarios/README.md`** - ConfiguraÃ§Ã£o dos 5 cenÃ¡rios de teste
- **`scenario-comparison/README.md`** - InterpretaÃ§Ã£o dos grÃ¡ficos comparativos

---

## ğŸ› Troubleshooting

### Pods nÃ£o iniciam
```bash
kubectl describe pod -n pspd <pod-name>
kubectl logs -n pspd <pod-name>
```

### HPA nÃ£o escala
```bash
kubectl get hpa -n pspd
kubectl describe hpa -n pspd a-hpa
kubectl top pods -n pspd  # Verificar CPU
```

### MÃ©tricas nÃ£o aparecem no Prometheus
```bash
# Verificar ServiceMonitors
kubectl get servicemonitor -n pspd

# Testar endpoint direto
kubectl exec -n pspd <pod-a> -- curl localhost:9101/metrics
```

### Port-forward falha (porta jÃ¡ em uso)
```bash
# Encontrar processo
ps aux | grep port-forward

# Matar processo
pkill -f "port-forward"
```

---

## ğŸ‘¥ Autores

Projeto Final - PSPD 2025.2
