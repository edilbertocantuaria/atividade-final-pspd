# Guia de Migra√ß√£o: Cluster Multi-Node com Prometheus e Grafana

## üìã Pr√©-requisitos

- Docker instalado
- kubectl instalado
- minikube instalado
- helm instalado
- M√≠nimo 8GB RAM e 4 CPUs dispon√≠veis

## üöÄ Setup Completo em 3 Passos

### Passo 1: Criar Cluster Multi-Node (1 control-plane + 2 workers)

```bash
# Criar cluster com 3 nodes
minikube start --nodes 3 --cpus 4 --memory 8192

# Habilitar addons necess√°rios
minikube addons enable metrics-server
minikube addons enable ingress

# Verificar nodes
kubectl get nodes
```

Este processo automaticamente:
- ‚úÖ Cria cluster com 3 n√≥s (1 control-plane + 2 workers)
- ‚úÖ Habilita metrics-server e ingress
- ‚úÖ Configura rede entre os n√≥s

**Tempo estimado**: 3-5 minutos

### Passo 1.5: Instalar Prometheus Stack (Opcional)

```bash
# Adicionar reposit√≥rio Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Instalar stack completo
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
```

Instala:
- ‚úÖ Prometheus Operator
- ‚úÖ Grafana com dashboards pr√©-configurados
- ‚úÖ Alertmanager
- ‚úÖ ServiceMonitors autom√°ticos

**Tempo estimado**: 3-5 minutos

### Passo 2: Deploy das Aplica√ß√µes

```bash
# Build das imagens no contexto Docker do Minikube
eval $(minikube docker-env)
docker build -t a-py:latest ./services/a_py
docker build -t b-py:latest ./services/b_py
docker build -t p-node:latest ./gateway_p_node

# Deploy dos recursos Kubernetes
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/a.yaml
kubectl apply -f k8s/b.yaml
kubectl apply -f k8s/p.yaml

# Configurar ServiceMonitors para Prometheus (se instalou Prometheus)
kubectl apply -f k8s/servicemonitors.yaml
```

### Passo 3: Acessar Interfaces de Monitoramento

#### Op√ß√£o A: Via NodePort (mais est√°vel)

```bash
# Obter IP do cluster
MINIKUBE_IP=$(minikube ip -p pspd-cluster)

# Grafana
GRAFANA_PORT=$(kubectl get svc -n monitoring prometheus-grafana -o jsonpath='{.spec.ports[0].nodePort}')
echo "Grafana: http://$MINIKUBE_IP:$GRAFANA_PORT"

# Prometheus
PROMETHEUS_PORT=$(kubectl get svc -n monitoring prometheus-kube-prometheus-prometheus -o jsonpath='{.spec.ports[0].nodePort}')
echo "Prometheus: http://$MINIKUBE_IP:$PROMETHEUS_PORT"

# Gateway P
kubectl get svc -n pspd p-svc
```

#### Op√ß√£o B: Via Port-Forward

```bash
# Terminal 1: Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# Acesse: http://localhost:3000
# User: admin | Password: (recuperar do secret)

# Recuperar senha do Grafana:
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d

# Terminal 2: Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# Acesse: http://localhost:9090

# Terminal 3: Gateway P
kubectl port-forward -n pspd svc/p-svc 8080:80
# Acesse: http://localhost:8080
```

## üìä Configurar Dashboard no Grafana

1. Acesse Grafana (http://localhost:3000 ou NodePort)
2. Login: `admin` / `admin`
3. V√° em: **+** ‚Üí **Import** ‚Üí **Upload JSON file**
4. Selecione: `k8s/monitoring/grafana-dashboard.json`
5. Clique em **Import**

O dashboard inclui:
- üìà HTTP Request Rate
- ‚è±Ô∏è Request Duration (p95, p99)
- üî¢ Pod Replicas (HPA)
- üíª CPU Usage por pod
- üíæ Memory Usage por pod
- ‚ùå Error Rate

## üß™ Executar Testes de Carga

```bash
# Terminal 1: Monitoramento em tempo real
./scripts/run_all_tests.sh monitor

# Terminal 2: Port-forward para aplica√ß√£o
kubectl port-forward -n pspd svc/p-svc 8080:80

# Terminal 3: Executar testes
BASE_URL=http://localhost:8080 ./scripts/run_all_tests.sh all

# Gerar gr√°ficos
./scripts/run_all_tests.sh analyze
```

## üîç Verificar Cluster Multi-Node

```bash
# Ver todos os n√≥s
kubectl get nodes -o wide

# Deve mostrar:
# NAME               STATUS   ROLES           AGE   VERSION
# pspd-cluster       Ready    control-plane   10m   v1.28.x
# pspd-cluster-m02   Ready    worker          9m    v1.28.x
# pspd-cluster-m03   Ready    worker          8m    v1.28.x

# Ver distribui√ß√£o de pods nos n√≥s
kubectl get pods -n pspd -o wide

# Ver m√©tricas dos n√≥s
kubectl top nodes
```

## üìä Verificar Prometheus

```bash
# Ver ServiceMonitors configurados
kubectl get servicemonitor -n pspd

# Ver targets no Prometheus
# Acesse: http://localhost:9090/targets
# Deve mostrar:
# - pspd/service-a-monitor
# - pspd/service-b-monitor
# - pspd/gateway-p-monitor

# Queries √∫teis:
# rate(http_requests_total{namespace="pspd"}[1m])
# histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))
```

## üéØ Valida√ß√£o Completa

### ‚úÖ Cluster Multi-Node
```bash
kubectl get nodes
# Deve mostrar 3 n√≥s (1 master + 2 workers)
```

### ‚úÖ Prometheus Instalado
```bash
kubectl get pods -n monitoring | grep prometheus
# Deve mostrar: prometheus-kube-prometheus-prometheus-0 Running
```

### ‚úÖ Grafana Instalado
```bash
kubectl get pods -n monitoring | grep grafana
# Deve mostrar: prometheus-grafana-xxx Running
```

### ‚úÖ ServiceMonitors Configurados
```bash
kubectl get servicemonitor -n pspd
# Deve mostrar 3 ServiceMonitors
```

### ‚úÖ M√©tricas Sendo Coletadas
```bash
# Via Prometheus UI (http://localhost:9090)
# Query: up{namespace="pspd"}
# Deve retornar 3 targets UP
```

## üõ†Ô∏è Troubleshooting

### Cluster n√£o inicia
```bash
# Aumentar recursos
minikube delete
minikube start --nodes 3 --cpus 4 --memory 8192
```

### Prometheus n√£o coleta m√©tricas
```bash
# Verificar ServiceMonitors
kubectl get servicemonitor -n pspd

# Recriar ServiceMonitors
kubectl apply -f k8s/servicemonitors.yaml

# Verificar logs do Prometheus
kubectl logs -n monitoring prometheus-prometheus-kube-prometheus-prometheus-0 -c prometheus
```

### Grafana n√£o abre
```bash
# Verificar pod
kubectl get pods -n monitoring | grep grafana

# Ver logs
kubectl logs -n monitoring deployment/prometheus-grafana

# Restart
kubectl rollout restart deployment -n monitoring prometheus-grafana
```

### Pods n√£o distribuem nos workers
```bash
# Remover taint do master (se necess√°rio para testes)
kubectl taint nodes pspd-cluster node-role.kubernetes.io/control-plane:NoSchedule-

# Adicionar nodeSelector nos deployments (opcional)
# Editar k8s/a.yaml, k8s/b.yaml, k8s/p.yaml:
# spec:
#   template:
#     spec:
#       nodeSelector:
#         node-role.kubernetes.io/worker: "true"
```

## üìö Recursos Adicionais

### Comandos √öteis

```bash
# Listar todos os recursos
kubectl get all -n pspd
kubectl get all -n monitoring

# Ver logs agregados
kubectl logs -n pspd -l app=p --tail=100 -f

# Escalar manualmente
kubectl scale deployment -n pspd p-deploy --replicas=5

# Ver eventos do cluster
kubectl get events -n pspd --sort-by='.lastTimestamp'

# Ver uso de recursos
kubectl top pods -n pspd
kubectl top nodes
```

### Limpeza

```bash
# Parar cluster (preserva dados)
minikube stop

# Deletar cluster completamente
minikube delete

# Limpar apenas namespace pspd
kubectl delete namespace pspd
```

## üéì Atendimento aos Requisitos Acad√™micos

### ‚úÖ Requisito 1: Cluster Multi-Node
- **Requisito**: "Cluster composto por um n√≥ mestre e pelo menos dois n√≥s escravos"
- **Implementado**: 1 master (pspd-cluster) + 2 workers (pspd-cluster-m02, m03)
- **Verifica√ß√£o**: `kubectl get nodes`

### ‚úÖ Requisito 2: Prometheus no K8s
- **Requisito**: "Estudar e instalar, no K8S, o Prometheus"
- **Implementado**: kube-prometheus-stack via Helm
- **Verifica√ß√£o**: `kubectl get pods -n monitoring | grep prometheus`

### ‚úÖ Requisito 3: Interface Web de Monitoramento
- **Requisito**: "Interface web de monitoramento do cluster"
- **Implementado**: Grafana com dashboard customizado
- **Verifica√ß√£o**: Acesse http://localhost:3000 ap√≥s port-forward: `kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80`

### ‚úÖ Requisito 4: ServiceMonitors
- **Implementado**: 3 ServiceMonitors (gateway-p, service-a, service-b)
- **Verifica√ß√£o**: `kubectl get servicemonitor -n pspd`

### ‚úÖ Requisito 5: M√©tricas Expostas
- **Implementado**: M√©tricas HTTP e gRPC em todos os servi√ßos
- **Verifica√ß√£o**: `curl http://localhost:8080/metrics`

## üìù Notas de Implementa√ß√£o

### Escolha do minikube multi-node

Optamos por **minikube multi-node** em vez de kind ou k3s por:
- ‚úÖ Suporte nativo a drivers (Docker, VirtualBox, KVM)
- ‚úÖ F√°cil integra√ß√£o com imagens locais (`minikube image load`)
- ‚úÖ Comandos consistentes com single-node (migra√ß√£o suave)
- ‚úÖ Suporte a NodePort direto (`minikube ip`)

### Stack de Monitoramento

Optamos por **kube-prometheus-stack** (Helm) por:
- ‚úÖ Prometheus Operator incluso
- ‚úÖ Grafana pr√©-configurado
- ‚úÖ Alertmanager incluso
- ‚úÖ ServiceMonitor CRD nativo
- ‚úÖ Dashboards padr√£o para K8s

### Configura√ß√µes Customizadas

- ServiceMonitors coletam m√©tricas a cada 15s
- Grafana com senha `admin` (trocar em produ√ß√£o!)
- NodePort habilitado para acesso externo f√°cil
- HPA configurado para auto-scaling baseado em CPU
