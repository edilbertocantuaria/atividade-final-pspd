# üìä M√©tricas Prometheus Customizadas

## Vis√£o Geral

Todos os tr√™s servi√ßos (A, B e P) foram instrumentados com m√©tricas customizadas usando Prometheus client libraries:
- **Servi√ßos A e B (Python)**: `prometheus-client==0.20.0`
- **Gateway P (Node.js)**: `prom-client==15.1.0`

---

## Servi√ßo A (Python gRPC)

### Porta de M√©tricas
- **Porta**: `9101`
- **Endpoint**: `http://<pod-ip>:9101/metrics`

### M√©tricas Expostas

#### `grpc_server_requests_total`
- **Tipo**: Counter
- **Descri√ß√£o**: Total de requisi√ß√µes gRPC recebidas pelo servi√ßo A
- **Labels**:
  - `method`: Nome do m√©todo gRPC (ex: `GetContent`)
  - `status`: Resultado (`success` ou `error`)

#### `grpc_server_request_duration_seconds`
- **Tipo**: Histogram
- **Descri√ß√£o**: Lat√™ncia das requisi√ß√µes gRPC em segundos
- **Labels**:
  - `method`: Nome do m√©todo gRPC
- **Buckets**: `[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]`

### Queries PromQL √öteis

```promql
# Taxa de requisi√ß√µes por segundo
rate(grpc_server_requests_total{container="a"}[1m])

# Taxa de erros
rate(grpc_server_requests_total{container="a",status="error"}[1m])

# Lat√™ncia P50
histogram_quantile(0.50, rate(grpc_server_request_duration_seconds_bucket{container="a"}[1m]))

# Lat√™ncia P95
histogram_quantile(0.95, rate(grpc_server_request_duration_seconds_bucket{container="a"}[1m]))

# Lat√™ncia P99
histogram_quantile(0.99, rate(grpc_server_request_duration_seconds_bucket{container="a"}[1m]))
```

---

## Servi√ßo B (Python gRPC Streaming)

### Porta de M√©tricas
- **Porta**: `9102`
- **Endpoint**: `http://<pod-ip>:9102/metrics`

### M√©tricas Expostas

#### `grpc_server_requests_total`
- **Tipo**: Counter
- **Descri√ß√£o**: Total de requisi√ß√µes gRPC recebidas pelo servi√ßo B
- **Labels**:
  - `method`: Nome do m√©todo gRPC (ex: `StreamMetadata`)
  - `status`: Resultado (`success` ou `error`)

#### `grpc_server_request_duration_seconds`
- **Tipo**: Histogram
- **Descri√ß√£o**: Lat√™ncia das requisi√ß√µes gRPC em segundos (streaming completo)
- **Labels**:
  - `method`: Nome do m√©todo gRPC
- **Buckets**: `[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]`

#### `grpc_server_stream_items_total`
- **Tipo**: Counter
- **Descri√ß√£o**: Total de items enviados via streaming
- **Labels**:
  - `method`: Nome do m√©todo gRPC

### Queries PromQL √öteis

```promql
# Taxa de requisi√ß√µes streaming por segundo
rate(grpc_server_requests_total{container="b",method="StreamMetadata"}[1m])

# Items streamed por segundo
rate(grpc_server_stream_items_total{container="b"}[1m])

# Lat√™ncia m√©dia do streaming
rate(grpc_server_request_duration_seconds_sum{container="b"}[1m]) 
/ 
rate(grpc_server_request_duration_seconds_count{container="b"}[1m])

# Lat√™ncia P95 do streaming
histogram_quantile(0.95, rate(grpc_server_request_duration_seconds_bucket{container="b"}[1m]))
```

---

## Gateway P (Node.js HTTP + gRPC Client)

### Porta de M√©tricas
- **Porta**: `8080` (mesma porta HTTP)
- **Endpoint**: `http://<pod-ip>:8080/metrics`

### M√©tricas Expostas

#### `http_requests_total`
- **Tipo**: Counter
- **Descri√ß√£o**: Total de requisi√ß√µes HTTP recebidas pelo gateway
- **Labels**:
  - `method`: M√©todo HTTP (ex: `GET`)
  - `route`: Rota acessada (ex: `/api/content`)
  - `status_code`: C√≥digo de resposta HTTP

#### `http_request_duration_seconds`
- **Tipo**: Histogram
- **Descri√ß√£o**: Lat√™ncia das requisi√ß√µes HTTP em segundos
- **Labels**:
  - `method`: M√©todo HTTP
  - `route`: Rota acessada
  - `status_code`: C√≥digo de resposta HTTP
- **Buckets**: `[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]`

#### `grpc_client_requests_total`
- **Tipo**: Counter
- **Descri√ß√£o**: Total de requisi√ß√µes gRPC feitas pelo gateway aos servi√ßos A e B
- **Labels**:
  - `service`: Servi√ßo destino (`ServiceA` ou `ServiceB`)
  - `method`: M√©todo gRPC chamado
  - `status`: Resultado (`success` ou `error`)

#### `grpc_client_request_duration_seconds`
- **Tipo**: Histogram
- **Descri√ß√£o**: Lat√™ncia das chamadas gRPC feitas pelo gateway
- **Labels**:
  - `service`: Servi√ßo destino
  - `method`: M√©todo gRPC chamado
  - `status`: Resultado
- **Buckets**: `[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]`

#### M√©tricas Padr√£o do Node.js
O gateway tamb√©m exp√µe m√©tricas padr√£o do processo Node.js:
- `process_cpu_user_seconds_total`
- `process_resident_memory_bytes`
- `nodejs_heap_size_total_bytes`
- `nodejs_eventloop_lag_seconds`

### Queries PromQL √öteis

```promql
# Taxa de requisi√ß√µes HTTP por segundo
rate(http_requests_total{container="p"}[1m])

# Taxa de erros HTTP (5xx)
rate(http_requests_total{container="p",status_code=~"5.."}[1m])

# Lat√™ncia P95 HTTP
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{container="p"}[1m]))

# Taxa de chamadas gRPC para servi√ßo A
rate(grpc_client_requests_total{container="p",service="ServiceA"}[1m])

# Lat√™ncia P95 das chamadas gRPC
histogram_quantile(0.95, rate(grpc_client_request_duration_seconds_bucket{container="p"}[1m]))

# Erros gRPC por servi√ßo
rate(grpc_client_requests_total{container="p",status="error"}[1m])

# Uso de mem√≥ria do processo Node.js
process_resident_memory_bytes{container="p"}
```

---

## ServiceMonitors Configurados

### Service A Monitor
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-a-monitor
  namespace: pspd
spec:
  selector:
    matchLabels:
      app: a
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
```

### Service B Monitor
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: service-b-monitor
  namespace: pspd
spec:
  selector:
    matchLabels:
      app: b
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
```

### Gateway P Monitor
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: gateway-p-metrics
  namespace: pspd
spec:
  selector:
    matchLabels:
      app: p
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
```

---

## Verifica√ß√£o de M√©tricas

### Verifica√ß√£o Manual via port-forward

#### Teste local via port-forward
```bash
# Servi√ßo A
kubectl port-forward -n pspd svc/a-svc 9101:9101
curl http://localhost:9101/metrics | grep grpc_server

# Servi√ßo B
kubectl port-forward -n pspd svc/b-svc 9102:9102
curl http://localhost:9102/metrics | grep grpc_server

# Gateway P
kubectl port-forward -n pspd svc/p-svc 8080:8080
curl http://localhost:8080/metrics | grep -E "(http_|grpc_client)"
```

#### Verificar targets no Prometheus
```bash
# Port-forward do Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090

# Acessar: http://localhost:9090/targets
# Procurar por: serviceMonitor/pspd/service-a-monitor/0
```

---

## üìä Acessar Grafana

### Port-Forward
```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```
Acesse: http://localhost:3000

### Credenciais
- **Usu√°rio**: `admin`
- **Senha**: Recuperar do secret:
```bash
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d
```

### Importar Dashboard Customizado

1. Acesse Grafana ‚Üí **+** ‚Üí **Import**
2. Upload: `k8s/monitoring/grafana-dashboard.json`
3. Selecione **prometheus** como data source
4. Clique em **Import**

---

## Dashboards Grafana Sugeridos

### Dashboard: Vis√£o Geral da Aplica√ß√£o

#### Painel 1: Taxa de Requisi√ß√µes
```promql
# HTTP (Gateway P)
sum(rate(http_requests_total{container="p"}[1m])) by (route)

# gRPC Servi√ßo A
sum(rate(grpc_server_requests_total{container="a"}[1m])) by (method)

# gRPC Servi√ßo B
sum(rate(grpc_server_requests_total{container="b"}[1m])) by (method)
```

#### Painel 2: Lat√™ncia P95
```promql
# Gateway P (HTTP)
histogram_quantile(0.95, 
  sum(rate(http_request_duration_seconds_bucket{container="p"}[1m])) by (le, route)
)

# Servi√ßo A
histogram_quantile(0.95, 
  sum(rate(grpc_server_request_duration_seconds_bucket{container="a"}[1m])) by (le)
)

# Servi√ßo B
histogram_quantile(0.95, 
  sum(rate(grpc_server_request_duration_seconds_bucket{container="b"}[1m])) by (le)
)
```

#### Painel 3: Taxa de Erros
```promql
# HTTP 5xx
sum(rate(http_requests_total{container="p",status_code=~"5.."}[1m]))

# gRPC Errors (Gateway ‚Üí A/B)
sum(rate(grpc_client_requests_total{container="p",status="error"}[1m])) by (service)

# gRPC Errors (Servi√ßos A e B)
sum(rate(grpc_server_requests_total{status="error"}[1m])) by (app)
```

#### Painel 4: Throughput gRPC Client (Gateway P)
```promql
sum(rate(grpc_client_requests_total{container="p",status="success"}[1m])) by (service, method)
```

#### Painel 5: Streaming (Servi√ßo B)
```promql
# Items por segundo
rate(grpc_server_stream_items_total{container="b"}[1m])

# Streams ativos
grpc_server_requests_total{container="b",method="StreamMetadata"} - grpc_server_requests_total{container="b",method="StreamMetadata"} offset 1m
```

---

## Integra√ß√£o com Testes k6

Durante os testes de carga, voc√™ pode correlacionar:

1. **M√©tricas k6** (cliente):
   - `http_req_duration` ‚Üí Lat√™ncia percebida pelo cliente
   - `http_reqs` ‚Üí Taxa de requisi√ß√µes enviadas
   - `http_req_failed` ‚Üí Taxa de falhas

2. **M√©tricas Prometheus** (servidor):
   - `http_request_duration_seconds` ‚Üí Lat√™ncia no gateway
   - `grpc_client_request_duration_seconds` ‚Üí Lat√™ncia nas chamadas gRPC
   - `grpc_server_request_duration_seconds` ‚Üí Lat√™ncia nos servi√ßos A/B

**An√°lise √∫til**:
```
Lat√™ncia Total (k6) = 
  Lat√™ncia Gateway (http_request_duration) + 
  Lat√™ncia gRPC A (grpc_client_request_duration) + 
  Lat√™ncia gRPC B (grpc_client_request_duration) +
  Network overhead
```

---

## Troubleshooting

### M√©tricas n√£o aparecem no Prometheus

1. **Verificar ServiceMonitor**:
```bash
kubectl get servicemonitor -n pspd
kubectl describe servicemonitor service-a-monitor -n pspd
```

2. **Verificar labels no Prometheus Operator**:
```bash
kubectl get prometheus -n monitoring -o yaml | grep serviceMonitorSelector -A 5
```

3. **Verificar targets no Prometheus**:
   - Acesse `http://localhost:9090/targets`
   - Procure por `pspd/service-a-monitor`
   - Se estiver **DOWN**, verifique logs do pod

4. **Testar endpoint manualmente**:
```bash
kubectl exec -n pspd <pod-a> -- curl localhost:9101/metrics
```

### M√©tricas vazias ap√≥s deploy

- M√©tricas tipo **Counter** e **Histogram** s√≥ aparecem ap√≥s receber dados
- Fa√ßa requisi√ß√µes de teste:
```bash
curl "http://localhost:8080/api/content?type=all&limit=5"
curl "http://localhost:8080/api/metadata/m1?userId=test"
```

### Port-forward falha

```bash
# Verificar se pod est√° Ready
kubectl get pods -n pspd

# Verificar logs
kubectl logs -n pspd <pod-name>

# Verificar se porta est√° ouvindo
kubectl exec -n pspd <pod-name> -- netstat -tuln | grep 9101
```

---

## Resumo das Portas

| Servi√ßo | Porta gRPC | Porta M√©tricas | Endpoint |
|---------|-----------|----------------|----------|
| A       | 50051     | 9101          | `/metrics` |
| B       | 50052     | 9102          | `/metrics` |
| P       | 8080      | 8080          | `/metrics` |

---

## Pr√≥ximos Passos

1. ‚úÖ M√©tricas implementadas
2. ‚úÖ ServiceMonitors configurados
3. ‚è≥ Verificar m√©tricas via port-forward (comandos acima)
4. ‚è≥ Criar dashboards Grafana customizados
5. ‚è≥ Executar testes de carga e correlacionar m√©tricas
6. ‚è≥ Documentar insights obtidos das m√©tricas
